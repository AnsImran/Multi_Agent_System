"subject","sub_subject","book_name","author_name","page_number","text","word_count"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","1","","0"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","2","The Feynman
LECTURES ON
PHYSICS
NEW MILLENNIUM EDITION
FEYNMAN•LEIGHTON•SANDS
VOLUME III","11"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","3","Copyright © 1965, 2006, 2010 by California Institute of  Technology,
Michael A. Gottlieb, and Rudolf  Pfeiffer
Published by Basic Books,
A Member of  the Perseus Books Group
All rights reserved. Printed in the United States of  America.
No part of  this book may be reproduced in any manner whatsoever without written permission
except in the case of  brief  quotations embodied in critical articles and reviews.
For information, address Basic Books, 250 West 57th Street, 15th Floor, New York, NY 10107.
Books published by Basic Books are available at special discounts for bulk purchases
in the United States by corporations, institutions, and other organizations.
For more information, please contact the Special Markets Department at the
Perseus Books Group, 2300 Chestnut Street, Suite 200, Philadelphia, PA 19103,
or call (800) 810-4145, ext. 5000, or e-mail special.markets@perseusbooks.com.
A CIP catalog record for the hardcover edition of  
this book is available from the Library of  Congress.
LCCN: 2010938208
Hardcover ISBN: 978-0-465-02417-9
E-book ISBN: 978-0-465-07294-1","160"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","4","About Richard Feynman
Born in 1918 in New York City, Richard P. Feynman received his Ph.D.
from Princeton in 1942. Despite his youth, he played an important part in the
Manhattan Project at Los Alamos during World War II. Subsequently, he taught
at Cornell and at the California Institute of Technology. In 1965 he received the
Nobel Prize in Physics, along with Sin-Itiro Tomonaga and Julian Schwinger, for
his work in quantum electrodynamics.
Dr. Feynman won his Nobel Prize for successfully resolving problems with the
theory of quantum electrodynamics. He also created a mathematical theory that
accounts for the phenomenon of superﬂuidity in liquid helium. Thereafter, with
Murray Gell-Mann, he did fundamental work in the area of weak interactions such
as beta decay. In later years Feynman played a key role in the development of
quark theory by putting forward his parton model of high energy proton collision
processes.
Beyond these achievements, Dr. Feynman introduced basic new computa-
tional techniques and notations into physics—above all, the ubiquitous Feynman
diagrams that, perhaps more than any other formalism in recent scientiﬁc history,
have changed the way in which basic physical processes are conceptualized and
calculated.
Feynman was a remarkably eﬀective educator. Of all his numerous awards,
he was especially proud of the Oersted Medal for Teaching, which he won in
1972. The Feynman Lectures on Physics, originally published in 1963, were
described by a reviewer in Scientiﬁc American as “tough, but nourishing and full
of ﬂavor. After 25 years it is the guide for teachers and for the best of beginning
students.” In order to increase the understanding of physics among the lay public,
Dr. Feynman wrote The Character of Physical Law and QED: The Strange
Theory of Light and Matter. He also authored a number of advanced publications
that have become classic references and textbooks for researchers and students.
Richard Feynman was a constructive public man. His work on the Challenger
commission is well known, especially his famous demonstration of the susceptibility
of the O-rings to cold, an elegant experiment which required nothing more than
a glass of ice water and a C-clamp. Less well known were Dr. Feynman’s eﬀorts
on the California State Curriculum Committee in the 1960s, where he protested
the mediocrity of textbooks.
iii","376"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","5","A recital of Richard Feynman’s myriad scientiﬁc and educational accomplish-
ments cannot adequately capture the essence of the man. As any reader of
even his most technical publications knows, Feynman’s lively and multi-sided
personality shines through all his work. Besides being a physicist, he was at
various times a repairer of radios, a picker of locks, an artist, a dancer, a bongo
player, and even a decipherer of Mayan Hieroglyphics. Perpetually curious about
his world, he was an exemplary empiricist.
Richard Feynman died on February 15, 1988, in Los Angeles.
iv","91"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","6","Preface to the New Millennium Edition
Nearly ﬁfty years have passed since Richard Feynman taught the introductory
physics course at Caltech that gave rise to these three volumes, The Feynman
Lectures on Physics.
In those ﬁfty years our understanding of the physical
world has changed greatly, but The Feynman Lectures on Physics has endured.
Feynman’s lectures are as powerful today as when ﬁrst published, thanks to
Feynman’s unique physics insights and pedagogy.
They have been studied
worldwide by novices and mature physicists alike; they have been translated
into at least a dozen languages with more than 1.5 millions copies printed in the
English language alone. Perhaps no other set of physics books has had such wide
impact, for so long.
This New Millennium Edition ushers in a new era for The Feynman Lectures
on Physics (FLP): the twenty-ﬁrst century era of electronic publishing. FLP
has been converted to eFLP, with the text and equations expressed in the LATEX
electronic typesetting language, and all ﬁgures redone using modern drawing
software.
The consequences for the print version of this edition are not startling; it
looks almost the same as the original red books that physics students have known
and loved for decades. The main diﬀerences are an expanded and improved index,
the correction of 885 errata found by readers over the ﬁve years since the ﬁrst
printing of the previous edition, and the ease of correcting errata that future
readers may ﬁnd. To this I shall return below.
The eBook Version of this edition, and the Enhanced Electronic Version are
electronic innovations. By contrast with most eBook versions of 20th century tech-
nical books, whose equations, ﬁgures and sometimes even text become pixellated
when one tries to enlarge them, the LATEX manuscript of the New Millennium
Edition makes it possible to create eBooks of the highest quality, in which all
v","309"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","7","features on the page (except photographs) can be enlarged without bound and
retain their precise shapes and sharpness. And the Enhanced Electronic Version,
with its audio and blackboard photos from Feynman’s original lectures, and its
links to other resources, is an innovation that would have given Feynman great
pleasure.
Memories of Feynman’s Lectures
These three volumes are a self-contained pedagogical treatise. They are also a
historical record of Feynman’s 1961–64 undergraduate physics lectures, a course
required of all Caltech freshmen and sophomores regardless of their majors.
Readers may wonder, as I have, how Feynman’s lectures impacted the students
who attended them. Feynman, in his Preface to these volumes, oﬀered a somewhat
negative view. “I don’t think I did very well by the students,” he wrote. Matthew
Sands, in his memoir in Feynman’s Tips on Physics expressed a far more positive
view. Out of curiosity, in spring 2005 I emailed or talked to a quasi-random set
of 17 students (out of about 150) from Feynman’s 1961–63 class—some who had
great diﬃculty with the class, and some who mastered it with ease; majors in
biology, chemistry, engineering, geology, mathematics and astronomy, as well as
in physics.
The intervening years might have glazed their memories with a euphoric tint,
but about 80 percent recall Feynman’s lectures as highlights of their college years.
“It was like going to church.” The lectures were “a transformational experience,”
“the experience of a lifetime, probably the most important thing I got from
Caltech.” “I was a biology major but Feynman’s lectures stand out as a high
point in my undergraduate experience . . . though I must admit I couldn’t do
the homework at the time and I hardly turned any of it in.” “I was among the
least promising of students in this course, and I never missed a lecture. . . . I
remember and can still feel Feynman’s joy of discovery. . . . His lectures had an
. . . emotional impact that was probably lost in the printed Lectures.”
By contrast, several of the students have negative memories due largely to two
issues: (i) “You couldn’t learn to work the homework problems by attending the
lectures. Feynman was too slick—he knew tricks and what approximations could
be made, and had intuition based on experience and genius that a beginning
student does not possess.” Feynman and colleagues, aware of this ﬂaw in the
course, addressed it in part with materials that have been incorporated into
Feynman’s Tips on Physics: three problem-solving lectures by Feynman, and
vi","422"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","8","a set of exercises and answers assembled by Robert B. Leighton and Rochus
Vogt. (ii) “The insecurity of not knowing what was likely to be discussed in
the next lecture, the lack of a text book or reference with any connection to
the lecture material, and consequent inability for us to read ahead, were very
frustrating. . . . I found the lectures exciting and understandable in the hall, but
they were Sanskrit outside [when I tried to reconstruct the details].” This problem,
of course, was solved by these three volumes, the printed version of The Feynman
Lectures on Physics. They became the textbook from which Caltech students
studied for many years thereafter, and they live on today as one of Feynman’s
greatest legacies.
A History of Errata
The Feynman Lectures on Physics was produced very quickly by Feynman
and his co-authors, Robert B. Leighton and Matthew Sands, working from
and expanding on tape recordings and blackboard photos of Feynman’s course
lectures† (both of which are incorporated into the Enhanced Electronic Version
of this New Millennium Edition). Given the high speed at which Feynman,
Leighton and Sands worked, it was inevitable that many errors crept into the ﬁrst
edition. Feynman accumulated long lists of claimed errata over the subsequent
years—errata found by students and faculty at Caltech and by readers around
the world. In the 1960’s and early 70’s, Feynman made time in his intense life
to check most but not all of the claimed errata for Volumes I and II, and insert
corrections into subsequent printings. But Feynman’s sense of duty never rose
high enough above the excitement of discovering new things to make him deal
with the errata in Volume III.‡ After his untimely death in 1988, lists of errata
for all three volumes were deposited in the Caltech Archives, and there they lay
forgotten.
In 2002 Ralph Leighton (son of the late Robert Leighton and compatriot of
Feynman) informed me of the old errata and a new long list compiled by Ralph’s
† For descriptions of the genesis of Feynman’s lectures and of these volumes, see Feynman’s
Preface and the Forewords to each of the three volumes, and also Matt Sands’ Memoir in
Feynman’s Tips on Physics, and the Special Preface to the Commemorative Edition of FLP,
written in 1989 by David Goodstein and Gerry Neugebauer, which also appears in the 2005
Deﬁnitive Edition.
‡ In 1975, he started checking errata for Volume III but got distracted by other things and
never ﬁnished the task, so no corrections were made.
vii","424"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","9","friend Michael Gottlieb. Leighton proposed that Caltech produce a new edition
of The Feynman Lectures with all errata corrected, and publish it alongside a new
volume of auxiliary material, Feynman’s Tips on Physics, which he and Gottlieb
were preparing.
Feynman was my hero and a close personal friend. When I saw the lists of
errata and the content of the proposed new volume, I quickly agreed to oversee
this project on behalf of Caltech (Feynman’s long-time academic home, to which
he, Leighton and Sands had entrusted all rights and responsibilities for The
Feynman Lectures). After a year and a half of meticulous work by Gottlieb, and
careful scrutiny by Dr. Michael Hartl (an outstanding Caltech postdoc who vetted
all errata plus the new volume), the 2005 Deﬁnitive Edition of The Feynman
Lectures on Physics was born, with about 200 errata corrected and accompanied
by Feynman’s Tips on Physics by Feynman, Gottlieb and Leighton.
I thought that edition was going to be “Deﬁnitive”. What I did not antic-
ipate was the enthusiastic response of readers around the world to an appeal
from Gottlieb to identify further errata, and submit them via a website that
Gottlieb created and continues to maintain, The Feynman Lectures Website,
www.feynmanlectures.info. In the ﬁve years since then, 965 new errata have
been submitted and survived the meticulous scrutiny of Gottlieb, Hartl, and Nate
Bode (an outstanding Caltech physics graduate student, who succeeded Hartl
as Caltech’s vetter of errata). Of these, 965 vetted errata, 80 were corrected in
the fourth printing of the Deﬁnitive Edition (August 2006) and the remaining
885 are corrected in the ﬁrst printing of this New Millennium Edition (332 in
volume I, 263 in volume II, and 200 in volume III). For details of the errata, see
www.feynmanlectures.info.
Clearly, making The Feynman Lectures on Physics error-free has become a
world-wide community enterprise. On behalf of Caltech I thank the 50 readers
who have contributed since 2005 and the many more who may contribute over the
coming years. The names of all contributors are posted at www.feynmanlectures.
info/flp_errata.html.
Almost all the errata have been of three types: (i) typographical errors
in prose; (ii) typographical and mathematical errors in equations, tables and
ﬁgures—sign errors, incorrect numbers (e.g., a 5 that should be a 4), and missing
subscripts, summation signs, parentheses and terms in equations; (iii) incorrect
cross references to chapters, tables and ﬁgures. These kinds of errors, though
not terribly serious to a mature physicist, can be frustrating and confusing to
Feynman’s primary audience: students.
viii","420"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","10","It is remarkable that among the 1165 errata corrected under my auspices,
only several do I regard as true errors in physics. An example is Volume II,
page 5-9, which now says “. . . no static distribution of charges inside a closed
grounded conductor can produce any [electric] ﬁelds outside” (the word grounded
was omitted in previous editions). This error was pointed out to Feynman by a
number of readers, including Beulah Elizabeth Cox, a student at The College of
William and Mary, who had relied on Feynman’s erroneous passage in an exam.
To Ms. Cox, Feynman wrote in 1975,† “Your instructor was right not to give
you any points, for your answer was wrong, as he demonstrated using Gauss’s
law. You should, in science, believe logic and arguments, carefully drawn, and
not authorities. You also read the book correctly and understood it. I made a
mistake, so the book is wrong. I probably was thinking of a grounded conducting
sphere, or else of the fact that moving the charges around in diﬀerent places
inside does not aﬀect things on the outside. I am not sure how I did it, but I
goofed. And you goofed, too, for believing me.”
How this New Millennium Edition Came to Be
Between November 2005 and July 2006, 340 errata were submitted to The
Feynman Lectures Website www.feynmanlectures.info. Remarkably, the bulk
of these came from one person: Dr. Rudolf Pfeiﬀer, then a physics postdoctoral
fellow at the University of Vienna, Austria. The publisher, Addison Wesley, ﬁxed
80 errata, but balked at ﬁxing more because of cost: the books were being printed
by a photo-oﬀset process, working from photographic images of the pages from
the 1960s. Correcting an error involved re-typesetting the entire page, and to
ensure no new errors crept in, the page was re-typeset twice by two diﬀerent
people, then compared and proofread by several other people—a very costly
process indeed, when hundreds of errata are involved.
Gottlieb, Pfeiﬀer and Ralph Leighton were very unhappy about this, so they
formulated a plan aimed at facilitating the repair of all errata, and also aimed
at producing eBook and enhanced electronic versions of The Feynman Lectures
on Physics. They proposed their plan to me, as Caltech’s representative, in
2007. I was enthusiastic but cautious. After seeing further details, including a
one-chapter demonstration of the Enhanced Electronic Version, I recommended
† Pages 288–289 of Perfectly Reasonable Deviations from the Beaten Track, The Letters of
Richard P. Feynman, ed. Michelle Feynman (Basic Books, New York, 2005).
ix","421"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","11","that Caltech cooperate with Gottlieb, Pfeiﬀer and Leighton in the execution of
their plan. The plan was approved by three successive chairs of Caltech’s Division
of Physics, Mathematics and Astronomy—Tom Tombrello, Andrew Lange, and
Tom Soifer—and the complex legal and contractual details were worked out by
Caltech’s Intellectual Property Counsel, Adam Cochran. With the publication of
this New Millennium Edition, the plan has been executed successfully, despite
its complexity. Speciﬁcally:
Pfeiﬀer and Gottlieb have converted into LATEX all three volumes of FLP
(and also more than 1000 exercises from the Feynman course for incorporation
into Feynman’s Tips on Physics). The FLP ﬁgures were redrawn in modern
electronic form in India, under guidance of the FLP German translator, Henning
Heinze, for use in the German edition. Gottlieb and Pfeiﬀer traded non-exclusive
use of their LATEX equations in the German edition (published by Oldenbourg)
for non-exclusive use of Heinze’s ﬁgures in this New Millennium English edition.
Pfeiﬀer and Gottlieb have meticulously checked all the LATEX text and equations
and all the redrawn ﬁgures, and made corrections as needed. Nate Bode and
I, on behalf of Caltech, have done spot checks of text, equations, and ﬁgures;
and remarkably, we have found no errors. Pfeiﬀer and Gottlieb are unbelievably
meticulous and accurate. Gottlieb and Pfeiﬀer arranged for John Sullivan at the
Huntington Library to digitize the photos of Feynman’s 1962–64 blackboards,
and for George Blood Audio to digitize the lecture tapes—with ﬁnancial support
and encouragement from Caltech Professor Carver Mead, logistical support from
Caltech Archivist Shelley Erwin, and legal support from Cochran.
The legal issues were serious: In the 1960s, Caltech licensed to Addison Wesley
rights to publish the print edition, and in the 1990s, rights to distribute the audio
of Feynman’s lectures and a variant of an electronic edition. In the 2000s, through
a sequence of acquisitions of those licenses, the print rights were transferred to
the Pearson publishing group, while rights to the audio and the electronic version
were transferred to the Perseus publishing group. Cochran, with the aid of Ike
Williams, an attorney who specializes in publishing, succeeded in uniting all of
these rights with Perseus (Basic Books), making possible this New Millennium
Edition.
Acknowledgments
On behalf of Caltech, I thank the many people who have made this New
Millennium Edition possible. Speciﬁcally, I thank the key people mentioned
x","389"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","12","above: Ralph Leighton, Michael Gottlieb, Tom Tombrello, Michael Hartl, Rudolf
Pfeiﬀer, Henning Heinze, Adam Cochran, Carver Mead, Nate Bode, Shelley Erwin,
Andrew Lange, Tom Soifer, Ike Williams, and the 50 people who submitted errata
(listed at www.feynmanlectures.info). And I also thank Michelle Feynman
(daughter of Richard Feynman) for her continuing support and advice, Alan Rice
for behind-the-scenes assistance and advice at Caltech, Stephan Puchegger and
Calvin Jackson for assistance and advice to Pfeiﬀer about conversion of FLP to
LATEX, Michael Figl, Manfred Smolik, and Andreas Stangl for discussions about
corrections of errata; and the Staﬀof Perseus/Basic Books, and (for previous
editions) the staﬀof Addison Wesley.
Kip S. Thorne
The Feynman Professor of Theoretical Physics, Emeritus
California Institute of Technology
October 2010
xi","122"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","13","L E C T U R E S  O N
P H Y S I C S
QUANTUM MECHANICS
RICHARD P. FEYNMAN
Richard Chace Tolman Professor of Theoretical Physics
California Institute of Technology
ROBERT B. LEIGHTON
Professor of Physics
California Institute of Technology
MATTHEW SANDS
Professor of Physics
California Institute of Technology","52"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","14","Copyright © 1965 
 
CALIFORNIA INSTITUTE OF TECHNOLOGY 
————————— 
Printed in the United States of America 
 
 
ALL RIGHTS RESERVED. THIS BOOK, OR PARTS THEREOF 
MAY NOT BE REPRODUCED IN ANY FORM WITHOUT 
WRITTEN PERMISSION OF THE COPYRIGHT HOLDER. 
 
 
Library of Congress Catalog Card No. 63-20717 
 
Third printing, July 1966 
 
ISBN 0-201-02118-8-P 
     0-201-02114-9-H 
               BBCCDDEEFFGG-MU-898","52"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","15","Feynman’s Preface
These are the lectures in physics that I gave last year and the year before
to the freshman and sophomore classes at Caltech. The lectures are, of course,
not verbatim—they have been edited, sometimes extensively and sometimes less
so. The lectures form only part of the complete course. The whole group of 180
students gathered in a big lecture room twice a week to hear these lectures and
then they broke up into small groups of 15 to 20 students in recitation sections
under the guidance of a teaching assistant. In addition, there was a laboratory
session once a week.
The special problem we tried to get at with these lectures was to maintain
the interest of the very enthusiastic and rather smart students coming out of
the high schools and into Caltech. They have heard a lot about how interesting
and exciting physics is—the theory of relativity, quantum mechanics, and other
modern ideas. By the end of two years of our previous course, many would be
very discouraged because there were really very few grand, new, modern ideas
presented to them. They were made to study inclined planes, electrostatics, and
so forth, and after two years it was quite stultifying. The problem was whether
or not we could make a course which would save the more advanced and excited
student by maintaining his enthusiasm.
The lectures here are not in any way meant to be a survey course, but are very
serious. I thought to address them to the most intelligent in the class and to make
sure, if possible, that even the most intelligent student was unable to completely
encompass everything that was in the lectures—by putting in suggestions of
3","285"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","16","applications of the ideas and concepts in various directions outside the main line
of attack. For this reason, though, I tried very hard to make all the statements
as accurate as possible, to point out in every case where the equations and ideas
ﬁtted into the body of physics, and how—when they learned more—things would
be modiﬁed. I also felt that for such students it is important to indicate what
it is that they should—if they are suﬃciently clever—be able to understand by
deduction from what has been said before, and what is being put in as something
new. When new ideas came in, I would try either to deduce them if they were
deducible, or to explain that it was a new idea which hadn’t any basis in terms of
things they had already learned and which was not supposed to be provable—but
was just added in.
At the start of these lectures, I assumed that the students knew something
when they came out of high school—such things as geometrical optics, simple
chemistry ideas, and so on. I also didn’t see that there was any reason to make the
lectures in a deﬁnite order, in the sense that I would not be allowed to mention
something until I was ready to discuss it in detail. There was a great deal of
mention of things to come, without complete discussions. These more complete
discussions would come later when the preparation became more advanced.
Examples are the discussions of inductance, and of energy levels, which are at
ﬁrst brought in in a very qualitative way and are later developed more completely.
At the same time that I was aiming at the more active student, I also wanted
to take care of the fellow for whom the extra ﬁreworks and side applications are
merely disquieting and who cannot be expected to learn most of the material
in the lecture at all. For such students I wanted there to be at least a central
core or backbone of material which he could get. Even if he didn’t understand
everything in a lecture, I hoped he wouldn’t get nervous. I didn’t expect him to
understand everything, but only the central and most direct features. It takes,
of course, a certain intelligence on his part to see which are the central theorems
and central ideas, and which are the more advanced side issues and applications
which he may understand only in later years.
In giving these lectures there was one serious diﬃculty: in the way the course
was given, there wasn’t any feedback from the students to the lecturer to indicate
how well the lectures were going over. This is indeed a very serious diﬃculty, and
I don’t know how good the lectures really are. The whole thing was essentially
an experiment. And if I did it again I wouldn’t do it the same way—I hope I
don’t have to do it again! I think, though, that things worked out—so far as the
physics is concerned—quite satisfactorily in the ﬁrst year.
4","507"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","17","In the second year I was not so satisﬁed. In the ﬁrst part of the course, dealing
with electricity and magnetism, I couldn’t think of any really unique or diﬀerent
way of doing it—of any way that would be particularly more exciting than the
usual way of presenting it. So I don’t think I did very much in the lectures on
electricity and magnetism. At the end of the second year I had originally intended
to go on, after the electricity and magnetism, by giving some more lectures on
the properties of materials, but mainly to take up things like fundamental modes,
solutions of the diﬀusion equation, vibrating systems, orthogonal functions, . . .
developing the ﬁrst stages of what are usually called “the mathematical methods
of physics.” In retrospect, I think that if I were doing it again I would go back
to that original idea. But since it was not planned that I would be giving these
lectures again, it was suggested that it might be a good idea to try to give an
introduction to the quantum mechanics—what you will ﬁnd in Volume III.
It is perfectly clear that students who will major in physics can wait until
their third year for quantum mechanics. On the other hand, the argument was
made that many of the students in our course study physics as a background for
their primary interest in other ﬁelds. And the usual way of dealing with quantum
mechanics makes that subject almost unavailable for the great majority of students
because they have to take so long to learn it. Yet, in its real applications—
especially in its more complex applications, such as in electrical engineering
and chemistry—the full machinery of the diﬀerential equation approach is not
actually used. So I tried to describe the principles of quantum mechanics in
a way which wouldn’t require that one ﬁrst know the mathematics of partial
diﬀerential equations. Even for a physicist I think that is an interesting thing
to try to do—to present quantum mechanics in this reverse fashion—for several
reasons which may be apparent in the lectures themselves. However, I think that
the experiment in the quantum mechanics part was not completely successful—in
large part because I really did not have enough time at the end (I should, for
instance, have had three or four more lectures in order to deal more completely
with such matters as energy bands and the spatial dependence of amplitudes).
Also, I had never presented the subject this way before, so the lack of feedback was
particularly serious. I now believe the quantum mechanics should be given at a
later time. Maybe I’ll have a chance to do it again someday. Then I’ll do it right.
The reason there are no lectures on how to solve problems is because there
were recitation sections. Although I did put in three lectures in the ﬁrst year on
how to solve problems, they are not included here. Also there was a lecture on
inertial guidance which certainly belongs after the lecture on rotating systems,
5","511"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","18","but which was, unfortunately, omitted. The ﬁfth and sixth lectures are actually
due to Matthew Sands, as I was out of town.
The question, of course, is how well this experiment has succeeded. My own
point of view—which, however, does not seem to be shared by most of the people
who worked with the students—is pessimistic. I don’t think I did very well by
the students. When I look at the way the majority of the students handled the
problems on the examinations, I think that the system is a failure. Of course,
my friends point out to me that there were one or two dozen students who—very
surprisingly—understood almost everything in all of the lectures, and who were
quite active in working with the material and worrying about the many points
in an excited and interested way. These people have now, I believe, a ﬁrst-rate
background in physics—and they are, after all, the ones I was trying to get at.
But then, “The power of instruction is seldom of much eﬃcacy except in those
happy dispositions where it is almost superﬂuous.” (Gibbon)
Still, I didn’t want to leave any student completely behind, as perhaps I did.
I think one way we could help the students more would be by putting more hard
work into developing a set of problems which would elucidate some of the ideas
in the lectures. Problems give a good opportunity to ﬁll out the material of the
lectures and make more realistic, more complete, and more settled in the mind
the ideas that have been exposed.
I think, however, that there isn’t any solution to this problem of education
other than to realize that the best teaching can be done only when there is a
direct individual relationship between a student and a good teacher—a situation
in which the student discusses the ideas, thinks about the things, and talks about
the things. It’s impossible to learn very much by simply sitting in a lecture, or
even by simply doing problems that are assigned. But in our modern times we
have so many students to teach that we have to try to ﬁnd some substitute for
the ideal. Perhaps my lectures can make some contribution. Perhaps in some
small place where there are individual teachers and students, they may get some
inspiration or some ideas from the lectures. Perhaps they will have fun thinking
them through—or going on to develop some of the ideas further.
Richard P. Feynman
June, 1963
6","415"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","19","Foreword
A great triumph of twentieth-century physics, the theory of quantum mechan-
ics, is now nearly 40 years old, yet we have generally been giving our students
their introductory course in physics (for many students, their last) with hardly
more than a casual allusion to this central part of our knowledge of the physical
world. We should do better by them. These lectures are an attempt to present
them with the basic and essential ideas of the quantum mechanics in a way
that would, hopefully, be comprehensible. The approach you will ﬁnd here is
novel, particularly at the level of a sophomore course, and was considered very
much an experiment. After seeing how easily some of the students take to it,
however, I believe that the experiment was a success. There is, of course, room
for improvement, and it will come with more experience in the classroom. What
you will ﬁnd here is a record of that ﬁrst experiment.
In the two-year sequence of the Feynman Lectures on Physics which were
given from September 1961 through May 1963 for the introductory physics course
at Caltech, the concepts of quantum physics were brought in whenever they were
necessary for an understanding of the phenomena being described. In addition,
the last twelve lectures of the second year were given over to a more coherent
introduction to some of the concepts of quantum mechanics. It became clear
as the lectures drew to a close, however, that not enough time had been left
for the quantum mechanics. As the material was prepared, it was continually
discovered that other important and interesting topics could be treated with the
elementary tools that had been developed. There was also a fear that the too
brief treatment of the Schrödinger wave function which had been included in the
twelfth lecture would not provide a suﬃcient bridge to the more conventional
7","313"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","20","treatments of many books the students might hope to read. It was therefore
decided to extend the series with seven additional lectures; they were given to
the sophomore class in May of 1964. These lectures rounded out and extended
somewhat the material developed in the earlier lectures.
In this volume we have put together the lectures from both years with some
adjustment of the sequence. In addition, two lectures originally given to the
freshman class as an introduction to quantum physics have been lifted bodily
from Volume I (where they were Chapters 37 and 38) and placed as the ﬁrst two
chapters here—to make this volume a self-contained unit, relatively independent
of the ﬁrst two. A few ideas about the quantization of angular momentum
(including a discussion of the Stern-Gerlach experiment) had been introduced in
Chapters 34 and 35 of Volume II, and familiarity with them is assumed; for the
convenience of those who will not have that volume at hand, those two chapters
are reproduced here as an Appendix.
This set of lectures tries to elucidate from the beginning those features of the
quantum mechanics which are most basic and most general. The ﬁrst lectures
tackle head on the ideas of a probability amplitude, the interference of amplitudes,
the abstract notion of a state, and the superposition and resolution of states—
and the Dirac notation is used from the start. In each instance the ideas are
introduced together with a detailed discussion of some speciﬁc examples—to try
to make the physical ideas as real as possible. The time dependence of states
including states of deﬁnite energy comes next, and the ideas are applied at once
to the study of two-state systems. A detailed discussion of the ammonia maser
provides the frame-work for the introduction to radiation absorption and induced
transitions. The lectures then go on to consider more complex systems, leading to
a discussion of the propagation of electrons in a crystal, and to a rather complete
treatment of the quantum mechanics of angular momentum. Our introduction
to quantum mechanics ends in Chapter 20 with a discussion of the Schrödinger
wave function, its diﬀerential equation, and the solution for the hydrogen atom.
The last chapter of this volume is not intended to be a part of the “course.”
It is a “seminar” on superconductivity and was given in the spirit of some of the
entertainment lectures of the ﬁrst two volumes, with the intent of opening to the
students a broader view of the relation of what they were learning to the general
culture of physics. Feynman’s “epilogue” serves as the period to the three-volume
series.
As explained in the Foreword to Volume I, these lectures were but one aspect
of a program for the development of a new introductory course carried out at the
8","467"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","21","California Institute of Technology under the supervision of the Physics Course
Revision Committee (Robert Leighton, Victor Neher, and Matthew Sands). The
program was made possible by a grant from the Ford Foundation. Many people
helped with the technical details of the preparation of this volume: Marylou
Clayton, Julie Curcio, James Hartle, Tom Harvey, Martin Israel, Patricia Preuss,
Fanny Warren, and Barbara Zimmerman. Professors Gerry Neugebauer and
Charles Wilts contributed greatly to the accuracy and clarity of the material by
reviewing carefully much of the manuscript.
But the story of quantum mechanics you will ﬁnd here is Richard Feynman’s.
Our labors will have been well spent if we have been able to bring to others even
some of the intellectual excitement we experienced as we saw the ideas unfold in
his real-life Lectures on Physics.
Matthew Sands
December, 1964
9","139"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","22","Contents
Chapter 1.
Quantum Behavior
1-1
Atomic mechanics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1-1
1-2
An experiment with bullets . . . . . . . . . . . . . . . . . . . . . . . . . .
1-2
1-3
An experiment with waves . . . . . . . . . . . . . . . . . . . . . . . . . . .
1-4
1-4
An experiment with electrons . . . . . . . . . . . . . . . . . . . . . . . . .
1-6
1-5
The interference of electron waves
. . . . . . . . . . . . . . . . . . . . . .
1-8
1-6
Watching the electrons . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1-10
1-7
First principles of quantum mechanics . . . . . . . . . . . . . . . . . . . .
1-15
1-8
The uncertainty principle
. . . . . . . . . . . . . . . . . . . . . . . . . . .
1-17
Chapter 2.
The Relation of Wave and Particle Viewpoints
2-1
Probability wave amplitudes . . . . . . . . . . . . . . . . . . . . . . . . . .
2-1
2-2
Measurement of position and momentum
. . . . . . . . . . . . . . . . . .
2-3
2-3
Crystal diﬀraction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2-8
2-4
The size of an atom
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2-10
2-5
Energy levels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2-13
2-6
Philosophical implications . . . . . . . . . . . . . . . . . . . . . . . . . . .
2-15
Chapter 3.
Probability Amplitudes
3-1
The laws for combining amplitudes
. . . . . . . . . . . . . . . . . . . . .
3-1
3-2
The two-slit interference pattern
. . . . . . . . . . . . . . . . . . . . . . .
3-8
3-3
Scattering from a crystal . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3-12
3-4
Identical particles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3-16
10","596"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","23","Chapter 4.
Identical Particles
4-1
Bose particles and Fermi particles
. . . . . . . . . . . . . . . . . . . . . .
4-1
4-2
States with two Bose particles . . . . . . . . . . . . . . . . . . . . . . . . .
4-6
4-3
States with n Bose particles
. . . . . . . . . . . . . . . . . . . . . . . . .
4-10
4-4
Emission and absorption of photons
. . . . . . . . . . . . . . . . . . . . .
4-13
4-5
The blackbody spectrum . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4-15
4-6
Liquid helium . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4-22
4-7
The exclusion principle
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
4-23
Chapter 5.
Spin One
5-1
Filtering atoms with a Stern-Gerlach apparatus
. . . . . . . . . . . . . .
5-1
5-2
Experiments with ﬁltered atoms
. . . . . . . . . . . . . . . . . . . . . . .
5-9
5-3
Stern-Gerlach ﬁlters in series
. . . . . . . . . . . . . . . . . . . . . . . . .
5-11
5-4
Base states
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5-13
5-5
Interfering amplitudes . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5-16
5-6
The machinery of quantum mechanics . . . . . . . . . . . . . . . . . . . .
5-21
5-7
Transforming to a diﬀerent base
. . . . . . . . . . . . . . . . . . . . . . .
5-24
5-8
Other situations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5-27
Chapter 6.
Spin One-Half
6-1
Transforming amplitudes
. . . . . . . . . . . . . . . . . . . . . . . . . . .
6-1
6-2
Transforming to a rotated coordinate system
. . . . . . . . . . . . . . . .
6-4
6-3
Rotations about the z-axis
. . . . . . . . . . . . . . . . . . . . . . . . . .
6-10
6-4
Rotations of 180◦and 90◦about y . . . . . . . . . . . . . . . . . . . . . .
6-15
6-5
Rotations about x
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6-20
6-6
Arbitrary rotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6-22
Chapter 7.
The Dependence of Amplitudes on Time
7-1
Atoms at rest; stationary states . . . . . . . . . . . . . . . . . . . . . . . .
7-1
7-2
Uniform motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7-5
7-3
Potential energy; energy conservation
. . . . . . . . . . . . . . . . . . . .
7-9
7-4
Forces; the classical limit
. . . . . . . . . . . . . . . . . . . . . . . . . . .
7-16
7-5
The “precession” of a spin one-half particle
. . . . . . . . . . . . . . . . .
7-18
11","833"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","24","Chapter 8.
The Hamiltonian Matrix
8-1
Amplitudes and vectors
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
8-1
8-2
Resolving state vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8-4
8-3
What are the base states of the world?
. . . . . . . . . . . . . . . . . . .
8-8
8-4
How states change with time
. . . . . . . . . . . . . . . . . . . . . . . . .
8-11
8-5
The Hamiltonian matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8-16
8-6
The ammonia molecule
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
8-17
Chapter 9.
The Ammonia Maser
9-1
The states of an ammonia molecule
. . . . . . . . . . . . . . . . . . . . .
9-1
9-2
The molecule in a static electric ﬁeld
. . . . . . . . . . . . . . . . . . . .
9-7
9-3
Transitions in a time-dependent ﬁeld
. . . . . . . . . . . . . . . . . . . .
9-14
9-4
Transitions at resonance . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9-18
9-5
Transitions oﬀresonance . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9-21
9-6
The absorption of light . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9-23
Chapter 10.
Other Two-State Systems
10-1 The hydrogen molecular ion . . . . . . . . . . . . . . . . . . . . . . . . . .
10-1
10-2 Nuclear forces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10-10
10-3 The hydrogen molecule
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 10-13
10-4 The benzene molecule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10-17
10-5 Dyes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10-21
10-6 The Hamiltonian of a spin one-half particle in a magnetic ﬁeld
. . . . . . 10-22
10-7 The spinning electron in a magnetic ﬁeld
. . . . . . . . . . . . . . . . . . 10-26
Chapter 11.
More Two-State Systems
11-1 The Pauli spin matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11-1
11-2 The spin matrices as operators
. . . . . . . . . . . . . . . . . . . . . . . .
11-9
11-3 The solution of the two-state equations
. . . . . . . . . . . . . . . . . . . 11-14
11-4 The polarization states of the photon
. . . . . . . . . . . . . . . . . . . . 11-15
11-5 The neutral K-meson
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11-21
12","779"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","25","Chapter 12.
The Hyperfine Splitting in Hydrogen
12-1 Base states for a system with two spin one-half particles
. . . . . . . . .
12-1
12-2 The Hamiltonian for the ground state of hydrogen
. . . . . . . . . . . . .
12-4
12-3 The energy levels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12-12
12-4 The Zeeman splitting
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12-15
12-5 The states in a magnetic ﬁeld . . . . . . . . . . . . . . . . . . . . . . . . . 12-22
12-6 The projection matrix for spin one . . . . . . . . . . . . . . . . . . . . . . 12-26
Chapter 13.
Propagation in a Crystal Lattice
13-1 States for an electron in a one-dimensional lattice
. . . . . . . . . . . . .
13-1
13-2 States of deﬁnite energy . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13-5
13-3 Time-dependent states . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13-10
13-4 An electron in a three-dimensional lattice . . . . . . . . . . . . . . . . . . 13-12
13-5 Other states in a lattice . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13-14
13-6 Scattering from imperfections in the lattice
. . . . . . . . . . . . . . . . . 13-16
13-7 Trapping by a lattice imperfection
. . . . . . . . . . . . . . . . . . . . . . 13-20
13-8 Scattering amplitudes and bound states . . . . . . . . . . . . . . . . . . . 13-21
Chapter 14.
Semiconductors
14-1 Electrons and holes in semiconductors . . . . . . . . . . . . . . . . . . . .
14-1
14-2 Impure semiconductors
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
14-8
14-3 The Hall eﬀect
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14-12
14-4 Semiconductor junctions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14-15
14-5 Rectiﬁcation at a semiconductor junction
. . . . . . . . . . . . . . . . . . 14-19
14-6 The transistor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14-21
Chapter 15.
The Independent Particle Approximation
15-1 Spin waves
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15-1
15-2 Two spin waves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15-7
15-3 Independent particles
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15-9
15-4 The benzene molecule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15-11
15-5 More organic chemistry
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 15-18
15-6 Other uses of the approximation
. . . . . . . . . . . . . . . . . . . . . . . 15-24
13","832"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","26","Chapter 16.
The Dependence of Amplitudes on Position
16-1 Amplitudes on a line . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16-1
16-2 The wave function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16-7
16-3 States of deﬁnite momentum
. . . . . . . . . . . . . . . . . . . . . . . . . 16-10
16-4 Normalization of the states in x
. . . . . . . . . . . . . . . . . . . . . . . 16-14
16-5 The Schrödinger equation . . . . . . . . . . . . . . . . . . . . . . . . . . . 16-18
16-6 Quantized energy levels
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 16-23
Chapter 17.
Symmetry and Conservation Laws
17-1 Symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17-1
17-2 Symmetry and conservation . . . . . . . . . . . . . . . . . . . . . . . . . .
17-6
17-3 The conservation laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17-13
17-4 Polarized light
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17-17
17-5 The disintegration of the Λ0
. . . . . . . . . . . . . . . . . . . . . . . . . 17-21
17-6 Summary of the rotation matrices
. . . . . . . . . . . . . . . . . . . . . . 17-28
Chapter 18.
Angular Momentum
18-1 Electric dipole radiation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18-1
18-2 Light scattering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18-6
18-3 The annihilation of positronium . . . . . . . . . . . . . . . . . . . . . . . .
18-9
18-4 Rotation matrix for any spin
. . . . . . . . . . . . . . . . . . . . . . . . . 18-18
18-5 Measuring a nuclear spin
. . . . . . . . . . . . . . . . . . . . . . . . . . . 18-23
18-6 Composition of angular momentum
. . . . . . . . . . . . . . . . . . . . . 18-25
18-8 Added Note 2: Conservation of parity in photon emission
. . . . . . . . . 18-39
Chapter 19.
The Hydrogen Atom and The Periodic Table
19-1 Schrödinger’s equation for the hydrogen atom
. . . . . . . . . . . . . . .
19-1
19-2 Spherically symmetric solutions . . . . . . . . . . . . . . . . . . . . . . . .
19-4
19-3 States with an angular dependence
. . . . . . . . . . . . . . . . . . . . .
19-9
19-4 The general solution for hydrogen
. . . . . . . . . . . . . . . . . . . . . . 19-17
19-5 The hydrogen wave functions . . . . . . . . . . . . . . . . . . . . . . . . . 19-21
19-6 The periodic table
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19-25
14","817"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","27","Chapter 20.
Operators
20-1 Operations and operators . . . . . . . . . . . . . . . . . . . . . . . . . . .
20-1
20-2 Average energies
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20-5
20-3 The average energy of an atom . . . . . . . . . . . . . . . . . . . . . . . .
20-9
20-4 The position operator
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20-12
20-5 The momentum operator
. . . . . . . . . . . . . . . . . . . . . . . . . . . 20-15
20-6 Angular momentum
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20-22
20-7 The change of averages with time . . . . . . . . . . . . . . . . . . . . . . . 20-25
Chapter 21.
The Schrödinger Equation in a Classical Context: A Seminar
on Superconductivity
21-1 Schrödinger’s equation in a magnetic ﬁeld
. . . . . . . . . . . . . . . . .
21-1
21-2 The equation of continuity for probabilities
. . . . . . . . . . . . . . . . .
21-5
21-3 Two kinds of momentum . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21-7
21-4 The meaning of the wave function
. . . . . . . . . . . . . . . . . . . . . . 21-10
21-5 Superconductivity
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21-11
21-6 The Meissner eﬀect . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21-14
21-7 Flux quantization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21-16
21-8 The dynamics of superconductivity
. . . . . . . . . . . . . . . . . . . . . 21-22
21-9 The Josephson junction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 21-25
Feynman’s Epilogue
Appendix
Index
Name Index
List of Symbols
15","537"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","28","1
Quantum Behavior
Note: This chapter is almost exactly the same as Chapter 37 of Volume I.
1-1 Atomic mechanics
“Quantum mechanics” is the description of the behavior of matter and light
in all its details and, in particular, of the happenings on an atomic scale. Things
on a very small scale behave like nothing that you have any direct experience
about. They do not behave like waves, they do not behave like particles, they do
not behave like clouds, or billiard balls, or weights on springs, or like anything
that you have ever seen.
Newton thought that light was made up of particles, but then it was discovered
that it behaves like a wave. Later, however (in the beginning of the twentieth
century), it was found that light did indeed sometimes behave like a particle.
Historically, the electron, for example, was thought to behave like a particle, and
then it was found that in many respects it behaved like a wave. So it really
behaves like neither. Now we have given up. We say: “It is like neither.”
There is one lucky break, however—electrons behave just like light. The
quantum behavior of atomic objects (electrons, protons, neutrons, photons, and
so on) is the same for all, they are all “particle waves,” or whatever you want to
call them. So what we learn about the properties of electrons (which we shall use
for our examples) will apply also to all “particles,” including photons of light.
The gradual accumulation of information about atomic and small-scale be-
havior during the ﬁrst quarter of the 20th century, which gave some indications
about how small things do behave, produced an increasing confusion which was
ﬁnally resolved in 1926 and 1927 by Schrödinger, Heisenberg, and Born. They
ﬁnally obtained a consistent description of the behavior of matter on a small
scale. We take up the main features of that description in this chapter.
1-1","319"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","29","Because atomic behavior is so unlike ordinary experience, it is very diﬃcult
to get used to, and it appears peculiar and mysterious to everyone—both to the
novice and to the experienced physicist. Even the experts do not understand it
the way they would like to, and it is perfectly reasonable that they should not,
because all of direct, human experience and of human intuition applies to large
objects. We know how large objects will act, but things on a small scale just
do not act that way. So we have to learn about them in a sort of abstract or
imaginative fashion and not by connection with our direct experience.
In this chapter we shall tackle immediately the basic element of the mysterious
behavior in its most strange form. We choose to examine a phenomenon which is
impossible, absolutely impossible, to explain in any classical way, and which has
in it the heart of quantum mechanics. In reality, it contains the only mystery.
We cannot make the mystery go away by “explaining” how it works. We will just
tell you how it works. In telling you how it works we will have told you about
the basic peculiarities of all quantum mechanics.
1-2 An experiment with bullets
To try to understand the quantum behavior of electrons, we shall compare
and contrast their behavior, in a particular experimental setup, with the more
familiar behavior of particles like bullets, and with the behavior of waves like
water waves. We consider ﬁrst the behavior of bullets in the experimental setup
shown diagrammatically in Fig. 1-1. We have a machine gun that shoots a stream
GUN
(a)
WALL
1
2
x
MOVABLE
DETECTOR
BACKSTOP
(b)
P1
P2
(c)
P12
P12 = P1 + P2
Fig. 1-1. Interference experiment with bullets.
1-2","297"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","30","of bullets. It is not a very good gun, in that it sprays the bullets (randomly) over a
fairly large angular spread, as indicated in the ﬁgure. In front of the gun we have
a wall (made of armor plate) that has in it two holes just about big enough to let
a bullet through. Beyond the wall is a backstop (say a thick wall of wood) which
will “absorb” the bullets when they hit it. In front of the wall we have an object
which we shall call a “detector” of bullets. It might be a box containing sand. Any
bullet that enters the detector will be stopped and accumulated. When we wish,
we can empty the box and count the number of bullets that have been caught.
The detector can be moved back and forth (in what we will call the x-direction).
With this apparatus, we can ﬁnd out experimentally the answer to the question:
“What is the probability that a bullet which passes through the holes in the wall
will arrive at the backstop at the distance x from the center?” First, you should
realize that we should talk about probability, because we cannot say deﬁnitely
where any particular bullet will go. A bullet which happens to hit one of the
holes may bounce oﬀthe edges of the hole, and may end up anywhere at all. By
“probability” we mean the chance that the bullet will arrive at the detector, which
we can measure by counting the number which arrive at the detector in a certain
time and then taking the ratio of this number to the total number that hit the
backstop during that time. Or, if we assume that the gun always shoots at the
same rate during the measurements, the probability we want is just proportional
to the number that reach the detector in some standard time interval.
For our present purposes we would like to imagine a somewhat idealized
experiment in which the bullets are not real bullets, but are indestructible bullets—
they cannot break in half. In our experiment we ﬁnd that bullets always arrive in
lumps, and when we ﬁnd something in the detector, it is always one whole bullet.
If the rate at which the machine gun ﬁres is made very low, we ﬁnd that at any
given moment either nothing arrives, or one and only one—exactly one—bullet
arrives at the backstop. Also, the size of the lump certainly does not depend on
the rate of ﬁring of the gun. We shall say: “Bullets always arrive in identical
lumps.” What we measure with our detector is the probability of arrival of a
lump. And we measure the probability as a function of x. The result of such
measurements with this apparatus (we have not yet done the experiment, so we
are really imagining the result) are plotted in the graph drawn in part (c) of
Fig. 1-1. In the graph we plot the probability to the right and x vertically, so
that the x-scale ﬁts the diagram of the apparatus. We call the probability P12
because the bullets may have come either through hole 1 or through hole 2. You
will not be surprised that P12 is large near the middle of the graph but gets
1-3","547"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","31","small if x is very large. You may wonder, however, why P12 has its maximum
value at x = 0. We can understand this fact if we do our experiment again after
covering up hole 2, and once more while covering up hole 1. When hole 2 is
covered, bullets can pass only through hole 1, and we get the curve marked P1 in
part (b) of the ﬁgure. As you would expect, the maximum of P1 occurs at the
value of x which is on a straight line with the gun and hole 1. When hole 1 is
closed, we get the symmetric curve P2 drawn in the ﬁgure. P2 is the probability
distribution for bullets that pass through hole 2. Comparing parts (b) and (c) of
Fig. 1-1, we ﬁnd the important result that
P12 = P1 + P2.
(1.1)
The probabilities just add together. The eﬀect with both holes open is the sum
of the eﬀects with each hole open alone. We shall call this result an observation
of “no interference,” for a reason that you will see later. So much for bullets.
They come in lumps, and their probability of arrival shows no interference.
1-3 An experiment with waves
Now we wish to consider an experiment with water waves. The apparatus
is shown diagrammatically in Fig. 1-2. We have a shallow trough of water. A
small object labeled the “wave source” is jiggled up and down by a motor and
WAVE
SOURCE
(a)
WALL
1
2
DETECTOR
ABSORBER
(b)
x
I1
I2
I1 = |h1|2
I2 = |h2|2
(c)
x
I12
I12 = |h1 + h2|2
Fig. 1-2. Interference experiment with water waves.
1-4","278"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","32","makes circular waves. To the right of the source we have again a wall with two
holes, and beyond that is a second wall, which, to keep things simple, is an
“absorber,” so that there is no reﬂection of the waves that arrive there. This can
be done by building a gradual sand “beach.” In front of the beach we place a
detector which can be moved back and forth in the x-direction, as before. The
detector is now a device which measures the “intensity” of the wave motion. You
can imagine a gadget which measures the height of the wave motion, but whose
scale is calibrated in proportion to the square of the actual height, so that the
reading is proportional to the intensity of the wave. Our detector reads, then, in
proportion to the energy being carried by the wave—or rather, the rate at which
energy is carried to the detector.
With our wave apparatus, the ﬁrst thing to notice is that the intensity can have
any size. If the source just moves a very small amount, then there is just a little bit
of wave motion at the detector. When there is more motion at the source, there
is more intensity at the detector. The intensity of the wave can have any value
at all. We would not say that there was any “lumpiness” in the wave intensity.
Now let us measure the wave intensity for various values of x (keeping the
wave source operating always in the same way). We get the interesting-looking
curve marked I12 in part (c) of the ﬁgure.
We have already worked out how such patterns can come about when we
studied the interference of electric waves in Volume I. In this case we would
observe that the original wave is diﬀracted at the holes, and new circular waves
spread out from each hole. If we cover one hole at a time and measure the
intensity distribution at the absorber we ﬁnd the rather simple intensity curves
shown in part (b) of the ﬁgure. I1 is the intensity of the wave from hole 1 (which
we ﬁnd by measuring when hole 2 is blocked oﬀ) and I2 is the intensity of the
wave from hole 2 (seen when hole 1 is blocked).
The intensity I12 observed when both holes are open is certainly not the sum
of I1 and I2. We say that there is “interference” of the two waves. At some places
(where the curve I12 has its maxima) the waves are “in phase” and the wave
peaks add together to give a large amplitude and, therefore, a large intensity.
We say that the two waves are “interfering constructively” at such places. There
will be such constructive interference wherever the distance from the detector to
one hole is a whole number of wavelengths larger (or shorter) than the distance
from the detector to the other hole.
At those places where the two waves arrive at the detector with a phase
diﬀerence of π (where they are “out of phase”) the resulting wave motion at
1-5","514"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","33","the detector will be the diﬀerence of the two amplitudes. The waves “interfere
destructively,” and we get a low value for the wave intensity. We expect such low
values wherever the distance between hole 1 and the detector is diﬀerent from the
distance between hole 2 and the detector by an odd number of half-wavelengths.
The low values of I12 in Fig. 1-2 correspond to the places where the two waves
interfere destructively.
You will remember that the quantitative relationship between I1, I2, and I12
can be expressed in the following way: The instantaneous height of the water
wave at the detector for the wave from hole 1 can be written as (the real part
of) h1eiωt, where the “amplitude” h1 is, in general, a complex number. The
intensity is proportional to the mean squared height or, when we use the complex
numbers, to the absolute value squared |h1|2. Similarly, for hole 2 the height
is h2eiωt and the intensity is proportional to |h2|2. When both holes are open,
the wave heights add to give the height (h1 + h2)eiωt and the intensity |h1 + h2|2.
Omitting the constant of proportionality for our present purposes, the proper
relations for interfering waves are
I1 = |h1|2,
I2 = |h2|2,
I12 = |h1 + h2|2.
(1.2)
You will notice that the result is quite diﬀerent from that obtained with
bullets (Eq. 1.1). If we expand |h1 + h2|2 we see that
|h1 + h2|2 = |h1|2 + |h2|2 + 2|h1||h2| cos δ,
(1.3)
where δ is the phase diﬀerence between h1 and h2. In terms of the intensities,
we could write
I12 = I1 + I2 + 2
p
I1I2 cos δ.
(1.4)
The last term in (1.4) is the “interference term.” So much for water waves. The
intensity can have any value, and it shows interference.
1-4 An experiment with electrons
Now we imagine a similar experiment with electrons. It is shown diagram-
matically in Fig. 1-3. We make an electron gun which consists of a tungsten wire
heated by an electric current and surrounded by a metal box with a hole in it. If
the wire is at a negative voltage with respect to the box, electrons emitted by
the wire will be accelerated toward the walls and some will pass through the hole.
1-6","385"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","34","ELECTRON
GUN
(a)
WALL
1
2
DETECTOR
BACKSTOP
(b)
x
P1
P2
P1 = |φ1|2
P2 = |φ2|2
(c)
x
P12
P12 = |φ1 + φ2|2
Fig. 1-3. Interference experiment with electrons.
All the electrons which come out of the gun will have (nearly) the same energy.
In front of the gun is again a wall (just a thin metal plate) with two holes in it.
Beyond the wall is another plate which will serve as a “backstop.” In front of the
backstop we place a movable detector. The detector might be a geiger counter
or, perhaps better, an electron multiplier, which is connected to a loudspeaker.
We should say right away that you should not try to set up this experiment (as
you could have done with the two we have already described). This experiment
has never been done in just this way. The trouble is that the apparatus would
have to be made on an impossibly small scale to show the eﬀects we are interested
in. We are doing a “thought experiment,” which we have chosen because it is easy
to think about. We know the results that would be obtained because there are
many experiments that have been done, in which the scale and the proportions
have been chosen to show the eﬀects we shall describe.
The ﬁrst thing we notice with our electron experiment is that we hear sharp
“clicks” from the detector (that is, from the loudspeaker). And all “clicks” are
the same. There are no “half-clicks.”
We would also notice that the “clicks” come very erratically. Something like:
click . . . . . click-click . . . click . . . . . . . . click . . . . click-click . . . . . . click . . . , etc.,
just as you have, no doubt, heard a geiger counter operating. If we count the
clicks which arrive in a suﬃciently long time—say for many minutes—and then
count again for another equal period, we ﬁnd that the two numbers are very
1-7","343"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","35","nearly the same. So we can speak of the average rate at which the clicks are
heard (so-and-so-many clicks per minute on the average).
As we move the detector around, the rate at which the clicks appear is faster
or slower, but the size (loudness) of each click is always the same. If we lower
the temperature of the wire in the gun, the rate of clicking slows down, but still
each click sounds the same. We would notice also that if we put two separate
detectors at the backstop, one or the other would click, but never both at once.
(Except that once in a while, if there were two clicks very close together in time,
our ear might not sense the separation.) We conclude, therefore, that whatever
arrives at the backstop arrives in “lumps.” All the “lumps” are the same size:
only whole “lumps” arrive, and they arrive one at a time at the backstop. We
shall say: “Electrons always arrive in identical lumps.”
Just as for our experiment with bullets, we can now proceed to ﬁnd exper-
imentally the answer to the question: “What is the relative probability that
an electron ‘lump’ will arrive at the backstop at various distances x from the
center?” As before, we obtain the relative probability by observing the rate of
clicks, holding the operation of the gun constant. The probability that lumps
will arrive at a particular x is proportional to the average rate of clicks at that x.
The result of our experiment is the interesting curve marked P12 in part (c)
of Fig. 1-3. Yes! That is the way electrons go.
1-5 The interference of electron waves
Now let us try to analyze the curve of Fig. 1-3 to see whether we can understand
the behavior of the electrons. The ﬁrst thing we would say is that since they
come in lumps, each lump, which we may as well call an electron, has come either
through hole 1 or through hole 2. Let us write this in the form of a “Proposition”:
Proposition A: Each electron either goes through hole 1 or it goes through
hole 2.
Assuming Proposition A, all electrons that arrive at the backstop can be
divided into two classes: (1) those that come through hole 1, and (2) those that
come through hole 2. So our observed curve must be the sum of the eﬀects of
the electrons which come through hole 1 and the electrons which come through
hole 2. Let us check this idea by experiment. First, we will make a measurement
for those electrons that come through hole 1. We block oﬀhole 2 and make
our counts of the clicks from the detector. From the clicking rate, we get P1.
1-8","459"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","36","The result of the measurement is shown by the curve marked P1 in part (b) of
Fig. 1-3. The result seems quite reasonable. In a similar way, we measure P2, the
probability distribution for the electrons that come through hole 2. The result of
this measurement is also drawn in the ﬁgure.
The result P12 obtained with both holes open is clearly not the sum of P1
and P2, the probabilities for each hole alone. In analogy with our water-wave
experiment, we say: “There is interference.”
For electrons:
P12 ̸= P1 + P2.
(1.5)
How can such an interference come about? Perhaps we should say: “Well,
that means, presumably, that it is not true that the lumps go either through
hole 1 or hole 2, because if they did, the probabilities should add. Perhaps they
go in a more complicated way. They split in half and . . . ” But no! They cannot,
they always arrive in lumps . . . “Well, perhaps some of them go through 1, and
then they go around through 2, and then around a few more times, or by some
other complicated path . . . then by closing hole 2, we changed the chance that
an electron that started out through hole 1 would ﬁnally get to the backstop . . . ”
But notice! There are some points at which very few electrons arrive when both
holes are open, but which receive many electrons if we close one hole, so closing
one hole increased the number from the other. Notice, however, that at the
center of the pattern, P12 is more than twice as large as P1 + P2. It is as though
closing one hole decreased the number of electrons which come through the other
hole. It seems hard to explain both eﬀects by proposing that the electrons travel
in complicated paths.
It is all quite mysterious. And the more you look at it the more mysterious
it seems. Many ideas have been concocted to try to explain the curve for P12
in terms of individual electrons going around in complicated ways through the
holes. None of them has succeeded. None of them can get the right curve for P12
in terms of P1 and P2.
Yet, surprisingly enough, the mathematics for relating P1 and P2 to P12 is
extremely simple. For P12 is just like the curve I12 of Fig. 1-2, and that was simple.
What is going on at the backstop can be described by two complex numbers that
we can call φ1 and φ2 (they are functions of x, of course). The absolute square
of φ1 gives the eﬀect with only hole 1 open. That is, P1 = |φ1|2. The eﬀect with
only hole 2 open is given by φ2 in the same way. That is, P2 = |φ2|2. And the
combined eﬀect of the two holes is just P12 = |φ1 + φ2|2. The mathematics is the
1-9","492"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","37","same as that we had for the water waves! (It is hard to see how one could get
such a simple result from a complicated game of electrons going back and forth
through the plate on some strange trajectory.)
We conclude the following: The electrons arrive in lumps, like particles, and
the probability of arrival of these lumps is distributed like the distribution of
intensity of a wave. It is in this sense that an electron behaves “sometimes like a
particle and sometimes like a wave.”
Incidentally, when we were dealing with classical waves we deﬁned the intensity
as the mean over time of the square of the wave amplitude, and we used complex
numbers as a mathematical trick to simplify the analysis.
But in quantum
mechanics it turns out that the amplitudes must be represented by complex
numbers. The real parts alone will not do. That is a technical point, for the
moment, because the formulas look just the same.
Since the probability of arrival through both holes is given so simply, although
it is not equal to (P1 + P2), that is really all there is to say. But there are a
large number of subtleties involved in the fact that nature does work this way.
We would like to illustrate some of these subtleties for you now. First, since the
number that arrives at a particular point is not equal to the number that arrives
through 1 plus the number that arrives through 2, as we would have concluded
from Proposition A, undoubtedly we should conclude that Proposition A is false.
It is not true that the electrons go either through hole 1 or hole 2. But that
conclusion can be tested by another experiment.
1-6 Watching the electrons
We shall now try the following experiment. To our electron apparatus we
add a very strong light source, placed behind the wall and between the two holes,
as shown in Fig. 1-4. We know that electric charges scatter light. So when an
electron passes, however it does pass, on its way to the detector, it will scatter
some light to our eye, and we can see where the electron goes. If, for instance, an
electron were to take the path via hole 2 that is sketched in Fig. 1-4, we should
see a ﬂash of light coming from the vicinity of the place marked A in the ﬁgure. If
an electron passes through hole 1, we would expect to see a ﬂash from the vicinity
of the upper hole. If it should happen that we get light from both places at the
same time, because the electron divides in half . . . Let us just do the experiment!
Here is what we see: every time that we hear a “click” from our electron
detector (at the backstop), we also see a ﬂash of light either near hole 1 or near
1-10","484"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","38","ELECTRON
GUN
(a)
1
2
LIGHT
SOURCE
A
(b)
x
P ′
1
P ′
2
(c)
x
P ′
12
P ′
12 = P ′
1 + P ′
2
Fig. 1-4. A diﬀerent electron experiment.
hole 2, but never both at once! And we observe the same result no matter where
we put the detector. From this observation we conclude that when we look at
the electrons we ﬁnd that the electrons go either through one hole or the other.
Experimentally, Proposition A is necessarily true.
What, then, is wrong with our argument against Proposition A? Why isn’t P12
just equal to P1 + P2? Back to experiment! Let us keep track of the electrons
and ﬁnd out what they are doing. For each position (x-location) of the detector
we will count the electrons that arrive and also keep track of which hole they
went through, by watching for the ﬂashes. We can keep track of things this
way: whenever we hear a “click” we will put a count in Column 1 if we see the
ﬂash near hole 1, and if we see the ﬂash near hole 2, we will record a count
in Column 2. Every electron which arrives is recorded in one of two classes:
those which come through 1 and those which come through 2. From the number
recorded in Column 1 we get the probability P ′
1 that an electron will arrive at
the detector via hole 1; and from the number recorded in Column 2 we get P ′
2,
the probability that an electron will arrive at the detector via hole 2. If we now
repeat such a measurement for many values of x, we get the curves for P ′
1 and P ′
2
shown in part (b) of Fig. 1-4.
Well, that is not too surprising! We get for P ′
1 something quite similar to
what we got before for P1 by blocking oﬀhole 2; and P ′
2 is similar to what we got
by blocking hole 1. So there is not any complicated business like going through
both holes. When we watch them, the electrons come through just as we would
1-11","369"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","39","expect them to come through. Whether the holes are closed or open, those which
we see come through hole 1 are distributed in the same way whether hole 2 is
open or closed.
But wait! What do we have now for the total probability, the probability
that an electron will arrive at the detector by any route? We already have that
information. We just pretend that we never looked at the light ﬂashes, and we
lump together the detector clicks which we have separated into the two columns.
We must just add the numbers. For the probability that an electron will arrive
at the backstop by passing through either hole, we do ﬁnd P ′
12 = P ′
1 + P ′
2. That
is, although we succeeded in watching which hole our electrons come through,
we no longer get the old interference curve P12, but a new one, P ′
12, showing no
interference! If we turn out the light P12 is restored.
We must conclude that when we look at the electrons the distribution of them
on the screen is diﬀerent than when we do not look. Perhaps it is turning on
our light source that disturbs things? It must be that the electrons are very
delicate, and the light, when it scatters oﬀthe electrons, gives them a jolt that
changes their motion. We know that the electric ﬁeld of the light acting on a
charge will exert a force on it. So perhaps we should expect the motion to be
changed. Anyway, the light exerts a big inﬂuence on the electrons. By trying to
“watch” the electrons we have changed their motions. That is, the jolt given to
the electron when the photon is scattered by it is such as to change the electron’s
motion enough so that if it might have gone to where P12 was at a maximum it
will instead land where P12 was a minimum; that is why we no longer see the
wavy interference eﬀects.
You may be thinking: “Don’t use such a bright source! Turn the brightness
down! The light waves will then be weaker and will not disturb the electrons so
much. Surely, by making the light dimmer and dimmer, eventually the wave will
be weak enough that it will have a negligible eﬀect.” O.K. Let’s try it. The ﬁrst
thing we observe is that the ﬂashes of light scattered from the electrons as they
pass by does not get weaker. It is always the same-sized ﬂash. The only thing
that happens as the light is made dimmer is that sometimes we hear a “click”
from the detector but see no ﬂash at all. The electron has gone by without being
“seen.” What we are observing is that light also acts like electrons, we knew that
it was “wavy,” but now we ﬁnd that it is also “lumpy.” It always arrives—or is
scattered—in lumps that we call “photons.” As we turn down the intensity of
the light source we do not change the size of the photons, only the rate at which
they are emitted. That explains why, when our source is dim, some electrons get
1-12","528"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","40","by without being seen. There did not happen to be a photon around at the time
the electron went through.
This is all a little discouraging. If it is true that whenever we “see” the electron
we see the same-sized ﬂash, then those electrons we see are always the disturbed
ones. Let us try the experiment with a dim light anyway. Now whenever we
hear a click in the detector we will keep a count in three columns: in Column (1)
those electrons seen by hole 1, in Column (2) those electrons seen by hole 2,
and in Column (3) those electrons not seen at all. When we work up our data
(computing the probabilities) we ﬁnd these results: Those “seen by hole 1” have
a distribution like P ′
1; those “seen by hole 2” have a distribution like P ′
2 (so that
those “seen by either hole 1 or 2” have a distribution like P ′
12); and those “not
seen at all” have a “wavy” distribution just like P12 of Fig. 1-3! If the electrons
are not seen, we have interference!
That is understandable. When we do not see the electron, no photon disturbs
it, and when we do see it, a photon has disturbed it. There is always the same
amount of disturbance because the light photons all produce the same-sized
eﬀects and the eﬀect of the photons being scattered is enough to smear out any
interference eﬀect.
Is there not some way we can see the electrons without disturbing them?
We learned in an earlier chapter that the momentum carried by a “photon” is
inversely proportional to its wavelength (p = h/λ). Certainly the jolt given
to the electron when the photon is scattered toward our eye depends on the
momentum that photon carries. Aha! If we want to disturb the electrons only
slightly we should not have lowered the intensity of the light, we should have
lowered its frequency (the same as increasing its wavelength). Let us use light of
a redder color. We could even use infrared light, or radiowaves (like radar), and
“see” where the electron went with the help of some equipment that can “see”
light of these longer wavelengths. If we use “gentler” light perhaps we can avoid
disturbing the electrons so much.
Let us try the experiment with longer waves. We shall keep repeating our
experiment, each time with light of a longer wavelength. At ﬁrst, nothing seems to
change. The results are the same. Then a terrible thing happens. You remember
that when we discussed the microscope we pointed out that, due to the wave
nature of the light, there is a limitation on how close two spots can be and still
be seen as two separate spots. This distance is of the order of the wavelength of
light. So now, when we make the wavelength longer than the distance between
our holes, we see a big fuzzy ﬂash when the light is scattered by the electrons.
1-13","501"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","41","We can no longer tell which hole the electron went through! We just know it went
somewhere! And it is just with light of this color that we ﬁnd that the jolts given
to the electron are small enough so that P ′
12 begins to look like P12—that we
begin to get some interference eﬀect. And it is only for wavelengths much longer
than the separation of the two holes (when we have no chance at all of telling
where the electron went) that the disturbance due to the light gets suﬃciently
small that we again get the curve P12 shown in Fig. 1-3.
In our experiment we ﬁnd that it is impossible to arrange the light in such
a way that one can tell which hole the electron went through, and at the same
time not disturb the pattern. It was suggested by Heisenberg that the then
new laws of nature could only be consistent if there were some basic limitation
on our experimental capabilities not previously recognized. He proposed, as a
general principle, his uncertainty principle, which we can state in terms of our
experiment as follows: “It is impossible to design an apparatus to determine
which hole the electron passes through, that will not at the same time disturb the
electrons enough to destroy the interference pattern.” If an apparatus is capable
of determining which hole the electron goes through, it cannot be so delicate
that it does not disturb the pattern in an essential way. No one has ever found
(or even thought of) a way around the uncertainty principle. So we must assume
that it describes a basic characteristic of nature.
The complete theory of quantum mechanics which we now use to describe
atoms and, in fact, all matter, depends on the correctness of the uncertainty
principle. Since quantum mechanics is such a successful theory, our belief in
the uncertainty principle is reinforced. But if a way to “beat” the uncertainty
principle were ever discovered, quantum mechanics would give inconsistent results
and would have to be discarded as a valid theory of nature.
“Well,” you say, “what about Proposition A? Is it true, or is it not true,
that the electron either goes through hole 1 or it goes through hole 2?” The only
answer that can be given is that we have found from experiment that there is
a certain special way that we have to think in order that we do not get into
inconsistencies. What we must say (to avoid making wrong predictions) is the
following. If one looks at the holes or, more accurately, if one has a piece of
apparatus which is capable of determining whether the electrons go through
hole 1 or hole 2, then one can say that it goes either through hole 1 or hole 2. But,
when one does not try to tell which way the electron goes, when there is nothing
in the experiment to disturb the electrons, then one may not say that an electron
goes either through hole 1 or hole 2. If one does say that, and starts to make any
1-14","521"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","42","deductions from the statement, he will make errors in the analysis. This is the
logical tightrope on which we must walk if we wish to describe nature successfully.
If the motion of all matter—as well as electrons—must be described in terms
of waves, what about the bullets in our ﬁrst experiment? Why didn’t we see an
interference pattern there? It turns out that for the bullets the wavelengths were
so tiny that the interference patterns became very ﬁne. So ﬁne, in fact, that with
any detector of ﬁnite size one could not distinguish the separate maxima and
minima. What we saw was only a kind of average, which is the classical curve.
In Fig. 1-5 we have tried to indicate schematically what happens with large-scale
objects. Part (a) of the ﬁgure shows the probability distribution one might
predict for bullets, using quantum mechanics. The rapid wiggles are supposed to
represent the interference pattern one gets for waves of very short wavelength.
Any physical detector, however, straddles several wiggles of the probability curve,
so that the measurements show the smooth curve drawn in part (b) of the ﬁgure.
(a)
P12
(b)
P12 (smoothed)
x
Fig. 1-5. Interference pattern with bullets: (a) actual (schematic),
(b) observed.
1-7 First principles of quantum mechanics
We will now write a summary of the main conclusions of our experiments. We
will, however, put the results in a form which makes them true for a general class
of such experiments. We can write our summary more simply if we ﬁrst deﬁne
1-15","255"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","43","an “ideal experiment” as one in which there are no uncertain external inﬂuences,
i.e., no jiggling or other things going on that we cannot take into account. We
would be quite precise if we said: “An ideal experiment is one in which all of the
initial and ﬁnal conditions of the experiment are completely speciﬁed.” What we
will call “an event” is, in general, just a speciﬁc set of initial and ﬁnal conditions.
(For example: “an electron leaves the gun, arrives at the detector, and nothing
else happens.”) Now for our summary.
Summary
(1) The probability of an event in an ideal experiment is given by the square
of the absolute value of a complex number φ which is called the probability
amplitude:
P = probability,
φ = probability amplitude,
P = |φ|2.
(1.6)
(2) When an event can occur in several alternative ways, the probability
amplitude for the event is the sum of the probability amplitudes for each
way considered separately. There is interference:
φ = φ1 + φ2,
P = |φ1 + φ2|2.
(1.7)
(3) If an experiment is performed which is capable of determining whether one
or another alternative is actually taken, the probability of the event is the
sum of the probabilities for each alternative. The interference is lost:
P = P1 + P2.
(1.8)
One might still like to ask: “How does it work? What is the machinery
behind the law?” No one has found any machinery behind the law. No one can
“explain” any more than we have just “explained.” No one will give you any deeper
representation of the situation. We have no ideas about a more basic mechanism
from which these results can be deduced.
We would like to emphasize a very important diﬀerence between classical and
quantum mechanics. We have been talking about the probability that an electron
1-16","308"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","44","will arrive in a given circumstance. We have implied that in our experimental
arrangement (or even in the best possible one) it would be impossible to predict
exactly what would happen. We can only predict the odds! This would mean, if
it were true, that physics has given up on the problem of trying to predict exactly
what will happen in a deﬁnite circumstance. Yes! physics has given up. We do
not know how to predict what would happen in a given circumstance, and we
believe now that it is impossible—that the only thing that can be predicted is the
probability of diﬀerent events. It must be recognized that this is a retrenchment
in our earlier ideal of understanding nature. It may be a backward step, but no
one has seen a way to avoid it.
We make now a few remarks on a suggestion that has sometimes been made
to try to avoid the description we have given: “Perhaps the electron has some
kind of internal works—some inner variables—that we do not yet know about.
Perhaps that is why we cannot predict what will happen. If we could look more
closely at the electron, we could be able to tell where it would end up.” So far as
we know, that is impossible. We would still be in diﬃculty. Suppose we were to
assume that inside the electron there is some kind of machinery that determines
where it is going to end up. That machine must also determine which hole it is
going to go through on its way. But we must not forget that what is inside the
electron should not be dependent on what we do, and in particular upon whether
we open or close one of the holes. So if an electron, before it starts, has already
made up its mind (a) which hole it is going to use, and (b) where it is going to
land, we should ﬁnd P1 for those electrons that have chosen hole 1, P2 for those
that have chosen hole 2, and necessarily the sum P1 + P2 for those that arrive
through the two holes. There seems to be no way around this. But we have
veriﬁed experimentally that that is not the case. And no one has ﬁgured a way
out of this puzzle. So at the present time we must limit ourselves to computing
probabilities. We say “at the present time,” but we suspect very strongly that
it is something that will be with us forever—that it is impossible to beat that
puzzle—that this is the way nature really is.
1-8 The uncertainty principle
This is the way Heisenberg stated the uncertainty principle originally: If you
make the measurement on any object, and you can determine the x-component
of its momentum with an uncertainty ∆p, you cannot, at the same time, know its
x-position more accurately than ∆x ≥ℏ/2∆p, where ℏis a deﬁnite ﬁxed number
1-17","489"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","45","given by nature. It is called the “reduced Planck constant,” and is approximately
1.05 × 10−34 joule-seconds. The uncertainties in the position and momentum of
a particle at any instant must have their product greater than half the reduced
Planck constant. This is a special case of the uncertainty principle that was
stated above more generally. The more general statement was that one cannot
design equipment in any way to determine which of two alternatives is taken,
without, at the same time, destroying the pattern of interference.
ELECTRON
GUN
MOTION FREE
WALL
ROLLERS
ROLLERS
1
2
DETECTOR
pa
pb
∆px
pa
pb
∆px
BACKSTOP
Fig. 1-6. An experiment in which the recoil of the wall is measured.
Let us show for one particular case that the kind of relation given by Heisenberg
must be true in order to keep from getting into trouble. We imagine a modiﬁcation
of the experiment of Fig. 1-3, in which the wall with the holes consists of a plate
mounted on rollers so that it can move freely up and down (in the x-direction),
as shown in Fig. 1-6. By watching the motion of the plate carefully we can
try to tell which hole an electron goes through. Imagine what happens when
the detector is placed at x = 0.
We would expect that an electron which
passes through hole 1 must be deﬂected downward by the plate to reach the
detector. Since the vertical component of the electron momentum is changed,
the plate must recoil with an equal momentum in the opposite direction. The
plate will get an upward kick. If the electron goes through the lower hole, the
plate should feel a downward kick. It is clear that for every position of the
detector, the momentum received by the plate will have a diﬀerent value for
a traversal via hole 1 than for a traversal via hole 2. So! Without disturbing
the electrons at all, but just by watching the plate, we can tell which path the
electron used.
1-18","337"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","46","Now in order to do this it is necessary to know what the momentum of the
screen is, before the electron goes through. So when we measure the momentum
after the electron goes by, we can ﬁgure out how much the plate’s momentum has
changed. But remember, according to the uncertainty principle we cannot at the
same time know the position of the plate with an arbitrary accuracy. But if we do
not know exactly where the plate is, we cannot say precisely where the two holes
are. They will be in a diﬀerent place for every electron that goes through. This
means that the center of our interference pattern will have a diﬀerent location for
each electron. The wiggles of the interference pattern will be smeared out. We
shall show quantitatively in the next chapter that if we determine the momentum
of the plate suﬃciently accurately to determine from the recoil measurement
which hole was used, then the uncertainty in the x-position of the plate will,
according to the uncertainty principle, be enough to shift the pattern observed at
the detector up and down in the x-direction about the distance from a maximum
to its nearest minimum. Such a random shift is just enough to smear out the
pattern so that no interference is observed.
The uncertainty principle “protects” quantum mechanics. Heisenberg rec-
ognized that if it were possible to measure the momentum and the position
simultaneously with a greater accuracy, the quantum mechanics would collapse.
So he proposed that it must be impossible. Then people sat down and tried to
ﬁgure out ways of doing it, and nobody could ﬁgure out a way to measure the
position and the momentum of anything—a screen, an electron, a billiard ball,
anything—with any greater accuracy. Quantum mechanics maintains its perilous
but still correct existence.
1-19","304"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","47","2
The Relation of Wave and
Particle Viewpoints
Note: This chapter is almost exactly the same as Chapter 38 of Volume I.
2-1 Probability wave amplitudes
In this chapter we shall discuss the relationship of the wave and particle
viewpoints. We already know, from the last chapter, that neither the wave
viewpoint nor the particle viewpoint is correct. We would always like to present
things accurately, or at least precisely enough that they will not have to be
changed when we learn more—it may be extended, but it will not be changed!
But when we try to talk about the wave picture or the particle picture, both
are approximate, and both will change. Therefore what we learn in this chapter
will not be accurate in a certain sense; we will deal with some half-intuitive
arguments which will be made more precise later. But certain things will be
changed a little bit when we interpret them correctly in quantum mechanics. We
are doing this so that you can have some qualitative feeling for some quantum
phenomena before we get into the mathematical details of quantum mechanics.
Furthermore, all our experiences are with waves and with particles, and so it is
rather handy to use the wave and particle ideas to get some understanding of
what happens in given circumstances before we know the complete mathematics
of the quantum-mechanical amplitudes. We shall try to indicate the weakest
places as we go along, but most of it is very nearly correct—it is just a matter of
interpretation.
First of all, we know that the new way of representing the world in quantum
mechanics—the new framework—is to give an amplitude for every event that can
occur, and if the event involves the reception of one particle, then we can give
the amplitude to ﬁnd that one particle at diﬀerent places and at diﬀerent times.
2-1","311"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","48","The probability of ﬁnding the particle is then proportional to the absolute square
of the amplitude. In general, the amplitude to ﬁnd a particle in diﬀerent places
at diﬀerent times varies with position and time.
In some special case it can be that the amplitude varies sinusoidally in space
and time like ei(ωt−k·r), where r is the vector position from some origin. (Do not
forget that these amplitudes are complex numbers, not real numbers.) Such an
amplitude varies according to a deﬁnite frequency ω and wave number k. Then
it turns out that this corresponds to a classical limiting situation where we would
have believed that we have a particle whose energy E was known and is related
to the frequency by
E = ℏω,
(2.1)
and whose momentum p is also known and is related to the wave number by
p = ℏk.
(2.2)
(The symbol ℏrepresents the number h divided by 2π; ℏ= h/2π.)
This means that the idea of a particle is limited. The idea of a particle—
its location, its momentum, etc.—which we use so much, is in certain ways
unsatisfactory. For instance, if an amplitude to ﬁnd a particle at diﬀerent places
is given by ei(ωt−k·r), whose absolute square is a constant, that would mean that
the probability of ﬁnding a particle is the same at all points. That means we do
not know where it is—it can be anywhere—there is a great uncertainty in its
location.
On the other hand, if the position of a particle is more or less well known and
we can predict it fairly accurately, then the probability of ﬁnding it in diﬀerent
places must be conﬁned to a certain region, whose length we call ∆x. Outside
this region, the probability is zero. Now this probability is the absolute square
of an amplitude, and if the absolute square is zero, the amplitude is also zero,
so that we have a wave train whose length is ∆x (Fig. 2-1), and the wavelength
∆x
Fig. 2-1. A wave packet of length ∆x.
2-2","341"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","49","(the distance between nodes of the waves in the train) of that wave train is what
corresponds to the particle momentum.
Here we encounter a strange thing about waves; a very simple thing which
has nothing to do with quantum mechanics strictly. It is something that anybody
who works with waves, even if he knows no quantum mechanics, knows: namely,
we cannot deﬁne a unique wavelength for a short wave train. Such a wave train
does not have a deﬁnite wavelength; there is an indeﬁniteness in the wave number
that is related to the ﬁnite length of the train, and thus there is an indeﬁniteness
in the momentum.
2-2 Measurement of position and momentum
Let us consider two examples of this idea—to see the reason that there is an
uncertainty in the position and/or the momentum, if quantum mechanics is right.
We have also seen before that if there were not such a thing—if it were possible
to measure the position and the momentum of anything simultaneously—we
would have a paradox; it is fortunate that we do not have such a paradox, and
the fact that such an uncertainty comes naturally from the wave picture shows
that everything is mutually consistent.
Here is one example which shows the relationship between the position and
the momentum in a circumstance that is easy to understand. Suppose we have a
single slit, and particles are coming from very far away with a certain energy—so
that they are all coming essentially horizontally (Fig. 2-2). We are going to
concentrate on the vertical components of momentum. All of these particles have
a certain horizontal momentum p0, say, in a classical sense. So, in the classical
sense, the vertical momentum py, before the particle goes through the hole, is
B
C
∆θ
Fig. 2-2. Diﬀraction of particles passing through a slit.
2-3","306"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","50","deﬁnitely known. The particle is moving neither up nor down, because it came
from a source that is far away—and so the vertical momentum is of course zero.
But now let us suppose that it goes through a hole whose width is B. Then
after it has come out through the hole, we know the position vertically—the
y-position—with considerable accuracy—namely ±B.† That is, the uncertainty
in position, ∆y, is of order B. Now we might also want to say, since we known the
momentum is absolutely horizontal, that ∆py is zero; but that is wrong. We once
knew the momentum was horizontal, but we do not know it any more. Before the
particles passed through the hole, we did not know their vertical positions. Now
that we have found the vertical position by having the particle come through the
hole, we have lost our information on the vertical momentum! Why? According
to the wave theory, there is a spreading out, or diﬀraction, of the waves after
they go through the slit, just as for light. Therefore there is a certain probability
that particles coming out of the slit are not coming exactly straight. The pattern
is spread out by the diﬀraction eﬀect, and the angle of spread, which we can
deﬁne as the angle of the ﬁrst minimum, is a measure of the uncertainty in the
ﬁnal angle.
How does the pattern become spread? To say it is spread means that there
is some chance for the particle to be moving up or down, that is, to have a
component of momentum up or down. We say chance and particle because we
can detect this diﬀraction pattern with a particle counter, and when the counter
receives the particle, say at C in Fig. 2-2, it receives the entire particle, so that,
in a classical sense, the particle has a vertical momentum, in order to get from
the slit up to C.
To get a rough idea of the spread of the momentum, the vertical momentum py
has a spread which is equal to p0 ∆θ, where p0 is the horizontal momentum.
And how big is ∆θ in the spread-out pattern? We know that the ﬁrst minimum
occurs at an angle ∆θ such that the waves from one edge of the slit have to
travel one wavelength farther than the waves from the other side—we worked
that out before (Chapter 30 of Vol. I). Therefore ∆θ is λ/B, and so ∆py in this
experiment is p0λ/B. Note that if we make B smaller and make a more accurate
measurement of the position of the particle, the diﬀraction pattern gets wider.
So the narrower we make the slit, the wider the pattern gets, and the more is the
likelihood that we would ﬁnd that the particle has sidewise momentum. Thus the
† More precisely, the error in our knowledge of y is ±B/2. But we are now only interested
in the general idea, so we won’t worry about factors of 2.
2-4","500"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","51","uncertainty in the vertical momentum is inversely proportional to the uncertainty
of y. In fact, we see that the product of the two is equal to p0λ. But λ is the
wavelength and p0 is the momentum, and in accordance with quantum mechanics,
the wavelength times the momentum is Planck’s constant h. So we obtain the
rule that the uncertainties in the vertical momentum and in the vertical position
have a product of the order h:
∆y ∆py ≥ℏ/2.
(2.3)
We cannot prepare a system in which we know the vertical position of a particle
and can predict how it will move vertically with greater certainty than given
by (2.3). That is, the uncertainty in the vertical momentum must exceed ℏ/2∆y,
where ∆y is the uncertainty in our knowledge of the position.
Sometimes people say quantum mechanics is all wrong. When the particle
arrived from the left, its vertical momentum was zero. And now that it has gone
through the slit, its position is known. Both position and momentum seem to be
known with arbitrary accuracy. It is quite true that we can receive a particle,
and on reception determine what its position is and what its momentum would
have had to have been to have gotten there. That is true, but that is not what
the uncertainty relation (2.3) refers to. Equation (2.3) refers to the predictability
of a situation, not remarks about the past. It does no good to say “I knew what
the momentum was before it went through the slit, and now I know the position,”
because now the momentum knowledge is lost. The fact that it went through
the slit no longer permits us to predict the vertical momentum. We are talking
about a predictive theory, not just measurements after the fact. So we must talk
about what we can predict.
Now let us take the thing the other way around. Let us take another example
of the same phenomenon, a little more quantitatively. In the previous example
we measured the momentum by a classical method. Namely, we considered the
direction and the velocity and the angles, etc., so we got the momentum by
classical analysis. But since momentum is related to wave number, there exists
in nature still another way to measure the momentum of a particle—photon or
otherwise—which has no classical analog, because it uses Eq. (2.2). We measure
the wavelengths of the waves. Let us try to measure momentum in this way.
Suppose we have a grating with a large number of lines (Fig. 2-3), and send
a beam of particles at the grating. We have often discussed this problem: if
the particles have a deﬁnite momentum, then we get a very sharp pattern in a
2-5","455"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","52","Nmλ = L
Fig. 2-3. Determination of momentum by using a diﬀraction grating.
certain direction, because of the interference. And we have also talked about how
accurately we can determine that momentum, that is to say, what the resolving
power of such a grating is. Rather than derive it again, we refer to Chapter 30 of
Volume I, where we found that the relative uncertainty in the wavelength that
can be measured with a given grating is 1/Nm, where N is the number of lines
on the grating and m is the order of the diﬀraction pattern. That is,
∆λ/λ = 1/Nm.
(2.4)
Now formula (2.4) can be rewritten as
∆λ/λ2 = 1/Nmλ = 1/L,
(2.5)
where L is the distance shown in Fig. 2-3. This distance is the diﬀerence between
the total distance that the particle or wave or whatever it is has to travel if
it is reﬂected from the bottom of the grating, and the distance that it has to
travel if it is reﬂected from the top of the grating. That is, the waves which form
the diﬀraction pattern are waves which come from diﬀerent parts of the grating.
The ﬁrst ones that arrive come from the bottom end of the grating, from the
beginning of the wave train, and the rest of them come from later parts of the
wave train, coming from diﬀerent parts of the grating, until the last one ﬁnally
arrives, and that involves a point in the wave train a distance L behind the ﬁrst
point. So in order that we shall have a sharp line in our spectrum corresponding
to a deﬁnite momentum, with an uncertainty given by (2.4), we have to have a
wave train of at least length L. If the wave train is too short, we are not using
the entire grating. The waves which form the spectrum are being reﬂected from
only a very short sector of the grating if the wave train is too short, and the
2-6","333"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","53","grating will not work right—we will ﬁnd a big angular spread. In order to get a
narrower one, we need to use the whole grating, so that at least at some moment
the whole wave train is scattering simultaneously from all parts of the grating.
Thus the wave train must be of length L in order to have an uncertainty in the
wavelength less than that given by (2.5). Incidentally,
∆λ/λ2 = ∆(1/λ) = ∆k/2π.
(2.6)
Therefore
∆k = 2π/L,
(2.7)
where L is the length of the wave train.
This means that if we have a wave train whose length is less than L, the
uncertainty in the wave number must exceed 2π/L. Or the uncertainty in a wave
number times the length of the wave train—we will call that for a moment ∆x—
exceeds 2π. We call it ∆x because that is the uncertainty in the location of the
particle. If the wave train exists only in a ﬁnite length, then that is where we
could ﬁnd the particle, within an uncertainty ∆x. Now this property of waves,
that the length of the wave train times the uncertainty of the wave number
associated with it is at least 2π, is a property that is known to everyone who
studies them. It has nothing to do with quantum mechanics. It is simply that if
we have a ﬁnite train, we cannot count the waves in it very precisely.
Let us try another way to see the reason for that. Suppose that we have a
ﬁnite train of length L; then because of the way it has to decrease at the ends,
as in Fig. 2-1, the number of waves in the length L is uncertain by something
like ±1. But the number of waves in L is kL/2π. Thus k is uncertain, and we
again get the result (2.7), a property merely of waves. The same thing works
whether the waves are in space and k is the number of radians per centimeter
and L is the length of the train, or the waves are in time and ω is the number of
oscillations per second and T is the “length” in time that the wave train comes
in. That is, if we have a wave train lasting only for a certain ﬁnite time T, then
the uncertainty in the frequency is given by
∆ω = 2π/T.
(2.8)
We have tried to emphasize that these are properties of waves alone, and they
are well known, for example, in the theory of sound.
The point is that in quantum mechanics we interpret the wave number as
being a measure of the momentum of a particle, with the rule that p = ℏk, so
2-7","454"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","54","that relation (2.7) tells us that ∆p ≈h/∆x. This, then, is a limitation of the
classical idea of momentum. (Naturally, it has to be limited in some ways if we
are going to represent particles by waves!) It is nice that we have found a rule
that gives us some idea of when there is a failure of classical ideas.
2-3 Crystal diﬀraction
Next let us consider the reﬂection of particle waves from a crystal. A crystal
is a thick thing which has a whole lot of similar atoms—we will include some
complications later—in a nice array. The question is how to set the array so that
we get a strong reﬂected maximum in a given direction for a given beam of, say,
light (x-rays), electrons, neutrons, or anything else. In order to obtain a strong
reﬂection, the scattering from all of the atoms must be in phase. There cannot
be equal numbers in phase and out of phase, or the waves will cancel out. The
way to arrange things is to ﬁnd the regions of constant phase, as we have already
explained; they are planes which make equal angles with the initial and ﬁnal
directions (Fig. 2-4).
If we consider two parallel planes, as in Fig. 2-4, the waves scattered from
the two planes will be in phase, provided the diﬀerence in distance traveled by a
wave front is an integral number of wavelengths. This diﬀerence can be seen to
be 2d sin θ, where d is the perpendicular distance between the planes. Thus the
condition for coherent reﬂection is
2d sin θ = nλ
(n = 1, 2, . . . ).
(2.9)
θ
θ θ
d
d sin θ
d sin θ
Fig. 2-4. Scattering of waves by crystal planes.
2-8","294"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","55","If, for example, the crystal is such that the atoms happen to lie on planes
obeying condition (2.9) with n = 1, then there will be a strong reﬂection. If,
on the other hand, there are other atoms of the same nature (equal in density)
halfway between, then the intermediate planes will also scatter equally strongly
and will interfere with the others and produce no eﬀect. So d in (2.9) must refer
to adjacent planes; we cannot take a plane ﬁve layers farther back and use this
formula!
As a matter of interest, actual crystals are not usually as simple as a single
kind of atom repeated in a certain way. Instead, if we make a two-dimensional
analog, they are much like wallpaper, in which there is some kind of ﬁgure
which repeats all over the wallpaper. By “ﬁgure” we mean, in the case of atoms,
some arrangement—calcium and a carbon and three oxygens, etc., for calcium
carbonate, and so on—which may involve a relatively large number of atoms.
But whatever it is, the ﬁgure is repeated in a pattern. This basic ﬁgure is called
a unit cell.
The basic pattern of repetition deﬁnes what we call the lattice type; the lattice
type can be immediately determined by looking at the reﬂections and seeing
what their symmetry is. In other words, where we ﬁnd any reﬂections at all
determines the lattice type, but in order to determine what is in each of the
elements of the lattice one must take into account the intensity of the scattering
at the various directions. Which directions scatter depends on the type of lattice,
but how strongly each scatters is determined by what is inside each unit cell, and
in that way the structure of crystals is worked out.
Two photographs of x-ray diﬀraction patterns are shown in Figs. 2-5 and 2-6;
they illustrate scattering from rock salt and myoglobin, respectively.
Figure 2-5
Figure 2-6
2-9","322"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","56","Incidentally, an interesting thing happens if the spacings of the nearest planes
are less than λ/2. In this case (2.9) has no solution for n. Thus if λ is bigger
than twice the distance between adjacent planes, then there is no side diﬀraction
pattern, and the light—or whatever it is—will go right through the material
without bouncing oﬀor getting lost. So in the case of light, where λ is much
bigger than the spacing, of course it does go through and there is no pattern of
reﬂection from the planes of the crystal.
PILE
SHORT-λ NEUTRONS
GRAPHITE
SHORT-λ NEUTRONS
LONG-λ
NEUTRONS
Fig. 2-7. Diﬀusion of pile neutrons through graphite block.
This fact also has an interesting consequence in the case of piles which make
neutrons (these are obviously particles, for anybody’s money!). If we take these
neutrons and let them into a long block of graphite, the neutrons diﬀuse and
work their way along (Fig. 2-7). They diﬀuse because they are bounced by the
atoms, but strictly, in the wave theory, they are bounced by the atoms because of
diﬀraction from the crystal planes. It turns out that if we take a very long piece
of graphite, the neutrons that come out the far end are all of long wavelength! In
fact, if one plots the intensity as a function of wavelength, we get nothing except
for wavelengths longer than a certain minimum (Fig. 2-8). In other words, we
can get very slow neutrons that way. Only the slowest neutrons come through;
they are not diﬀracted or scattered by the crystal planes of the graphite, but
keep going right through like light through glass, and are not scattered out the
sides. There are many other demonstrations of the reality of neutron waves and
waves of other particles.
2-4 The size of an atom
We now consider another application of the uncertainty relation, Eq. (2.3).
It must not be taken too seriously; the idea is right but the analysis is not very
accurate. The idea has to do with the determination of the size of atoms, and
2-10","347"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","57","λ
Intensity
λmin
Fig. 2-8. Intensity of neutrons out of graphite rod as function of
wavelength.
the fact that, classically, the electrons would radiate light and spiral in until
they settle down right on top of the nucleus. But that cannot be right quantum-
mechanically because then we would know where each electron was and how fast
it was moving.
Suppose we have a hydrogen atom, and measure the position of the electron; we
must not be able to predict exactly where the electron will be, or the momentum
spread will then turn out to be inﬁnite. Every time we look at the electron,
it is somewhere, but it has an amplitude to be in diﬀerent places so there is a
probability of it being found in diﬀerent places. These places cannot all be at the
nucleus; we shall suppose there is a spread in position of order a. That is, the
distance of the electron from the nucleus is usually about a. We shall determine
a by minimizing the total energy of the atom.
The spread in momentum is roughly ℏ/a because of the uncertainty relation,
so that if we try to measure the momentum of the electron in some manner,
such as by scattering x-rays oﬀit and looking for the Doppler eﬀect from a
moving scatterer, we would expect not to get zero every time—the electron is
not standing still—but the momenta must be of the order p ≈ℏ/a. Then the
kinetic energy is roughly 1
2mv2 = p2/2m = ℏ2/2ma2. (In a sense, this is a kind
of dimensional analysis to ﬁnd out in what way the kinetic energy depends upon
the reduced Planck constant, upon m, and upon the size of the atom. We need
not trust our answer to within factors like 2, π, etc. We have not even deﬁned a
very precisely.) Now the potential energy is minus e2 over the distance from the
center, say −e2/a, where, as deﬁned in Volume I, e2 is the charge of an electron
squared, divided by 4πϵ0. Now the point is that the potential energy is reduced if
a gets smaller, but the smaller a is, the higher the momentum required, because
of the uncertainty principle, and therefore the higher the kinetic energy. The
2-11","378"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","58","total energy is
E = ℏ2/2ma2 −e2/a.
(2.10)
We do not know what a is, but we know that the atom is going to arrange itself
to make some kind of compromise so that the energy is as little as possible. In
order to minimize E, we diﬀerentiate with respect to a, set the derivative equal
to zero, and solve for a. The derivative of E is
dE/da = −ℏ2/ma3 + e2/a2,
(2.11)
and setting dE/da = 0 gives for a the value
a0 = ℏ2/me2 = 0.528 angstrom,
= 0.528 × 10−10 meter.
(2.12)
This particular distance is called the Bohr radius, and we have thus learned
that atomic dimensions are of the order of angstroms, which is right. This is
pretty good—in fact, it is amazing, since until now we have had no basis for
understanding the size of atoms! Atoms are completely impossible from the
classical point of view, since the electrons would spiral into the nucleus.
Now if we put the value (2.12) for a0 into (2.10) to ﬁnd the energy, it comes
out
E0 = −e2/2a0 = −me4/2ℏ2 = −13.6 eV.
(2.13)
What does a negative energy mean? It means that the electron has less energy
when it is in the atom than when it is free. It means it is bound. It means it takes
energy to kick the electron out; it takes energy of the order of 13.6 eV to ionize
a hydrogen atom. We have no reason to think that it is not two or three times
this—or half of this—or (1/π) times this, because we have used such a sloppy
argument. However, we have cheated, we have used all the constants in such a
way that it happens to come out the right number! This number, 13.6 electron
volts, is called a Rydberg of energy; it is the ionization energy of hydrogen.
So we now understand why we do not fall through the ﬂoor. As we walk, our
shoes with their masses of atoms push against the ﬂoor with its mass of atoms.
In order to squash the atoms closer together, the electrons would be conﬁned to
a smaller space and, by the uncertainty principle, their momenta would have to
be higher on the average, and that means high energy; the resistance to atomic
compression is a quantum-mechanical eﬀect and not a classical eﬀect. Classically,
2-12","395"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","59","we would expect that if we were to draw all the electrons and protons closer
together, the energy would be reduced still further, and the best arrangement of
positive and negative charges in classical physics is all on top of each other. This
was well known in classical physics and was a puzzle because of the existence of the
atom. Of course, the early scientists invented some ways out of the trouble—but
never mind, we have the right way out, now!
Incidentally, although we have no reason to understand it at the moment, in
a situation where there are many electrons it turns out that they try to keep
away from each other. If one electron is occupying a certain space, then another
does not occupy the same space. More precisely, there are two spin cases, so that
two can sit on top of each other, one spinning one way and one the other way.
But after that we cannot put any more there. We have to put others in another
place, and that is the real reason that matter has strength. If we could put all
the electrons in the same place, it would condense even more than it does. It is
the fact that the electrons cannot all get on top of each other that makes tables
and everything else solid.
Obviously, in order to understand the properties of matter, we will have to
use quantum mechanics and not be satisﬁed with classical mechanics.
2-5 Energy levels
We have talked about the atom in its lowest possible energy condition, but
it turns out that the electron can do other things. It can jiggle and wiggle in a
more energetic manner, and so there are many diﬀerent possible motions for the
atom. According to quantum mechanics, in a stationary condition there can only
be deﬁnite energies for an atom. We make a diagram (Fig. 2-9) in which we plot
the energy vertically, and we make a horizontal line for each allowed value of the
Energy
E = 0
E0
E1
E2
E3
Fig. 2-9.
Energy diagram for an atom, showing several possible
transitions.
2-13","355"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","60","energy. When the electron is free, i.e., when its energy is positive, it can have
any energy; it can be moving at any speed. But bound energies are not arbitrary.
The atom must have one or another out of a set of allowed values, such as those
in Fig. 2-9.
Now let us call the allowed values of the energy E0, E1, E2, E3. If an atom
is initially in one of these “excited states,” E1, E2, etc., it does not remain in
that state forever. Sooner or later it drops to a lower state and radiates energy
in the form of light. The frequency of the light that is emitted is determined
by conservation of energy plus the quantum-mechanical understanding that the
frequency of the light is related to the energy of the light by (2.1). Therefore
the frequency of the light which is liberated in a transition from energy E3 to
energy E1 (for example) is
ω31 = (E3 −E1)/ℏ.
(2.14)
This, then, is a characteristic frequency of the atom and deﬁnes a spectral emission
line. Another possible transition would be from E3 to E0. That would have a
diﬀerent frequency
ω30 = (E3 −E0)/ℏ.
(2.15)
Another possibility is that if the atom were excited to the state E1 it could drop
to the ground state E0, emitting a photon of frequency
ω10 = (E1 −E0)/ℏ.
(2.16)
The reason we bring up three transitions is to point out an interesting relationship.
It is easy to see from (2.14), (2.15), and (2.16) that
ω30 = ω31 + ω10.
(2.17)
In general, if we ﬁnd two spectral lines, we shall expect to ﬁnd another line at the
sum of the frequencies (or the diﬀerence in the frequencies), and that all the lines
can be understood by ﬁnding a series of levels such that every line corresponds
to the diﬀerence in energy of some pair of levels. This remarkable coincidence in
spectral frequencies was noted before quantum mechanics was discovered, and it
is called the Ritz combination principle. This is again a mystery from the point of
view of classical mechanics. Let us not belabor the point that classical mechanics
is a failure in the atomic domain; we seem to have demonstrated that pretty well.
2-14","375"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","61","We have already talked about quantum mechanics as being represented by
amplitudes which behave like waves, with certain frequencies and wave numbers.
Let us observe how it comes about from the point of view of amplitudes that the
atom has deﬁnite energy states. This is something we cannot understand from
what has been said so far, but we are all familiar with the fact that conﬁned
waves have deﬁnite frequencies. For instance, if sound is conﬁned to an organ
pipe, or anything like that, then there is more than one way that the sound can
vibrate, but for each such way there is a deﬁnite frequency. Thus an object in
which the waves are conﬁned has certain resonance frequencies. It is therefore a
property of waves in a conﬁned space—a subject which we will discuss in detail
with formulas later on—that they exist only at deﬁnite frequencies. And since the
general relation exists between frequencies of the amplitude and energy, we are
not surprised to ﬁnd deﬁnite energies associated with electrons bound in atoms.
2-6 Philosophical implications
Let us consider brieﬂy some philosophical implications of quantum mechanics.
As always, there are two aspects of the problem: one is the philosophical implica-
tion for physics, and the other is the extrapolation of philosophical matters to
other ﬁelds. When philosophical ideas associated with science are dragged into
another ﬁeld, they are usually completely distorted. Therefore we shall conﬁne
our remarks as much as possible to physics itself.
First of all, the most interesting aspect is the idea of the uncertainty principle;
making an observation aﬀects the phenomenon.
It has always been known
that making observations aﬀects a phenomenon, but the point is that the eﬀect
cannot be disregarded or minimized or decreased arbitrarily by rearranging the
apparatus. When we look for a certain phenomenon we cannot help but disturb
it in a certain minimum way, and the disturbance is necessary for the consistency
of the viewpoint. The observer was sometimes important in prequantum physics,
but only in a trivial sense. The problem has been raised: if a tree falls in a forest
and there is nobody there to hear it, does it make a noise? A real tree falling
in a real forest makes a sound, of course, even if nobody is there. Even if no
one is present to hear it, there are other traces left. The sound will shake some
leaves, and if we were careful enough we might ﬁnd somewhere that some thorn
had rubbed against a leaf and made a tiny scratch that could not be explained
unless we assumed the leaf were vibrating. So in a certain sense we would have
to admit that there is a sound made. We might ask: was there a sensation of
2-15","461"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","62","sound? No, sensations have to do, presumably, with consciousness. And whether
ants are conscious and whether there were ants in the forest, or whether the tree
was conscious, we do not know. Let us leave the problem in that form.
Another thing that people have emphasized since quantum mechanics was
developed is the idea that we should not speak about those things which we
cannot measure. (Actually relativity theory also said this.) Unless a thing can be
deﬁned by measurement, it has no place in a theory. And since an accurate value
of the momentum of a localized particle cannot be deﬁned by measurement it
therefore has no place in the theory. The idea that this is what was the matter
with classical theory is a false position. It is a careless analysis of the situation.
Just because we cannot measure position and momentum precisely does not a
priori mean that we cannot talk about them. It only means that we need not talk
about them. The situation in the sciences is this: A concept or an idea which
cannot be measured or cannot be referred directly to experiment may or may not
be useful. It need not exist in a theory. In other words, suppose we compare the
classical theory of the world with the quantum theory of the world, and suppose
that it is true experimentally that we can measure position and momentum only
imprecisely. The question is whether the ideas of the exact position of a particle
and the exact momentum of a particle are valid or not. The classical theory
admits the ideas; the quantum theory does not. This does not in itself mean that
classical physics is wrong. When the new quantum mechanics was discovered,
the classical people—which included everybody except Heisenberg, Schrödinger,
and Born—said: “Look, your theory is not any good because you cannot answer
certain questions like: what is the exact position of a particle?, which hole does
it go through?, and some others.” Heisenberg’s answer was: “I do not need to
answer such questions because you cannot ask such a question experimentally.” It
is that we do not have to. Consider two theories (a) and (b); (a) contains an idea
that cannot be checked directly but which is used in the analysis, and the other,
(b), does not contain the idea. If they disagree in their predictions, one could not
claim that (b) is false because it cannot explain this idea that is in (a), because
that idea is one of the things that cannot be checked directly. It is always good
to know which ideas cannot be checked directly, but it is not necessary to remove
them all. It is not true that we can pursue science completely by using only those
concepts which are directly subject to experiment.
In quantum mechanics itself there is a probability amplitude, there is a
potential, and there are many constructs that we cannot measure directly. The
basis of a science is its ability to predict. To predict means to tell what will
2-16","509"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","63","happen in an experiment that has never been done. How can we do that? By
assuming that we know what is there, independent of the experiment. We must
extrapolate the experiments to a region where they have not been done. We
must take our concepts and extend them to places where they have not yet
been checked. If we do not do that, we have no prediction. So it was perfectly
sensible for the classical physicists to go happily along and suppose that the
position—which obviously means something for a baseball—meant something
also for an electron. It was not stupidity. It was a sensible procedure. Today we
say that the law of relativity is supposed to be true at all energies, but someday
somebody may come along and say how stupid we were. We do not know where
we are “stupid” until we “stick our neck out,” and so the whole idea is to put our
neck out. And the only way to ﬁnd out that we are wrong is to ﬁnd out what
our predictions are. It is absolutely necessary to make constructs.
We have already made a few remarks about the indeterminacy of quantum
mechanics. That is, that we are unable now to predict what will happen in
physics in a given physical circumstance which is arranged as carefully as possible.
If we have an atom that is in an excited state and so is going to emit a photon,
we cannot say when it will emit the photon. It has a certain amplitude to emit
the photon at any time, and we can predict only a probability for emission; we
cannot predict the future exactly. This has given rise to all kinds of nonsense
and questions on the meaning of freedom of will, and of the idea that the world
is uncertain.
Of course we must emphasize that classical physics is also indeterminate, in a
sense. It is usually thought that this indeterminacy, that we cannot predict the
future, is an important quantum-mechanical thing, and this is said to explain the
behavior of the mind, feelings of free will, etc. But if the world were classical—if
the laws of mechanics were classical—it is not quite obvious that the mind would
not feel more or less the same. It is true classically that if we knew the position
and the velocity of every particle in the world, or in a box of gas, we could predict
exactly what would happen. And therefore the classical world is deterministic.
Suppose, however, that we have a ﬁnite accuracy and do not know exactly where
just one atom is, say to one part in a billion. Then as it goes along it hits another
atom, and because we did not know the position better than to one part in a
billion, we ﬁnd an even larger error in the position after the collision. And that
is ampliﬁed, of course, in the next collision, so that if we start with only a tiny
error it rapidly magniﬁes to a very great uncertainty. To give an example: if
water falls over a dam, it splashes. If we stand nearby, every now and then a
2-17","531"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","64","drop will land on our nose. This appears to be completely random, yet such a
behavior would be predicted by purely classical laws. The exact position of all
the drops depends upon the precise wigglings of the water before it goes over
the dam. How? The tiniest irregularities are magniﬁed in falling, so that we get
complete randomness. Obviously, we cannot really predict the position of the
drops unless we know the motion of the water absolutely exactly.
Speaking more precisely, given an arbitrary accuracy, no matter how precise,
one can ﬁnd a time long enough that we cannot make predictions valid for that
long a time. Now the point is that this length of time is not very large. It is not
that the time is millions of years if the accuracy is one part in a billion. The
time goes, in fact, only logarithmically with the error, and it turns out that in
only a very, very tiny time we lose all our information. If the accuracy is taken
to be one part in billions and billions and billions—no matter how many billions
we wish, provided we do stop somewhere—then we can ﬁnd a time less than the
time it took to state the accuracy—after which we can no longer predict what is
going to happen! It is therefore not fair to say that from the apparent freedom
and indeterminacy of the human mind, we should have realized that classical
“deterministic” physics could not ever hope to understand it, and to welcome
quantum mechanics as a release from a “completely mechanistic” universe. For
already in classical mechanics there was indeterminability from a practical point
of view.
2-18","279"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","65","3
Probability Amplitudes
3-1 The laws for combining amplitudes
When Schrödinger ﬁrst discovered the correct laws of quantum mechanics, he
wrote an equation which described the amplitude to ﬁnd a particle in various
places. This equation was very similar to the equations that were already known
to classical physicists—equations that they had used in describing the motion of
air in a sound wave, the transmission of light, and so on. So most of the time at the
beginning of quantum mechanics was spent in solving this equation. But at the
same time an understanding was being developed, particularly by Born and Dirac,
of the basically new physical ideas behind quantum mechanics. As quantum
mechanics developed further, it turned out that there were a large number of
things which were not directly encompassed in the Schrödinger equation—such
as the spin of the electron, and various relativistic phenomena. Traditionally, all
courses in quantum mechanics have begun in the same way, retracing the path
followed in the historical development of the subject. One ﬁrst learns a great deal
about classical mechanics so that he will be able to understand how to solve the
Schrödinger equation. Then he spends a long time working out various solutions.
Only after a detailed study of this equation does he get to the “advanced” subject
of the electron’s spin.
We had also originally considered that the right way to conclude these lec-
tures on physics was to show how to solve the equations of classical physics
in complicated situations—such as the description of sound waves in enclosed
regions, modes of electromagnetic radiation in cylindrical cavities, and so on.
That was the original plan for this course. However, we have decided to abandon
that plan and to give instead an introduction to the quantum mechanics. We
have come to the conclusion that what are usually called the advanced parts of
quantum mechanics are, in fact, quite simple. The mathematics that is involved
is particularly simple, involving simple algebraic operations and no diﬀerential
3-1","334"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","66","equations or at most only very simple ones. The only problem is that we must
jump the gap of no longer being able to describe the behavior in detail of particles
in space. So this is what we are going to try to do: to tell you about what
conventionally would be called the “advanced” parts of quantum mechanics. But
they are, we assure you, by all odds the simplest parts—in a deep sense of the
word—as well as the most basic parts. This is frankly a pedagogical experiment;
it has never been done before, as far as we know.
In this subject we have, of course, the diﬃculty that the quantum mechanical
behavior of things is quite strange. Nobody has an everyday experience to lean
on to get a rough, intuitive idea of what will happen. So there are two ways of
presenting the subject: We could either describe what can happen in a rather
rough physical way, telling you more or less what happens without giving the
precise laws of everything; or we could, on the other hand, give the precise laws
in their abstract form. But, then because of the abstractions, you wouldn’t know
what they were all about, physically. The latter method is unsatisfactory because
it is completely abstract, and the ﬁrst way leaves an uncomfortable feeling because
one doesn’t know exactly what is true and what is false. We are not sure how to
overcome this diﬃculty. You will notice, in fact, that Chapters 1 and 2 showed
this problem. The ﬁrst chapter was relatively precise; but the second chapter
was a rough description of the characteristics of diﬀerent phenomena. Here, we
will try to ﬁnd a happy medium between the two extremes.
We will begin in this chapter by dealing with some general quantum mechanical
ideas. Some of the statements will be quite precise, others only partially precise.
It will be hard to tell you as we go along which is which, but by the time you
have ﬁnished the rest of the book, you will understand in looking back which
parts hold up and which parts were only explained roughly. The chapters which
follow this one will not be so imprecise. In fact, one of the reasons we have tried
carefully to be precise in the succeeding chapters is so that we can show you
one of the most beautiful things about quantum mechanics—how much can be
deduced from so little.
We begin by discussing again the superposition of probability amplitudes. As
an example we will refer to the experiment described in Chapter 1, and shown
again here in Fig. 3-1. There is a source s of particles, say electrons; then there
is a wall with two slits in it; after the wall, there is a detector located at some
position x. We ask for the probability that a particle will be found at x. Our ﬁrst
general principle in quantum mechanics is that the probability that a particle
will arrive at x, when let out at the source s, can be represented quantitatively
3-2","512"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","67","ELECTRON
GUN
s
(a)
WALL
1
2
BACKSTOP
DETECTOR
x
(b)
x
P1
P2
(c)
x
P12
Fig. 3-1. Interference experiment with electrons.
by the absolute square of a complex number called a probability amplitude—in
this case, the “amplitude that a particle from s will arrive at x.” We will use
such amplitudes so frequently that we will use a shorthand notation—invented
by Dirac and generally used in quantum mechanics—to represent this idea. We
write the probability amplitude this way:
⟨Particle arrives at x | particle leaves s⟩.
(3.1)
In other words, the two brackets ⟨⟩are a sign equivalent to “the amplitude that”;
the expression at the right of the vertical line always gives the starting condition,
and the one at the left, the ﬁnal condition. Sometimes it will also be convenient
to abbreviate still more and describe the initial and ﬁnal conditions by single
letters. For example, we may on occasion write the amplitude (3.1) as
⟨x | s⟩.
(3.2)
We want to emphasize that such an amplitude is, of course, just a single number—a
complex number.
We have already seen in the discussion of Chapter 1 that when there are two
ways for the particle to reach the detector, the resulting probability is not the
sum of the two probabilities, but must be written as the absolute square of the
3-3","223"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","68","sum of two amplitudes. We had that the probability that an electron arrives at
the detector when both paths are open is
P12 = |φ1 + φ2|2.
(3.3)
We wish now to put this result in terms of our new notation. First, however,
we want to state our second general principle of quantum mechanics: When a
particle can reach a given state by two possible routes, the total amplitude for
the process is the sum of the amplitudes for the two routes considered separately.
In our new notation we write that
⟨x | s⟩both holes open = ⟨x | s⟩through 1 + ⟨x | s⟩through 2.
(3.4)
Incidentally, we are going to suppose that the holes 1 and 2 are small enough
that when we say an electron goes through the hole, we don’t have to discuss
which part of the hole. We could, of course, split each hole into pieces with a
certain amplitude that the electron goes to the top of the hole and the bottom
of the hole and so on. We will suppose that the hole is small enough so that we
don’t have to worry about this detail. That is part of the roughness involved;
the matter can be made more precise, but we don’t want to do so at this stage.
Now we want to write out in more detail what we can say about the amplitude
for the process in which the electron reaches the detector at x by way of hole 1.
We can do that by using our third general principle: When a particle goes by
some particular route the amplitude for that route can be written as the product
of the amplitude to go part way with the amplitude to go the rest of the way. For
the setup of Fig. 3-1 the amplitude to go from s to x by way of hole 1 is equal to
the amplitude to go from s to 1, multiplied by the amplitude to go from 1 to x.
⟨x | s⟩via 1 = ⟨x | 1⟩⟨1 | s⟩.
(3.5)
Again this result is not completely precise. We should also include a factor for
the amplitude that the electron will get through the hole at 1; but in the present
case it is a simple hole, and we will take this factor to be unity.
You will note that Eq. (3.5) appears to be written in reverse order. It is
to be read from right to left: The electron goes from s to 1 and then from 1
to x. In summary, if events occur in succession—that is, if you can analyze one
of the routes of the particle by saying it does this, then it does this, then it
does that—the resultant amplitude for that route is calculated by multiplying in
3-4","470"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","69","succession the amplitude for each of the successive events. Using this law we can
rewrite Eq. (3.4) as
⟨x | s⟩both = ⟨x | 1⟩⟨1 | s⟩+ ⟨x | 2⟩⟨2 | s⟩.
s
1
2
a
b
c
x
Fig. 3-2. A more complicated interference experiment.
Now we wish to show that just using these principles we can calculate a
much more complicated problem like the one shown in Fig. 3-2. Here we have
two walls, one with two holes, 1 and 2, and another which has three holes, a, b,
and c. Behind the second wall there is a detector at x, and we want to know the
amplitude for a particle to arrive there. Well, one way you can ﬁnd this is by
calculating the superposition, or interference, of the waves that go through; but
you can also do it by saying that there are six possible routes and superposing
an amplitude for each. The electron can go through hole 1, then through hole a,
and then to x; or it could go through hole 1, then through hole b, and then to x;
and so on. According to our second principle, the amplitudes for alternative
routes add, so we should be able to write the amplitude from s to x as a sum of
six separate amplitudes. On the other hand, using the third principle, each of
these separate amplitudes can be written as a product of three amplitudes. For
example, one of them is the amplitude for s to 1, times the amplitude for 1 to a,
times the amplitude for a to x. Using our shorthand notation, we can write the
3-5","277"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","70","complete amplitude to go from s to x as
⟨x | s⟩= ⟨x | a⟩⟨a | 1⟩⟨1 | s⟩+ ⟨x | b⟩⟨b | 1⟩⟨1 | s⟩+ · · · + ⟨x | c⟩⟨c | 2⟩⟨2 | s⟩.
We can save writing by using the summation notation
⟨x | s⟩=
X
i=1,2
α=a,b,c
⟨x | α⟩⟨α | i⟩⟨i | s⟩.
(3.6)
In order to make any calculations using these methods, it is, naturally,
necessary to know the amplitude to get from one place to another. We will
give a rough idea of a typical amplitude. It leaves out certain things like the
polarization of light or the spin of the electron, but aside from such features it
is quite accurate. We give it so that you can solve problems involving various
combinations of slits. Suppose a particle with a deﬁnite energy is going in empty
space from a location r1 to a location r2. In other words, it is a free particle
with no forces on it. Except for a numerical factor in front, the amplitude to go
from r1 to r2 is
⟨r2 | r1⟩= eip·r12/ℏ
r12
,
(3.7)
where r12 = r2 −r1, and p is the momentum which is related to the energy E
by the relativistic equation
p2c2 = E2 −(m0c2)2,
or the nonrelativistic equation
p2
2m = Kinetic energy.
Equation (3.7) says in eﬀect that the particle has wavelike properties, the ampli-
tude propagating as a wave with a wave number equal to the momentum divided
by ℏ.
In the most general case, the amplitude and the corresponding probability
will also involve the time. For most of these initial discussions we will suppose
that the source always emits the particles with a given energy so we will not
need to worry about the time. But we could, in the general case, be interested in
some other questions. Suppose that a particle is liberated at a certain place P at
3-6","324"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","71","a certain time, and you would like to know the amplitude for it to arrive at some
location, say r, at some later time. This could be represented symbolically as the
amplitude ⟨r, t = t1 | P, t = 0⟩. Clearly, this will depend upon both r and t. You
will get diﬀerent results if you put the detector in diﬀerent places and measure
at diﬀerent times. This function of r and t, in general, satisﬁes a diﬀerential
equation which is a wave equation. For example, in a nonrelativistic case it
is the Schrödinger equation. One has then a wave equation analogous to the
equation for electromagnetic waves or waves of sound in a gas. However, it must
be emphasized that the wave function that satisﬁes the equation is not like a real
wave in space; one cannot picture any kind of reality to this wave as one does for
a sound wave.
Although one may be tempted to think in terms of “particle waves” when
dealing with one particle, it is not a good idea, for if there are, say, two particles,
the amplitude to ﬁnd one at r1 and the other at r2 is not a simple wave in
three-dimensional space, but depends on the six space variables r1 and r2. If
we are, for example, dealing with two (or more) particles, we will need the
following additional principle: Provided that the two particles do not interact,
the amplitude that one particle will do one thing and the other one something
else is the product of the two amplitudes that the two particles would do the two
things separately. For example, if ⟨a | s1⟩is the amplitude for particle 1 to go
from s1 to a, and ⟨b | s2⟩is the amplitude for particle 2 to go from s2 to b, the
amplitude that both things will happen together is
⟨a | s1⟩⟨b | s2⟩.
There is one more point to emphasize. Suppose that we didn’t know where
the particles in Fig. 3-2 come from before arriving at holes 1 and 2 of the ﬁrst
wall. We can still make a prediction of what will happen beyond the wall (for
example, the amplitude to arrive at x) provided that we are given two numbers:
the amplitude to have arrived at 1 and the amplitude to have arrived at 2.
In other words, because of the fact that the amplitude for successive events
multiplies, as shown in Eq. (3.6), all you need to know to continue the analysis
is two numbers—in this particular case ⟨1 | s⟩and ⟨2 | s⟩. These two complex
numbers are enough to predict all the future. That is what really makes quantum
mechanics easy. It turns out that in later chapters we are going to do just such a
thing when we specify a starting condition in terms of two (or a few) numbers.
Of course, these numbers depend upon where the source is located and possibly
3-7","495"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","72","other details about the apparatus, but given the two numbers, we do not need to
know any more about such details.
3-2 The two-slit interference pattern
Now we would like to consider a matter which was discussed in some detail
in Chapter 1. This time we will do it with the full glory of the amplitude idea to
show you how it works out. We take the same experiment shown in Fig. 3-1, but
now with the addition of a light source behind the two holes, as shown in Fig. 3-3.
In Chapter 1, we discovered the following interesting result. If we looked behind
slit 1 and saw a photon scattered from there, then the distribution obtained for
the electrons at x in coincidence with these photons was the same as though
slit 2 were closed. The total distribution for electrons that had been “seen” at
either slit 1 or slit 2 was the sum of the separate distributions and was completely
diﬀerent from the distribution with the light turned oﬀ. This was true at least if
we used light of short enough wavelength. If the wavelength was made longer so
we could not be sure at which hole the scattering had occurred, the distribution
became more like the one with the light turned oﬀ.
ELECTRON
GUN
1
2
LIGHT
SOURCE
D1
D2
Fig. 3-3. An experiment to determine which hole the electron goes
through.
3-8","236"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","73","Let’s examine what is happening by using our new notation and the principles
of combining amplitudes. To simplify the writing, we can again let φ1 stand for
the amplitude that the electron will arrive at x by way of hole 1, that is,
φ1 = ⟨x | 1⟩⟨1 | s⟩.
Similarly, we’ll let φ2 stand for the amplitude that the electron gets to the detector
by way of hole 2:
φ2 = ⟨x | 2⟩⟨2 | s⟩.
These are the amplitudes to go through the two holes and arrive at x if there is no
light. Now if there is light, we ask ourselves the question: What is the amplitude
for the process in which the electron starts at s and a photon is liberated by the
light source L, ending with the electron at x and a photon seen behind slit 1?
Suppose that we observe the photon behind slit 1 by means of a detector D1,
as shown in Fig. 3-3, and use a similar detector D2 to count photons scattered
behind hole 2. There will be an amplitude for a photon to arrive at D1 and an
electron at x, and also an amplitude for a photon to arrive at D2 and an electron
at x. Let’s try to calculate them.
Although we don’t have the correct mathematical formula for all the factors
that go into this calculation, you will see the spirit of it in the following discussion.
First, there is the amplitude ⟨1 | s⟩that an electron goes from the source to hole 1.
Then we can suppose that there is a certain amplitude that while the electron
is at hole 1 it scatters a photon into the detector D1. Let us represent this
amplitude by a. Then there is the amplitude ⟨x | 1⟩that the electron goes from
slit 1 to the electron detector at x. The amplitude that the electron goes from s
to x via slit 1 and scatters a photon into D1 is then
⟨x | 1⟩a ⟨1 | s⟩.
Or, in our previous notation, it is just aφ1.
There is also some amplitude that an electron going through slit 2 will scatter
a photon into counter D1. You say, “That’s impossible; how can it scatter into
counter D1 if it is only looking at hole 1?” If the wavelength is long enough, there
are diﬀraction eﬀects, and it is certainly possible. If the apparatus is built well
and if we use photons of short wavelength, then the amplitude that a photon will
be scattered into detector 1, from an electron at 2 is very small. But to keep the
discussion general we want to take into account that there is always some such
3-9","454"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","74","amplitude, which we will call b. Then the amplitude that an electron goes via
slit 2 and scatters a photon into D1 is
⟨x | 2⟩b ⟨2 | s⟩= bφ2.
The amplitude to ﬁnd the electron at x and the photon in D1 is the sum of
two terms, one for each possible path for the electron. Each term is in turn made
up of two factors: ﬁrst, that the electron went through a hole, and second, that
the photon is scattered by such an electron into detector 1; we have

electron at x
photon at D1

electron from s
photon from L

= aφ1 + bφ2.
(3.8)
We can get a similar expression when the photon is found in the other
detector D2. If we assume for simplicity that the system is symmetrical, then a
is also the amplitude for a photon in D2 when an electron passes through hole 2,
and b is the amplitude for a photon in D2 when the electron passes through
hole 1. The corresponding total amplitude for a photon at D2 and an electron
at x is

electron at x
photon at D2

electron from s
photon from L

= aφ2 + bφ1.
(3.9)
Now we are ﬁnished. We can easily calculate the probability for various
situations. Suppose that we want to know with what probability we get a count
in D1 and an electron at x. That will be the absolute square of the amplitude
given in Eq. (3.8), namely, just |aφ1 + bφ2|2. Let’s look more carefully at this
expression. First of all, if b is zero—which is the way we would like to design
the apparatus—then the answer is simply |φ1|2 diminished in total amplitude by
the factor |a|2. This is the probability distribution that you would get if there
were only one hole—as shown in the graph of Fig. 3-4(a). On the other hand, if
the wavelength is very long, the scattering behind hole 2 into D1 may be just
about the same as for hole 1. Although there may be some phases involved in a
and b, we can ask about a simple case in which the two phases are equal. If a is
practically equal to b, then the total probability becomes |φ1 + φ2|2 multiplied
by |a|2, since the common factor a can be taken out. This, however, is just
the probability distribution we would have gotten without the photons at all.
Therefore, in the case that the wavelength is very long—and the photon detection
ineﬀective—you return to the original distribution curve which shows interference
eﬀects, as shown in Fig. 3-4(b). In the case that the detection is partially eﬀective,
3-10","445"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","75","(a)
x
P
(b)
x
P
(c)
x
P
Fig. 3-4. The probability of counting an electron at x in coincidence
with a photon at D in the experiment of Fig. 3-3: (a) for b = 0; (b) for
b = a; (c) for 0 < b < a.
there is an interference between a lot of φ1 and a little of φ2, and you will get an
intermediate distribution such as is sketched in Fig. 3-4(c). Needless to say, if we
look for coincidence counts of photons at D2 and electrons at x, we will get the
same kinds of results. If you remember the discussion in Chapter 1, you will see
that these results give a quantitative description of what was described there.
Now we would like to emphasize an important point so that you will avoid
a common error. Suppose that you only want the amplitude that the electron
arrives at x, regardless of whether the photon was counted at D1 or D2. Should
you add the amplitudes given in Eqs. (3.8) and (3.9)? No! You must never add
amplitudes for diﬀerent and distinct ﬁnal states. Once the photon is accepted by
one of the photon counters, we can always determine which alternative occurred
if we want, without any further disturbance to the system. Each alternative
has a probability completely independent of the other. To repeat, do not add
amplitudes for diﬀerent ﬁnal conditions, where by “ﬁnal” we mean at that moment
the probability is desired—that is, when the experiment is “ﬁnished.” You do
add the amplitudes for the diﬀerent indistinguishable alternatives inside the
experiment, before the complete process is ﬁnished. At the end of the process
you may say that you “don’t want to look at the photon.” That’s your business,
but you still do not add the amplitudes. Nature does not know what you are
looking at, and she behaves the way she is going to behave whether you bother
3-11","326"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","76","to take down the data or not. So here we must not add the amplitudes. We ﬁrst
square the amplitudes for all possible diﬀerent ﬁnal events and then sum. The
correct result for an electron at x and a photon at either D1 or D2 is

e at x
ph at D1

e from s
ph from L

2
+

e at x
ph at D2

e from s
ph from L

2
= |aφ1 + bφ2|2 + |aφ2 + bφ1|2.
(3.10)
3-3 Scattering from a crystal
Our next example is a phenomenon in which we have to analyze the interference
of probability amplitudes somewhat carefully. We look at the process of the
scattering of neutrons from a crystal. Suppose we have a crystal which has a
lot of atoms with nuclei at their centers, arranged in a periodic array, and a
neutron beam that comes from far away. We can label the various nuclei in the
crystal by an index i, where i runs over the integers 1, 2, 3, . . . , N, with N equal
to the total number of atoms. The problem is to calculate the probability of
getting a neutron into a counter with the arrangement shown in Fig. 3-5. For
any particular atom i, the amplitude that the neutron arrives at the counter C
is the amplitude that the neutron gets from the source S to nucleus i, multiplied
by the amplitude a that it gets scattered there, multiplied by the amplitude that
it gets from i to the counter C. Let’s write that down:
⟨neutron at C | neutron from S⟩via i = ⟨C | i⟩a ⟨i | S⟩.
(3.11)
In writing this equation we have assumed that the scattering amplitude a is the
same for all atoms. We have here a large number of apparently indistinguishable
S
NEUTRON
SOURCE
CRYSTAL
θ
C
NEUTRON
COUNTER
Fig. 3-5. Measuring the scattering of neutrons by a crystal.
3-12","324"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","77","routes. They are indistinguishable because a low-energy neutron is scattered
from a nucleus without knocking the atom out of its place in the crystal—no
“record” is left of the scattering. According to the earlier discussion, the total
amplitude for a neutron at C involves a sum of Eq. (3.11) over all the atoms:
⟨neutron at C | neutron from S⟩=
N
X
i=1
⟨C | i⟩a ⟨i | S⟩.
(3.12)
Because we are adding amplitudes of scattering from atoms with diﬀerent space
positions, the amplitudes will have diﬀerent phases giving the characteristic
interference pattern that we have already analyzed in the case of the scattering
of light from a grating.
The neutron intensity as a function of angle in such an experiment is indeed
often found to show tremendous variations, with very sharp interference peaks and
almost nothing in between—as shown in Fig. 3-6(a). However, for certain kinds
of crystals it does not work this way, and there is—along with the interference
peaks discussed above—a general background of scattering in all directions. We
must try to understand the apparently mysterious reasons for this. Well, we
have not considered one important property of the neutron. It has a spin of
one-half, and so there are two conditions in which it can be: either spin “up”
(say perpendicular to the page in Fig. 3-5) or spin “down.” If the nuclei of the
crystal have no spin, the neutron spin doesn’t have any eﬀect. But when the
nuclei of the crystal also have a spin, say a spin of one-half, you will observe the
background of smeared-out scattering described above. The explanation is as
follows.
If the neutron has one direction of spin and the atomic nucleus has the same
spin, then no change of spin can occur in the scattering process. If the neutron
and atomic nucleus have opposite spin, then scattering can occur by two processes,
one in which the spins are unchanged and another in which the spin directions
are exchanged. This rule for no net change of the sum of the spins is analogous
to our classical law of conservation of angular momentum. We can begin to
understand the phenomenon if we assume that all the scattering nuclei are set up
with spins in one direction. A neutron with the same spin will scatter with the
expected sharp interference distribution. What about one with opposite spin?
If it scatters without spin ﬂip, then nothing is changed from the above; but if
the two spins ﬂip over in the scattering, we could, in principle, ﬁnd out which
nucleus had done the scattering, since it would be the only one with spin turned
3-13","443"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","78","(a)
θ
COUNTING
RATE
(b)
θ
SPIN FLIP
PROBABILITY
(c)
θ
COUNTING
RATE
Fig. 3-6. The neutron counting rate as a function of angle: (a) for
spin zero nuclei; (b) the probability of scattering with spin ﬂip; (c) the
observed counting rate with a spin one-half nucleus.
3-14","48"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","79","over. Well, if we can tell which atom did the scattering, what have the other
atoms got to do with it? Nothing, of course. The scattering is exactly the same
as that from a single atom.
To include this eﬀect, the mathematical formulation of Eq. (3.12) must be
modiﬁed since we haven’t described the states completely in that analysis. Let’s
start with all neutrons from the source having spin up and all the nuclei of the
crystal having spin down. First, we would like the amplitude that at the counter
the spin of the neutron is up and all spins of the crystal are still down. This
is not diﬀerent from our previous discussion. We will let a be the amplitude to
scatter with no ﬂip or spin. The amplitude for scattering from the ith atom is,
of course,
⟨Cup, crystal all down | Sup, crystal all down⟩= ⟨C | i⟩a ⟨i | S⟩.
Since all the atomic spins are still down, the various alternatives (diﬀerent values
of i) cannot be distinguished. There is clearly no way to tell which atom did the
scattering. For this process, all the amplitudes interfere.
We have another case, however, where the spin of the detected neutron is
down although it started from S with spin up. In the crystal, one of the spins
must be changed to the up direction—let’s say that of the kth atom. We will
assume that there is the same scattering amplitude with spin ﬂip for every atom,
namely b. (In a real crystal there is the disagreeable possibility that the reversed
spin moves to some other atom, but let’s take the case of a crystal for which this
probability is very low.) The scattering amplitude is then
⟨Cdown, nucleus k up | Sup, crystal all down⟩= ⟨C | k⟩b ⟨k | S⟩.
(3.13)
If we ask for the probability of ﬁnding the neutron spin down and the kth nucleus
spin up, it is equal to the absolute square of this amplitude, which is simply |b|2
times |⟨C | k⟩⟨k | S⟩|2. The second factor is almost independent of location in
the crystal, and all phases have disappeared in taking the absolute square. The
probability of scattering from any nucleus in the crystal with spin ﬂip is now
|b|2
N
X
k=1
|⟨C | k⟩⟨k | S⟩|2,
which will show a smooth distribution as in Fig. 3-6(b).
3-15","399"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","80","You may argue, “I don’t care which atom is up.” Perhaps you don’t, but
nature knows; and the probability is, in fact, what we gave above—there isn’t
any interference. On the other hand, if we ask for the probability that the spin
is up at the detector and all the atoms still have spin down, then we must take
the absolute square of
N
X
i=1
⟨C | i⟩a ⟨i | S⟩.
Since the terms in this sum have phases, they do interfere, and we get a sharp
interference pattern. If we do an experiment in which we don’t observe the spin
of the detected neutron, then both kinds of events can occur; and the separate
probabilities add. The total probability (or counting rate) as a function of angle
then looks like the graph in Fig. 3-6(c).
Let’s review the physics of this experiment. If you could, in principle, distin-
guish the alternative ﬁnal states (even though you do not bother to do so), the
total, ﬁnal probability is obtained by calculating the probability for each state
(not the amplitude) and then adding them together. If you cannot distinguish the
ﬁnal states even in principle, then the probability amplitudes must be summed
before taking the absolute square to ﬁnd the actual probability. The thing you
should notice particularly is that if you were to try to represent the neutron by a
wave alone, you would get the same kind of distribution for the scattering of a
down-spinning neutron as for an up-spinning neutron. You would have to say
that the “wave” would come from all the diﬀerent atoms and interfere just as for
the up-spinning one with the same wavelength. But we know that is not the way
it works. So as we stated earlier, we must be careful not to attribute too much
reality to the waves in space. They are useful for certain problems but not for all.
3-4 Identical particles
The next experiment we will describe is one which shows one of the beautiful
consequences of quantum mechanics. It again involves a physical situation in
which a thing can happen in two indistinguishable ways, so that there is an
interference of amplitudes—as is always true in such circumstances. We are
going to discuss the scattering, at relatively low energy, of nuclei on other nuclei.
We start by thinking of α-particles (which, as you know, are helium nuclei)
bombarding, say, oxygen. To make it easier for us to analyze the reaction, we
will look at it in the center-of-mass system, in which the oxygen nucleus and the
3-16","430"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","81","α-particle have their velocities in opposite directions before the collision and again
in exactly opposite directions after the collision. See Fig. 3-7(a). (The magnitudes
of the velocities are, of course, diﬀerent, since the masses are diﬀerent.) We will
also suppose that there is conservation of energy and that the collision energy
is low enough that neither particle is broken up or left in an excited state. The
reason that the two particles deﬂect each other is, of course, that each particle
carries a positive charge and, classically speaking, there is an electrical repulsion
as they go by. The scattering will happen at diﬀerent angles with diﬀerent
probabilities, and we would like to discuss something about the angle dependence
of such scatterings. (It is possible, of course, to calculate this thing classically,
and it is one of the most remarkable accidents of quantum mechanics that the
answer to this problem comes out the same as it does classically. This is a curious
point because it happens for no other force except the inverse square law—so it
is indeed an accident.)
The probability of scattering in diﬀerent directions can be measured by an
experiment as shown in Fig. 3-7(a). The counter at position 1 could be designed
to detect only α-particles; the counter at position 2 could be designed to detect
only oxygen—just as a check. (In the laboratory system the detectors would
not be opposite; but in the CM system they are.) Our experiment consists in
measuring the probability of scattering in various directions. Let’s call f(θ) the
amplitude to scatter into the counters when they are at the angle θ; then |f(θ)|2
will be our experimentally determined probability.
Now we could set up another experiment in which our counters would respond
to either the α-particle or the oxygen nucleus. Then we have to work out what
happens when we do not bother to distinguish which particles are counted. Of
course, if we are to get an oxygen in the position θ, there must be an α-particle
on the opposite side at the angle (π −θ), as shown in Fig. 3-7(b). So if f(θ) is the
amplitude for α-scattering through the angle θ, then f(π −θ) is the amplitude
for oxygen scattering through the angle θ.† Thus, the probability for having some
particle in the detector at position 1 is:
Probability of some particle in D1 = |f(θ)|2 + |f(π −θ)|2.
(3.14)
† In general, a scattering direction should, of course, be described by two angles, the polar
angle φ, as well as the azimuthal angle θ. We would then say that an oxygen nucleus at (θ, φ)
means that the α-particle is at (π −θ, φ + π). However, for Coulomb scattering (and for many
other cases), the scattering amplitude is independent of φ. Then the amplitude to get an
oxygen at θ is the same as the amplitude to get the α-particle at (π −θ).
3-17","487"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","82","(a)
D1
D2
α-PARTICLE
OXYGEN
α
O
θ
(b)
D1
D2
α-PARTICLE
OXYGEN
O
α
θ
π −θ
Fig. 3-7. The scattering of α-particles from oxygen nuclei, as seen in
the center-of-mass system.
3-18","34"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","83","Note that the two states are distinguishable in principle. Even though in this
experiment we do not distinguish them, we could.
According to the earlier
discussion, then, we must add the probabilities, not the amplitudes.
The result given above is correct for a variety of target nuclei—for α-particles
on oxygen, on carbon, on beryllium, on hydrogen. But it is wrong for α-particles
on α-particles. For the one case in which both particles are exactly the same,
the experimental data disagree with the prediction of (3.14). For example, the
scattering probability at 90◦is exactly twice what the above theory predicts and
has nothing to do with the particles being “helium” nuclei. If the target is He3,
but the projectiles are α-particles (He4), then there is agreement. Only when
the target is He4—so its nuclei are identical with the incoming α-particle—does
the scattering vary in a peculiar way with angle.
Perhaps you can already see the explanation. There are two ways to get
an α-particle into the counter: by scattering the bombarding α-particle at an
angle θ, or by scattering it at an angle of (π −θ). How can we tell whether
the bombarding particle or the target particle entered the counter? The answer
is that we cannot. In the case of α-particles with α-particles there are two
alternatives that cannot be distinguished. Here, we must let the probability
amplitudes interfere by addition, and the probability of ﬁnding an α-particle in
the counter is the square of their sum:
Probability of an α-particle at D1 = |f(θ) + f(π −θ)|2.
(3.15)
This is quite a diﬀerent result than that in Eq. (3.14). We can take an
angle of π/2 as an example, because it is easy to ﬁgure out. For θ = π/2,
we obviously have f(θ) = f(π −θ), so the probability in Eq. (3.15) becomes
|f(π/2) + f(π/2)|2 = 4|f(π/2)|2.
On the other hand, if they did not interfere, the result of Eq. (3.14) gives
only 2|f(π/2)|2. So there is twice as much scattering at 90◦as we might have
expected. Of course, at other angles the results will also be diﬀerent. And so
you have the unusual result that when particles are identical, a certain new
thing happens that doesn’t happen when particles can be distinguished. In the
mathematical description you must add the amplitudes for alternative process in
which the two particles simply exchange roles and there is an interference.
An even more perplexing thing happens when we do the same kind of experi-
ment by scattering electrons on electrons, or protons on protons. Neither of the
above results is then correct! For these particles, we must invoke still a new rule,
3-19","444"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","84","a most peculiar rule, which is the following: When you have a situation in which
the identity of the electron which is arriving at a point is exchanged with another
one, the new amplitude interferes with the old one with an opposite phase. It is
interference all right, but with a minus sign. In the case of α-particles, when you
exchange the α-particle entering the detector, the interfering amplitudes interfere
with the positive sign. In the case of electrons, the interfering amplitudes for
exchange interfere with a negative sign. Except for another detail to be discussed
below, the proper equation for electrons in an experiment like the one shown in
Fig. 3-8 is
Probability of e at D1 = |f(θ) −f(π −θ)|2.
(3.16)
The above statement must be qualiﬁed, because we have not considered
the spin of the electron (α-particles have no spin). The electron spin may be
considered to be either “up” or “down” with respect to the plane of the scattering.
If the energy of the experiment is low enough, the magnetic forces due to the
currents will be small and the spin will not be aﬀected. We will assume that this
is the case for the present analysis, so that there is no chance that the spins are
changed during the collision. Whatever spin the electron has, it carries along
with it. Now you see there are many possibilities. The bombarding and target
particles can have both spins up, both down, or opposite spins. If both spins are
up, as in Fig. 3-8 (or if both spins are down), the same will be true of the recoil
particles and the amplitude for the process is the diﬀerence of the amplitudes for
the two possibilities shown in Fig. 3-8(a) and (b). The probability of detecting
an electron in D1 is then given by Eq. (3.16).
Suppose, however, the “bombarding” spin is up and the “target” spin is
down. The electron entering counter 1 can have spin up or spin down, and by
measuring this spin we can tell whether it came from the bombarding beam or
from the target. The two possibilities are shown in Fig. 3-9(a) and (b); they are
distinguishable in principle, and hence there will be no interference—merely an
addition of the two probabilities. The same argument holds if both of the original
spins are reversed—that is, if the left-hand spin is down and the right-hand spin
is up.
Now if we take our electrons at random—as from a tungsten ﬁlament in which
the electrons are completely unpolarized—then the odds are ﬁfty-ﬁfty that any
particular electron comes out with spin up or spin down. If we don’t bother to
measure the spin of the electrons at any point in the experiment, we have what we
call an unpolarized experiment. The results for this experiment are best calculated
3-20","473"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","85","(a)
D1
D2
ELECTRON
SPIN
UP
ELECTRON
SPIN
UP
SPIN
UP
SPIN
UP
θ
(b)
D1
D2
ELECTRON
SPIN
UP
ELECTRON
SPIN
UP
SPIN
UP
SPIN
UP
θ
π −θ
Fig. 3-8. The scattering of electrons on electrons. If the incoming
electrons have parallel spins, the processes (a) and (b) are indistinguish-
able.
3-21","54"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","86","(a)
D1
D2
ELECTRON
SPIN
UP
ELECTRON
SPIN
DOWN
SPIN
UP
SPIN
DOWN
θ
(b)
D1
D2
ELECTRON
SPIN
UP
ELECTRON
SPIN
DOWN
SPIN
DOWN
SPIN
UP
θ
π −θ
Fig. 3-9. The scattering of electrons with antiparallel spins.
3-22","40"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","87","Table 3-1
Scattering of unpolarized spin one-half particles
Fraction
Spin of
Spin of
Spin at
Spin at
Probability
of cases
particle 1
particle 2
D1
D2
1
4
up
up
up
up
|f(θ) −f(π −θ)|2
1
4
down
down
down
down
|f(θ) −f(π −θ)|2
1
4
up
down

up
down
|f(θ)|2
down
up
|f(π −θ)|2
1
4
down
up

up
down
|f(π −θ)|2
down
up
|f(θ)|2
Total probability = 1
2|f(θ) −f(π −θ)|2 + 1
2|f(θ)|2 + 1
2|f(π −θ)|2
by listing all of the various possibilities as we have done in Table 3-1. A separate
probability is computed for each distinguishable alternative. The total probability
is then the sum of all the separate probabilities. Note that for unpolarized beams
the result for θ = π/2 is one-half that of the classical result with independent
particles. The behavior of identical particles has many interesting consequences;
we will discuss them in greater detail in the next chapter.
3-23","159"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","88","4
Identical Particles
Review: Blackbody radiation in:
Chapter 41, Vol. I, The Brownian Movement
Chapter 42, Vol. I, Applications of Kinetic Theory
4-1 Bose particles and Fermi particles
In the last chapter we began to consider the special rules for the interference
that occurs in processes with two identical particles.
By identical particles
we mean things like electrons which can in no way be distinguished one from
another. If a process involves two particles that are identical, reversing which
one arrives at a counter is an alternative which cannot be distinguished and—
like all cases of alternatives which cannot be distinguished—interferes with the
original, unexchanged case. The amplitude for an event is then the sum of the
two interfering amplitudes; but, interestingly enough, the interference is in some
cases with the same phase and, in others, with the opposite phase.
Suppose we have a collision of two particles a and b in which particle a
scatters in the direction 1 and particle b scatters in the direction 2, as sketched in
Fig. 4-1(a). Let’s call f(θ) the amplitude for this process; then the probability P1
of observing such an event is proportional to |f(θ)|2. Of course, it could also
happen that particle b scattered into counter 1 and particle a went into counter 2,
as shown in Fig. 4-1(b). Assuming that there are no special directions deﬁned by
spins or such, the probability P2 for this process is just |f(π −θ)|2, because it is
just equivalent to the ﬁrst process with counter 1 moved over to the angle π −θ.
You might also think that the amplitude for the second process is just f(π −θ).
But that is not necessarily so, because there could be an arbitrary phase factor.
That is, the amplitude could be
eiδf(π −θ).
Such an amplitude still gives a probability P2 equal to |f(π −θ)|2.
4-1","310"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","89","(a)
a
b
1
2
θ
(b)
a
b
1
2
θ
π −θ
Fig. 4-1. In the scattering of two identical particles, the processes (a)
and (b) are indistinguishable.
Now let’s see what happens if a and b are identical particles. Then the two
diﬀerent processes shown in the two diagrams of Fig. 4-1 cannot be distinguished.
There is an amplitude that either a or b goes into counter 1, while the other
goes into counter 2. This amplitude is the sum of the amplitudes for the two
processes shown in Fig. 4-1. If we call the ﬁrst one f(θ), then the second one
is eiδf(π −θ), where now the phase factor is very important because we are going
4-2","121"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","90","to be adding two amplitudes. Suppose we have to multiply the amplitude by
a certain phase factor when we exchange the roles of the two particles. If we
exchange them again we should get the same factor again. But we are then back
to the ﬁrst process. The phase factor taken twice must bring us back where we
started—its square must be equal to 1. There are only two possibilities: eiδ is
equal to +1, or is equal to −1. Either the exchanged case contributes with the
same sign, or it contributes with the opposite sign. Both cases exist in nature,
each for a diﬀerent class of particles. Particles which interfere with a positive sign
are called Bose particles and those which interfere with a negative sign are called
Fermi particles. The Bose particles are the photon, the mesons, and the graviton.
The Fermi particles are the electron, the muon, the neutrinos, the nucleons, and
the baryons. We have, then, that the amplitude for the scattering of identical
particles is:
Bose particles:
(Amplitude direct) + (Amplitude exchanged).
(4.1)
Fermi particles:
(Amplitude direct) −(Amplitude exchanged).
(4.2)
For particles with spin—like electrons—there is an additional complication.
We must specify not only the location of the particles but the direction of their
spins. It is only for identical particles with identical spin states that the amplitudes
interfere when the particles are exchanged. If you think of the scattering of
unpolarized beams—which are a mixture of diﬀerent spin states—there is some
extra arithmetic.
Now an interesting problem arises when there are two or more particles bound
tightly together. For example, an α-particle has four particles in it—two neutrons
and two protons. When two α-particles scatter, there are several possibilities.
It may be that during the scattering there is a certain amplitude that one of
the neutrons will leap across from one α-particle to the other, while a neutron
from the other α-particle leaps the other way so that the two alphas which come
out of the scattering are not the original ones—there has been an exchange of a
pair of neutrons. See Fig. 4-2. The amplitude for scattering with an exchange
of a pair of neutrons will interfere with the amplitude for scattering with no
such exchange, and the interference must be with a minus sign because there
has been an exchange of one pair of Fermi particles. On the other hand, if the
relative energy of the two α-particles is so low that they stay fairly far apart—say,
4-3","416"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","91","(a)
α-particle
NEUTRON
PROTON
(b)
Fig. 4-2. The scattering of two α-particles. In (a) the two particles
retain their identity; in (b) a neutron is exchanged during the collision.
4-4","30"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","92","due to the Coulomb repulsion—and there is never any appreciable probability
of exchanging any of the internal particles, we can consider the α-particle as
a simple object, and we do not need to worry about its internal details. In
such circumstances, there are only two contributions to the scattering amplitude.
Either there is no exchange, or all four of the nucleons are exchanged in the
scattering. Since the protons and the neutrons in the α-particle are all Fermi
particles, an exchange of any pair reverses the sign of the scattering amplitude.
So long as there are no internal changes in the α-particles, interchanging the two
α-particles is the same as interchanging four pairs of Fermi particles. There is
a change in sign for each pair, so the net result is that the amplitudes combine
with a positive sign. The α-particle behaves like a Bose particle.
So the rule is that composite objects, in circumstances in which the composite
object can be considered as a single object, behave like Fermi particles or Bose
particles, depending on whether they contain an odd number or an even number
of Fermi particles.
All the elementary Fermi particles we have mentioned—such as the electron,
the proton, the neutron, and so on—have a spin j = 1/2. If several such Fermi
particles are put together to form a composite object, the resulting spin may
be either integral or half-integral. For example, the common isotope of helium,
He4, which has two neutrons and two protons, has a spin of zero, whereas Li7,
which has three protons and four neutrons, has a spin of 3/2. We will learn later
the rules for compounding angular momentum, and will just mention now that
every composite object which has a half-integral spin imitates a Fermi particle,
whereas every composite object with an integral spin imitates a Bose particle.
This brings up an interesting question: Why is it that particles with half-
integral spin are Fermi particles whose amplitudes add with the minus sign,
whereas particles with integral spin are Bose particles whose amplitudes add
with the positive sign? We apologize for the fact that we cannot give you an
elementary explanation. An explanation has been worked out by Pauli from
complicated arguments of quantum ﬁeld theory and relativity. He has shown
that the two must necessarily go together, but we have not been able to ﬁnd a
way of reproducing his arguments on an elementary level. It appears to be one of
the few places in physics where there is a rule which can be stated very simply,
but for which no one has found a simple and easy explanation. The explanation
is deep down in relativistic quantum mechanics. This probably means that we
do not have a complete understanding of the fundamental principle involved. For
the moment, you will just have to take it as one of the rules of the world.
4-5","483"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","93","4-2 States with two Bose particles
Now we would like to discuss an interesting consequence of the addition rule
for Bose particles. It has to do with their behavior when there are several particles
present. We begin by considering a situation in which two Bose particles are
scattered from two diﬀerent scatterers. We won’t worry about the details of the
scattering mechanism. We are interested only in what happens to the scattered
particles. Suppose we have the situation shown in Fig. 4-3. The particle a is
scattered into the state 1. By a state we mean a given direction and energy, or
some other given condition. The particle b is scattered into the state 2. We want
to assume that the two states 1 and 2 are nearly the same. (What we really want
to ﬁnd out eventually is the amplitude that the two particles are scattered into
identical directions, or states; but it is best if we think ﬁrst about what happens
if the states are almost the same and then work out what happens when they
become identical.)
a
b
1
2
Fig. 4-3. A double scattering into nearby ﬁnal states.
Suppose that we had only particle a; then it would have a certain amplitude
for scattering in direction 1, say ⟨1 | a⟩. And particle b alone would have the
amplitude ⟨2 | b⟩for landing in direction 2. If the two particles are not identical,
the amplitude for the two scatterings to occur at the same time is just the product
⟨1 | a⟩⟨2 | b⟩.
4-6","260"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","94","The probability for such an event is then
|⟨1 | a⟩⟨2 | b⟩|2,
which is also equal to
|⟨1 | a⟩|2|⟨2 | b⟩|2.
To save writing for the present arguments, we will sometimes set
⟨1 | a⟩= a1,
⟨2 | b⟩= b2.
Then the probability of the double scattering is
|a1|2|b2|2.
It could also happen that particle b is scattered into direction 1, while particle a
goes into direction 2. The amplitude for this process is
⟨2 | a⟩⟨1 | b⟩,
and the probability of such an event is
|⟨2 | a⟩⟨1 | b⟩|2 = |a2|2|b1|2.
Imagine now that we have a pair of tiny counters that pick up the two scattered
particles. The probability P2 that they will pick up two particles together is just
the sum
P2 = |a1|2|b2|2 + |a2|2|b1|2.
(4.3)
Now let’s suppose that the directions 1 and 2 are very close together. We
expect that a should vary smoothly with direction, so a1 and a2 must approach
each other as 1 and 2 get close together. If they are close enough, the amplitudes
a1 and a2 will be equal. We can set a1 = a2 and call them both just a; similarly,
we set b1 = b2 = b. Then we get that
P2 = 2|a|2|b|2.
(4.4)
Now suppose, however, that a and b are identical Bose particles. Then the
process of a going into 1 and b going into 2 cannot be distinguished from the
exchanged process in which a goes into 2 and b goes into 1. In this case the
4-7","259"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","95","amplitudes for the two diﬀerent processes can interfere. The total amplitude to
obtain a particle in each of the two counters is
⟨1 | a⟩⟨2 | b⟩+ ⟨2 | a⟩⟨1 | b⟩.
(4.5)
And the probability that we get a pair is the absolute square of this amplitude,
P2 = |a1b2 + a2b1|2 = 4|a|2|b|2.
(4.6)
We have the result that it is twice as likely to ﬁnd two identical Bose particles
scattered into the same state as you would calculate assuming the particles were
diﬀerent.
Although we have been considering that the two particles are observed in
separate counters, this is not essential—as we can see in the following way. Let’s
imagine that both the directions 1 and 2 would bring the particles into a single
small counter which is some distance away. We will let the direction 1 be deﬁned
by saying that it heads toward the element of area dS1 of the counter. Direction 2
heads toward the surface element dS2 of the counter. (We imagine that the
counter presents a surface at right angles to the line from the scatterings.) Now
we cannot give a probability that a particle will go into a precise direction or
to a particular point in space. Such a thing is impossible—the chance for any
exact direction is zero. When we want to be so speciﬁc, we shall have to deﬁne
our amplitudes so that they give the probability of arriving per unit area of a
counter. Suppose that we had only particle a; it would have a certain amplitude
for scattering in direction 1. Let’s deﬁne ⟨1 | a⟩= a1 to be the amplitude that a
will scatter into a unit area of the counter in the direction 1. In other words, the
scale of a1 is chosen—we say it is “normalized” so that the probability that it
will scatter into an element of area dS1 is
|⟨1 | a⟩|2 dS1 = |a1|2 dS1.
(4.7)
If our counter has the total area ∆S, and we let dS1 range over this area, the
total probability that the particle a will be scattered into the counter is
Z
∆S
|a1|2 dS1.
(4.8)
As before, we want to assume that the counter is suﬃciently small so that the
amplitude a1 doesn’t vary signiﬁcantly over the surface of the counter; a1 is then
4-8","390"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","96","a constant amplitude which we can call a. Then the probability that particle a
is scattered somewhere into the counter is
pa = |a|2 ∆S.
(4.9)
In the same way, we will have that the probability that particle b—when it is
alone—scatters into some element of area, say dS2, is
|b2|2 dS2.
(We use dS2 instead of dS1 because we will later want a and b to go into diﬀerent
directions.) Again we set b2 equal to the constant amplitude b; then the probability
that particle b is counted in the detector is
pb = |b|2 ∆S.
(4.10)
Now when both particles are present, the probability that a is scattered
into dS1 and b is scattered into dS2 is
|a1b2|2 dS1 dS2 = |a|2|b|2 dS1 dS2.
(4.11)
If we want the probability that both a and b get into the counter, we integrate
both dS1 and dS2 over ∆S and ﬁnd that
P2 = |a|2|b|2 (∆S)2.
(4.12)
We notice, incidentally, that this is just equal to pa ·pb, just as you would suppose
assuming that the particles a and b act independently of each other.
When the two particles are identical, however, there are two indistinguishable
possibilities for each pair of surface elements dS1 and dS2. Particle a going
into dS2 and particle b going into dS1 is indistinguishable from a into dS1 and b
into dS2, so the amplitudes for these processes will interfere. (When we had two
diﬀerent particles above—although we did not in fact care which particle went
where in the counter—we could, in principle, have found out; so there was no
interference. For identical particles we cannot tell, even in principle.) We must
write, then, that the probability that the two particles arrive at dS1 and dS2 is
|a1b2 + a2b1|2 dS1 dS2.
(4.13)
Now, however, when we integrate over the area of the counter, we must be careful.
If we let dS1 and dS2 range over the whole area ∆S, we would count each part
4-9","331"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","97","of the area twice since (4.13) contains everything that can happen with any pair
of surface elements dS1 and dS2.† We can still do the integral that way, if we
correct for the double counting by dividing the result by 2. We get then that P2
for identical Bose particles is
P2(Bose) = 1
2{4|a|2|b|2 (∆S)2} = 2|a|2|b|2 (∆S)2.
(4.14)
Again, this is just twice what we got in Eq. (4.12) for distinguishable particles.
If we imagine for a moment that we knew that the b channel had already
sent its particle into the particular direction, we can say that the probability that
a second particle will go into the same direction is twice as great as we would
have expected if we had calculated it as an independent event. It is a property
of Bose particles that if there is already one particle in a condition of some kind,
the probability of getting a second one in the same condition is twice as great
as it would be if the ﬁrst one were not already there. This fact is often stated
in the following way: If there is already one Bose particle in a given state, the
amplitude for putting an identical one on top of it is
√
2 greater than if it weren’t
there. (This is not a proper way of stating the result from the physical point of
view we have taken, but if it is used consistently as a rule, it will, of course, give
the correct result.)
4-3 States with n Bose particles
Let’s extend our result to a situation in which there are n particles present.
We imagine the circumstance shown in Fig. 4-4. We have n particles a, b, c, . . . ,
which are scattered and end up in the directions 1, 2, 3, . . . , n. All n directions
are headed toward a small counter a long distance away. As in the last section, we
choose to normalize all the amplitudes so that the probability that each particle
acting alone would go into an element of surface dS of the counter is
|⟨
⟩|2 dS.
First, let’s assume that the particles are all distinguishable; then the proba-
bility that n particles will be counted together in n diﬀerent surface elements
is
|a1b2c3 · · ·|2 dS1 dS2 dS3 · · ·
(4.15)
† In (4.11) interchanging dS1 and dS2 gives a diﬀerent event, so both surface elements
should range over the whole area of the counter. In (4.13) we are treating dS1 and dS2 as a
pair and including everything that can happen. If the integrals include again what happens
when dS1 and dS2 are reversed, everything is counted twice.
4-10","454"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","98","a
b
c
. . .
1
2
3
. . .
n
Fig. 4-4. The scattering of n particles into nearby states.
Again we take that the amplitudes don’t depend on where dS is located in the
counter (assumed small) and call them simply a, b, c, . . . The probability (4.15)
becomes
|a|2|b|2|c|2 · · · dS1 dS2 dS3 · · ·
(4.16)
Integrating each dS over the surface ∆S of the counter, we have that Pn(diﬀerent),
the probability of counting n diﬀerent particles at once, is
Pn(diﬀerent) = |a|2|b|2|c|2 · · · (∆S)n.
(4.17)
This is just the product of the probabilities for each particle to enter the counter
separately. They all act independently—the probability for one to enter does not
depend on how many others are also entering.
Now suppose that all the particles are identical Bose particles. For each set
of directions 1, 2, 3, . . . there are many indistinguishable possibilities. If there
were, for instance, just three particles, we would have the following possibilities:
a →1
a →1
a →2
b →2
b →3
b →1
c →3
c →2
c →3
4-11","192"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","99","a →2
a →3
a →3
b →3
b →1
b →2
c →1
c →2
c →1
There are six diﬀerent combinations. With n particles, there are n! diﬀerent, but
indistinguishable, possibilities for which we must add amplitudes. The probability
that n particles will be counted in n surface elements is then
|a1b2c3 · · · + a1b3c2 · · · + a2b1c3 · · ·
+ a2b3c1 · · · + etc. + etc.|2 dS1 dS2 dS3 · · · dSn.
(4.18)
Once more we assume that all the directions are so close that we can set a1 =
a2 = · · · = an = a, and similarly for b, c, . . . ; the probability of (4.18) becomes
|n!abc · · ·|2dS1 dS2 · · · dSn.
(4.19)
When we integrate each dS over the area ∆S of the counter, each possible
product of surface elements is counted n! times; we correct for this by dividing
by n! and get
Pn(Bose) = 1
n! |n!abc · · ·|2(∆S)n
or
Pn(Bose) = n! |abc · · ·|2(∆S)n.
(4.20)
Comparing this result with Eq. (4.17), we see that the probability of counting
n Bose particles together is n! greater than we would calculate assuming that
the particles were all distinguishable. We can summarize our result this way:
Pn(Bose) = n! Pn(diﬀerent).
(4.21)
Thus, the probability in the Bose case is larger by n! than you would calculate
assuming that the particles acted independently.
We can see better what this means if we ask the following question: What is
the probability that a Bose particle will go into a particular state when there
are already n others present? Let’s call the newly added particle w. If we have
(n + 1) particles, including w, Eq. (4.20) becomes
Pn+1(Bose) = (n + 1)! |abc · · · w|2(∆S)n+1.
(4.22)
4-12","312"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","100","We can write this as
Pn+1(Bose) = {(n + 1)|w|2 ∆S}n! |abc · · ·|2(∆S)n
or
Pn+1(Bose) = (n + 1)|w|2 ∆S Pn(Bose).
(4.23)
We can look at this result in the following way: The number |w|2 ∆S is the
probability for getting particle w into the detector if no other particles were
present; Pn(Bose) is the chance that there are already n other Bose particles
present. So Eq. (4.23) says that when there are n other identical Bose particles
present, the probability that one more particle will enter the same state is
enhanced by the factor (n + 1). The probability of getting a boson, where there
are already n, is (n + 1) times stronger than it would be if there were none before.
The presence of the other particles increases the probability of getting one more.
4-4 Emission and absorption of photons
Throughout our discussion we have talked about a process like the scattering
of α-particles. But that is not essential; we could have been speaking of the
creation of particles, as for instance the emission of light. When the light is
emitted, a photon is “created.” In such a case, we don’t need the incoming lines in
Fig. 4-4; we can consider merely that there are some atoms emitting n photons,
as in Fig. 4-5. So our result can also be stated: The probability that an atom will
emit a photon into a particular ﬁnal state is increased by the factor (n + 1) if
there are already n photons in that state.
1
2
3
. . .
n
Fig. 4-5. The creation of n photons in nearby states.
4-13","275"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","101","People like to summarize this result by saying that the amplitude to emit
a photon is increased by the factor √n + 1 when there are already n photons
present. It is, of course, another way of saying the same thing if it is understood
to mean that this amplitude is just to be squared to get the probability.
It is generally true in quantum mechanics that the amplitude to get from any
condition φ to any other condition χ is the complex conjugate of the amplitude
to get from χ to φ:
⟨χ | φ⟩= ⟨φ | χ⟩∗.
(4.24)
We will learn about this law a little later, but for the moment we will just assume
it is true. We can use it to ﬁnd out how photons are scattered or absorbed out of
a given state. We have that the amplitude that a photon will be added to some
state, say i, when there are already n photons present is, say,
⟨n + 1 | n⟩=
√
n + 1 a,
(4.25)
where a = ⟨i | a⟩is the amplitude when there are no others present. Using
Eq. (4.24), the amplitude to go the other way—from (n + 1) photons to n—is
⟨n | n + 1⟩=
√
n + 1 a∗.
(4.26)
This isn’t the way people usually say it; they don’t like to think of going from
(n + 1) to n, but prefer always to start with n photons present. Then they say
that the amplitude to absorb a photon when there are n present—in other words,
to go from n to (n −1)—is
⟨n −1 | n⟩= √n a∗.
(4.27)
which is, of course, just the same as Eq. (4.26). Then they have trouble trying to
remember when to use √n or √n + 1. Here’s the way to remember: The factor
is always the square root of the largest number of photons present, whether it
is before or after the reaction. Equations (4.25) and (4.26) show that the law is
really symmetric—it only appears unsymmetric if you write it as Eq. (4.27).
There are many physical consequences of these new rules; we want to describe
one of them having to do with the emission of light. Suppose we imagine a
situation in which photons are contained in a box—you can imagine a box
with mirrors for walls. Now say that in the box we have n photons, all of the
same state—the same frequency, direction, and polarization—so they can’t be
4-14","418"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","102","distinguished, and that also there is an atom in the box that can emit another
photon into the same state. Then the probability that it will emit a photon is
(n + 1)|a|2,
(4.28)
and the probability that it will absorb a photon is
n|a|2,
(4.29)
where |a|2 is the probability it would emit if no photons were present. We have
already discussed these rules in a somewhat diﬀerent way in Chapter 42 of Vol. I.
Equation (4.29) says that the probability that an atom will absorb a photon and
make a transition to a higher energy state is proportional to the intensity of the
light shining on it. But, as Einstein ﬁrst pointed out, the rate at which an atom
will make a transition downward has two parts. There is the probability that
it will make a spontaneous transition |a|2, plus the probability of an induced
transition n|a|2, which is proportional to the intensity of the light—that is, to
the number of photons present. Furthermore, as Einstein said, the coeﬃcients of
absorption and of induced emission are equal and are related to the probability
of spontaneous emission. What we learn here is that if the light intensity is
measured in terms of the number of photons present (instead of as the energy
per unit area, and per sec), the coeﬃcients of absorption of induced emission and
of spontaneous emission are all equal. This is the content of the relation between
the Einstein coeﬃcients A and B of Chapter 42, Vol. I, Eq. (42.18).
4-5 The blackbody spectrum
We would like to use our rules for Bose particles to discuss once more the
spectrum of blackbody radiation (see Chapter 42, Vol. I). We will do it by ﬁnding
out how many photons there are in a box if the radiation is in thermal equilibrium
with some atoms in the box. Suppose that for each light frequency ω, there are
a certain number N of atoms which have two energy states separated by the
energy ∆E = ℏω. See Fig. 4-6. We’ll call the lower-energy state the “ground”
state and the upper state the “excited” state. Let Ng and Ne be the average
numbers of atoms in the ground and excited states; then in thermal equilibrium
at the temperature T, we have from statistical mechanics that
Ne
Ng
= e−∆E/kT = e−ℏω/kT .
(4.30)
4-15","394"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","103","(a)
GROUND STATE g
e
∆E = ¯hω
(b)
GROUND STATE g
e
∆E = ¯hω
Fig. 4-6. Radiation and absorption of a photon with the frequency ω.
Each atom in the ground state can absorb a photon and go into the excited
state, and each atom in the excited state can emit a photon and go to the ground
state. In equilibrium, the rates for these two processes must be equal. The rates
are proportional to the probability for the event and to the number of atoms
present. Let’s let n be the average number of photons present in a given state
with the frequency ω. Then the absorption rate from that state is Ngn|a|2, and
the emission rate into that state is Ne(n + 1)|a|2. Setting the two rates equal,
we have that
Ngn = Ne(n + 1).
(4.31)
Combining this with Eq. (4.30), we have
n
n + 1 = e−ℏω/kT .
Solving for n, we have
n =
1
eℏω/kT −1,
(4.32)
which is the mean number of photons in any state with frequency ω, for a cavity
in thermal equilibrium. Since each photon has the energy ℏω, the energy in the
4-16","197"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","104","photons of a given state is nℏω, or
ℏω
eℏω/kT −1.
(4.33)
Incidentally, we once found a similar equation in another context [Chapter 41,
Vol. I, Eq. (41.15)]. You remember that for any harmonic oscillator—such as a
weight on a spring—the quantum mechanical energy levels are equally spaced with
a separation ℏω, as drawn in Fig. 4-7. If we call the energy of the nth level nℏω,
we ﬁnd that the mean energy of such an oscillator is also given by Eq. (4.33). Yet
this equation was derived here for photons, by counting particles, and it gives the
same results. That is one of the marvelous miracles of quantum mechanics. If one
begins by considering a kind of state or condition for Bose particles which do not
interact with each other (we have assumed that the photons do not interact with
each other), and then considers that into this state there can be put either zero,
or one, or two, . . . up to any number n of particles, one ﬁnds that this system
behaves for all quantum mechanical purposes exactly like a harmonic oscillator.
By such an oscillator we mean a dynamic system like a weight on a spring or a
standing wave in a resonant cavity. And that is why it is possible to represent the
electromagnetic ﬁeld by photon particles. From one point of view, we can analyze
the electromagnetic ﬁeld in a box or cavity in terms of a lot of harmonic oscillators,
treating each mode of oscillation according to quantum mechanics as a harmonic
E
GROUND STATE
0
¯hω
2¯hω
3¯hω
4¯hω
5¯hω
...
Fig. 4-7. The energy levels of a harmonic oscillator.
4-17","280"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","105","oscillator. From a diﬀerent point of view, we can analyze the same physics in
terms of identical Bose particles. And the results of both ways of working are
always in exact agreement. There is no way to make up your mind whether the
electromagnetic ﬁeld is really to be described as a quantized harmonic oscillator
or by giving how many photons there are in each condition. The two views turn
out to be mathematically identical. So in the future we can speak either about the
number of photons in a particular state in a box or the number of the energy level
associated with a particular mode of oscillation of the electromagnetic ﬁeld. They
are two ways of saying the same thing. The same is true of photons in free space.
They are equivalent to oscillations of a cavity whose walls have receded to inﬁnity.
We have computed the mean energy in any particular mode in a box at the
temperature T; we need only one more thing to get the blackbody radiation law:
We need to know how many modes there are at each energy. (We assume that
for every mode there are some atoms in the box—or in the walls—which have
energy levels that can radiate into that mode, so that each mode can get into
thermal equilibrium.) The blackbody radiation law is usually stated by giving
the energy per unit volume carried by the light in a small frequency interval from
ω to ω + ∆ω. So we need to know how many modes there are in a box with
frequencies in the interval ∆ω. Although this question continually comes up in
quantum mechanics, it is purely a classical question about standing waves.
We will get the answer only for a rectangular box. It comes out the same for
a box of any shape, but it’s very complicated to compute for the arbitrary case.
Also, we are only interested in a box whose dimensions are very large compared
with a wavelength of the light. Then there are billions and billions of modes;
there will be many in any small frequency interval ∆ω, so we can speak of the
“average number” in any ∆ω at the frequency ω. Let’s start by asking how many
modes there are in a one-dimensional case—as for waves on a stretched string.
You know that each mode is a sine wave that has to go to zero at both ends; in
other words, there must be an integral number of half-wavelengths in the length
of the line, as shown in Fig. 4-8. We prefer to use the wave number k = 2π/λ;
calling kj the wave number of the jth mode, we have that
kj = jπ
L ,
(4.34)
where j is any integer. The separation δk between successive modes is
δk = kj+1 −kj = π
L.
4-18","477"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","106","2L
λ
1
2
...
...
j
L
Fig. 4-8. The standing wave modes on a line.
We want to assume that kL is so large that in a small interval ∆k, there are
many modes. Calling ∆N the number of modes in the interval ∆k, we have
∆N = ∆k
δk = L
π ∆k.
(4.35)
Now theoretical physicists working in quantum mechanics usually prefer to
say that there are one-half as many modes; they write
∆N = L
2π ∆k.
(4.36)
We would like to explain why. They usually like to think in terms of travelling
waves—some going to the right (with a positive k) and some going to the left
(with a negative k). But a “mode” is a standing wave which is the sum of two
waves, one going in each direction. In other words, they consider each standing
wave as containing two distinct photon “states.” So if by ∆N, one prefers to mean
the number of photon states of a given k (where now k ranges over positive and
negative values), one should then take ∆N half as big. (All integrals must now
go from k = −∞to k = +∞, and the total number of states up to any given
absolute value of k will come out O.K.) Of course, we are not then describing
standing waves very well, but we are counting modes in a consistent way.
Now we want to extend the results to three dimensions. A standing wave in a
rectangular box must have an integral number of half-waves along each axis. The
4-19","263"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","107","Ly
Lx
kx
ky
k
Fig. 4-9. Standing wave modes in two dimensions.
situation for two of the dimensions is shown in Fig. 4-9. Each wave direction and
frequency is described by a vector wave number k, whose x, y, and z components
must satisfy equations like Eq. (4.34). So we have that
kx = jxπ
Lx
,
ky = jyπ
Ly
,
kz = jzπ
Lz
.
The number of modes with kx in an interval ∆kx is, as before,
Lx
2π ∆kx,
and similarly for ∆ky and ∆kz. If we call ∆N(k) the number of modes for a
vector wave number k whose x-component is between kx and kx + ∆kx, whose
y-component is between ky and ky + ∆ky, and whose z-component is between kz
and kz + ∆kz, then
∆N(k) = LxLyLz
(2π)3
∆kx ∆ky ∆kz.
(4.37)
4-20","142"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","108","The product LxLyLz is equal to the volume V of the box. So we have the
important result that for high frequencies (wavelengths small compared with the
dimensions), the number of modes in a cavity is proportional to the volume V
of the box and to the “volume in k-space” ∆kx ∆ky ∆kz. This result comes up
again and again in many problems and should be memorized:
dN(k) = V
d3k
(2π)3 .
(4.38)
Although we have not proved it, the result is independent of the shape of the
box.
We will now apply this result to ﬁnd the number of photon modes for photons
with frequencies in the range ∆ω. We are just interested in the energy in various
modes—but not interested in the directions of the waves. We would like to know
the number of modes in a given range of frequencies. In a vacuum the magnitude
of k is related to the frequency by
|k| = ω
c .
(4.39)
So in a frequency interval ∆ω, these are all the modes which correspond to k’s
with a magnitude between k and k + ∆k, independent of the direction. The
“volume in k-space” between k and k + ∆k is a spherical shell of volume
4πk2 ∆k.
The number of modes is then
∆N(ω) = V 4πk2 ∆k
(2π)3
.
(4.40)
However, since we are now interested in frequencies, we should substitute k = ω/c,
so we get
∆N(ω) = V 4πω2 ∆ω
(2π)3c3
.
(4.41)
There is one more complication. If we are talking about modes of an electro-
magnetic wave, for any given wave vector k there can be either of two polarizations
(at right angles to each other). Since these modes are independent, we must—for
light—double the number of modes. So we have
∆N(ω) = V ω2 ∆ω
π2c3
(for light).
(4.42)
4-21","308"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","109","We have shown, Eq. (4.33), that each mode (or each “state”) has on the average
the energy
nℏω =
ℏω
eℏω/kT −1.
Multiplying this by the number of modes, we get the energy ∆E in the modes
that lie in the interval ∆ω:
∆E =
ℏω
eℏω/kT −1
V ω2 ∆ω
π2c3
.
(4.43)
This is the law for the frequency spectrum of blackbody radiation, which we have
already found in Chapter 41 of Vol. I. The spectrum is plotted in Fig. 4-10. You
see now that the answer depends on the fact that photons are Bose particles,
which have a tendency to try to get all into the same state (because the amplitude
for doing so is large). You will remember, it was Planck’s study of the blackbody
spectrum (which was a mystery to classical physics), and his discovery of the
formula in Eq. (4.43) that started the whole subject of quantum mechanics.
¯hω/kT
1
2
3
4
5
6
7
8
(π¯h)2
V
 c
kT
3
× dE
dω
0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
Fig. 4-10. The frequency spectrum of radiation in a cavity in thermal
equilibrium, the “blackbody” spectrum.
4-6 Liquid helium
Liquid helium has at low temperatures many odd properties which we cannot
unfortunately take the time to describe in detail right now, but many of them
4-22","227"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","110","arise from the fact that a helium atom is a Bose particle. One of the things is that
liquid helium ﬂows without any viscous resistance. It is, in fact, the ideal “dry”
water we have been talking about in one of the earlier chapters—provided that
the velocities are low enough. The reason is the following. In order for a liquid to
have viscosity, there must be internal energy losses; there must be some way for
one part of the liquid to have a motion that is diﬀerent from that of the rest of the
liquid. This means that it must be possible to knock some of the atoms into states
that are diﬀerent from the states occupied by other atoms. But at suﬃciently
low temperatures, when the thermal motions get very small, all the atoms try
to get into the same condition. So, if some of them are moving along, then all
the atoms try to move together in the same state. There is a kind of rigidity
to the motion, and it is hard to break the motion up into irregular patterns of
turbulence, as would happen, for example, with independent particles. So in a
liquid of Bose particles, there is a strong tendency for all the atoms to go into
the same state—which is represented by the √n + 1 factor we found earlier. (For
a bottle of liquid helium n is, of course, a very large number!) This cooperative
motion does not happen at high temperatures, because then there is suﬃcient
thermal energy to put the various atoms into various diﬀerent higher states. But
at a suﬃciently low temperature there suddenly comes a moment in which all the
helium atoms try to go into the same state. The helium becomes a superﬂuid.
Incidentally, this phenomenon only appears with the isotope of helium which
has atomic weight 4. For the helium isotope of atomic weight 3, the individual
atoms are Fermi particles, and the liquid is a normal ﬂuid. Since superﬂuidity
occurs only with He4, it is evidently a quantum mechanical eﬀect—due to the
Bose nature of the α-particle.
4-7 The exclusion principle
Fermi particles act in a completely diﬀerent way. Let’s see what happens if
we try to put two Fermi particles into the same state. We will go back to our
original example and ask for the amplitude that two identical Fermi particles
will be scattered into almost exactly the same direction. The amplitude that
particle a will go in direction 1 and particle b will go in direction 2 is
⟨1 | a⟩⟨2 | b⟩,
whereas the amplitude that the outgoing directions will be interchanged is
⟨2 | a⟩⟨1 | b⟩.
4-23","447"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","111","Since we have Fermi particles, the amplitude for the process is the diﬀerence of
these two amplitudes:
⟨1 | a⟩⟨2 | b⟩−⟨2 | a⟩⟨1 | b⟩.
(4.44)
Let’s say that by “direction 1” we mean that the particle has not only a certain
direction but also a given direction of its spin, and that “direction 2” is almost
exactly the same as direction 1 and corresponds to the same spin direction. Then
⟨1 | a⟩and ⟨2 | a⟩are nearly equal. (This would not necessarily be true if the
outgoing states 1 and 2 did not have the same spin, because there might be some
reason why the amplitude would depend on the spin direction.) Now if we let
directions 1 and 2 approach each other, the total amplitude in Eq. (4.44) becomes
zero. The result for Fermi particles is much simpler than for Bose particles. It
just isn’t possible at all for two Fermi particles—such as two electrons—to get
into exactly the same state. You will never ﬁnd two electrons in the same position
with their two spins in the same direction. It is not possible for two electrons to
have the same momentum and the same spin directions. If they are at the same
location or with the same state of motion, the only possibility is that they must
be spinning opposite to each other.
What are the consequences of this? There are a number of most remarkable
eﬀects which are a consequence of the fact that two Fermi particles cannot get
into the same state. In fact, almost all the peculiarities of the material world
hinge on this wonderful fact. The variety that is represented in the periodic table
is basically a consequence of this one rule.
Of course, we cannot say what the world would be like if this one rule were
changed, because it is just a part of the whole structure of quantum mechanics,
and it is impossible to say what else would change if the rule about Fermi particles
were diﬀerent. Anyway, let’s just try to see what would happen if only this one rule
were changed. First, we can show that every atom would be more or less the same.
Let’s start with the hydrogen atom. It would not be noticeably aﬀected. The
proton of the nucleus would be surrounded by a spherically symmetric electron
cloud, as shown in Fig. 4-11(a). As we have described in Chapter 2, the electron
is attracted to the center, but the uncertainty principle requires that there be
a balance between the concentration in space and in momentum. The balance
means that there must be a certain energy and a certain spread in the electron
distribution which determines the characteristic dimension of the hydrogen atom.
Now suppose that we have a nucleus with two units of charge, such as the
helium nucleus. This nucleus would attract two electrons, and if they were Bose
4-24","486"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","112","(a)
+
NUCLEUS
ONE
ELECTRON
(b)
+ +
TWO
ELECTRONS
(c)
+
+ +
THREE
ELECTRONS
Fig. 4-11. How atoms might look if electrons behaved like Bose particles.
particles, they would—except for their electric repulsion—both crowd in as close
as possible to the nucleus. A helium atom might look as shown in part (b) of
the ﬁgure. Similarly, a lithium atom which has a triply charged nucleus would
have an electron distribution like that shown in part (c) of Fig. 4-11. Every atom
would look more or less the same—a little round ball with all the electrons sitting
near the nucleus, nothing directional and nothing complicated.
Because electrons are Fermi particles, however, the actual situation is quite
diﬀerent. For the hydrogen atom the situation is essentially unchanged. The
only diﬀerence is that the electron has a spin which we indicate by the little
arrow in Fig. 4-12(a). In the case of a helium atom, however, we cannot put
two electrons on top of each other. But wait, that is only true if their spins are
the same. Two electrons can occupy the same state if their spins are opposite.
So the helium atom does not look much diﬀerent either. It would appear as
shown in part (b) of Fig. 4-12. For lithium, however, the situation becomes quite
diﬀerent. Where can we put the third electron? The third electron cannot go on
top of the other two because both spin directions are occupied. (You remember
that for an electron or any particle with spin 1/2 there are only two possible
directions for the spin.) The third electron can’t go near the place occupied by
the other two, so it must take up a special condition in a diﬀerent kind of state
farther away from the nucleus in part (c) of the ﬁgure. (We are speaking only
in a rather rough way here, because in reality all three electrons are identical;
4-25","320"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","113","(a)
+
NUCLEUS
ONE
ELECTRON
SPIN
(b)
+ +
TWO
ELECTRONS
(c)
+
+ +
Fig. 4-12. Atomic conﬁgurations for real, Fermi-type, spin one-half
electrons.
since we cannot really distinguish which one is which, our picture is only an
approximate one.)
Now we can begin to see why diﬀerent atoms will have diﬀerent chemical
properties. Because the third electron in lithium is farther out, it is relatively
more loosely bound. It is much easier to remove an electron from lithium than
from helium. (Experimentally, it takes 25 electron volts to ionize helium but only
5 electron volts to ionize lithium.) This accounts for the valence of the lithium
atom. The directional properties of the valence have to do with the pattern of
the waves of the outer electron, which we will not go into at the moment. But we
can already see the importance of the so-called exclusion principle—which states
that no two electrons can be found in exactly the same state (including spin).
4-26","165"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","114","The exclusion principle is also responsible for the stability of matter on a large
scale. We explained earlier that the individual atoms in matter did not collapse
because of the uncertainty principle; but this does not explain why it is that two
hydrogen atoms can’t be squeezed together as close as you want—why it is that
all the protons don’t get close together with one big smear of electrons around
them. The answer is, of course, that since no more than two electrons—with
opposite spins—can be in roughly the same place, the hydrogen atoms must
keep away from each other. So the stability of matter on a large scale is really a
consequence of the Fermi particle nature of the electrons.
Of course, if the outer electrons on two atoms have spins in opposite direc-
tions, they can get close to each other. This is, in fact, just the way that the
chemical bond comes about. It turns out that two atoms together will generally
have the lowest energy if there is an electron between them. It is a kind of
an electrical attraction for the two positive nuclei toward the electron in the
middle. It is possible to put two electrons more or less between the two nuclei
so long as their spins are opposite, and the strongest chemical binding comes
about this way. There is no stronger binding, because the exclusion principle
does not allow there to be more than two electrons in the space between the
atoms. We expect the hydrogen molecule to look more or less as shown in
Fig. 4-13.
+
+
Fig. 4-13. The hydrogen molecule.
We want to mention one more consequence of the exclusion principle. You
remember that if both electrons in the helium atom are to be close to the nucleus,
their spins are necessarily opposite. Now suppose that we would like to try to
arrange to have both electrons with the same spin—as we might consider doing
by putting on a fantastically strong magnetic ﬁeld that would try to line up the
spins in the same direction. But then the two electrons could not occupy the
same state in space. One of them would have to take on a diﬀerent geometrical
position, as indicated in Fig. 4-14. The electron which is located farther from
the nucleus has less binding energy. The energy of the whole atom is therefore
4-27","399"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","115","+ +
Fig. 4-14. Helium with one electron in a higher energy state.
quite a bit higher. In other words, when the two spins are opposite, there is a
much stronger total attraction.
So, there is an apparent, enormous force trying to line up spins opposite to
each other when two electrons are close together. If two electrons are trying to go
in the same place, there is a very strong tendency for the spins to become lined
opposite. This apparent force trying to orient the two spins opposite to each other
is much more powerful than the tiny force between the two magnetic moments of
the electrons. You remember when we were speaking of ferromagnetism there
was the mystery of why the electrons in diﬀerent atoms had a strong tendency to
line up parallel. Although there is still no quantitative explanation, it is believed
that what happens is that the electrons around the core of one atom interact
through the exclusion principle with the outer electrons which have become free
to wander throughout the crystal. This interaction causes the spins of the free
electrons and the inner electrons to take on opposite directions. But the free
electrons and the inner atomic electrons can only be opposite provided all the
inner electrons have the same spin direction, as indicated in Fig. 4-15. It seems
probable that it is the eﬀect of the exclusion principle acting indirectly through
the free electrons that gives rise to the strong aligning forces responsible for
ferromagnetism.
We will mention one further example of the inﬂuence of the exclusion principle.
We have said earlier that the nuclear forces are the same between the neutron and
4-28","280"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","116","Fig. 4-15.
The likely mechanism in a ferromagnetic crystal; the
conduction electron is antiparallel to the unpaired inner electrons.
the proton, between the proton and the proton, and between the neutron and the
neutron. Why is it then that a proton and a neutron can stick together to make a
deuterium nucleus, whereas there is no nucleus with just two protons or with just
two neutrons? The deuteron is, as a matter of fact, bound by an energy of about
2.2 million electron volts, yet, there is no corresponding binding between a pair
of protons to make an isotope of helium with the atomic weight 2. Such nuclei
do not exist. The combination of two protons does not make a bound state.
The answer is a result of two eﬀects: ﬁrst, the exclusion principle; and second,
the fact that the nuclear forces are somewhat sensitive to the direction of spin.
The force between a neutron and a proton is attractive and somewhat stronger
when the spins are parallel than when they are opposite. It happens that these
forces are just diﬀerent enough that a deuteron can only be made if the neutron
and proton have their spins parallel; when their spins are opposite, the attraction
is not quite strong enough to bind them together. Since the spins of the neutron
and proton are each one-half and are in the same direction, the deuteron has a
spin of one. We know, however, that two protons are not allowed to sit on top of
each other if their spins are parallel. If it were not for the exclusion principle, two
protons would be bound, but since they cannot exist at the same place and with
the same spin directions, the He2 nucleus does not exist. The protons could come
together with their spins opposite, but then there is not enough binding to make
a stable nucleus, because the nuclear force for opposite spins is too weak to bind
a pair of nucleons. The attractive force between neutrons and protons of opposite
spins can be seen by scattering experiments. Similar scattering experiments with
two protons with parallel spins show that there is the corresponding attraction.
So it is the exclusion principle that helps explain why deuterium can exist when
He2 cannot.
4-29","379"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","117","5
Spin One
Review: Chapter 35, Vol. II, Paramagnetism and Magnetic Resonance. For
your convenience this chapter is reproduced in the Appendix of this
volume.
5-1 Filtering atoms with a Stern-Gerlach apparatus
In this chapter we really begin the quantum mechanics proper—in the sense
that we are going to describe a quantum mechanical phenomenon in a completely
quantum mechanical way. We will make no apologies and no attempt to ﬁnd
connections to classical mechanics.
We want to talk about something new
in a new language. The particular situation which we are going to describe
is the behavior of the so-called quantization of the angular momentum, for a
particle of spin one. But we won’t use words like “angular momentum” or other
concepts of classical mechanics until later. We have chosen this particular example
because it is relatively simple, although not the simplest possible example. It is
suﬃciently complicated that it can stand as a prototype which can be generalized
for the description of all quantum mechanical phenomena.
Thus, although
we are dealing with a particular example, all the laws which we mention are
immediately generalizable, and we will give the generalizations so that you will
see the general characteristics of a quantum mechanical description. We begin
with the phenomenon of the splitting of a beam of atoms into three separate
beams in a Stern-Gerlach experiment.
You remember that if we have an inhomogeneous magnetic ﬁeld made by a
magnet with a pointed pole tip and we send a beam through the apparatus, the
beam of particles may be split into a number of beams—the number depending
on the particular kind of atom and its state. We are going to take the case of an
atom which gives three beams, and we are going to call that a particle of spin
one. You can do for yourself the case of ﬁve beams, seven beams, two beams,
5-1","316"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","118","y
z
∇B
ATOMIC
BEAM
Fig. 5-1. In a Stern-Gerlach experiment, atoms of spin one are split
into three beams.
etc.—you just copy everything down and where we have three terms, you will
have ﬁve terms, seven terms, and so on.
Imagine the apparatus drawn schematically in Fig. 5-1. A beam of atoms (or
particles of any kind) is collimated by some slits and passes through a nonuniform
ﬁeld. Let’s say that the beam moves in the y-direction and that the magnetic
ﬁeld and its gradient are both in the z-direction. Then, looking from the side, we
will see the beam split vertically into three beams, as shown in the ﬁgure. Now
at the output end of the magnet we could put small counters which count the
rate of arrival of particles in any one of the three beams. Or we can block oﬀ
two of the beams and let the third one go on.
Suppose we block oﬀthe lower two beams and let the top-most beam go
on and enter a second Stern-Gerlach apparatus of the same kind, as shown in
Fig. 5-2. What happens? There are not three beams in the second apparatus;
there is only the top beam.† This is what you would expect if you think of the
∇B
∇B
Fig. 5-2. The atoms from one of the beams are sent into a second
identical apparatus.
† We are assuming that the deﬂection angles are very small.
5-2","242"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","119","second apparatus as simply an extension of the ﬁrst. Those atoms which are
being pushed upward continue to be pushed upward in the second magnet.
You can see then that the ﬁrst apparatus has produced a beam of “puriﬁed”
objects—atoms that get bent upward in the particular inhomogeneous ﬁeld. The
atoms, as they enter the original Stern-Gerlach apparatus, are of three “varieties,”
and the three kinds take diﬀerent trajectories. By ﬁltering out all but one of
the varieties, we can make a beam whose future behavior in the same kind of
apparatus is determined and predictable. We will call this a ﬁltered beam, or a
polarized beam, or a beam in which the atoms all are known to be in a deﬁnite
state.
For the rest of our discussion, it will be more convenient if we consider a
somewhat modiﬁed apparatus of the Stern-Gerlach type. The apparatus looks
more complicated at ﬁrst, but it will make all the arguments simpler. Anyway,
since they are only “thought experiments,” it doesn’t cost anything to complicate
the equipment. (Incidentally, no one has ever done all of the experiments we
will describe in just this way, but we know what would happen from the laws of
quantum mechanics, which are, of course, based on other similar experiments.
These other experiments are harder to understand at the beginning, so we want
to describe some idealized—but possible—experiments.)
Figure 5-3(a) shows a drawing of the “modiﬁed Stern-Gerlach apparatus”
we would like to use. It consists of a sequence of three high-gradient magnets.
The ﬁrst one (on the left) is just the usual Stern-Gerlach magnet and splits
the incoming beam of spin-one particles into three separate beams. The second
magnet has the same cross section as the ﬁrst, but is twice as long and the
polarity of its magnetic ﬁeld is opposite the ﬁeld in magnet 1. The second magnet
pushes in the opposite direction on the atomic magnets and bends their paths
back toward the axis, as shown in the trajectories drawn in the lower part of the
ﬁgure. The third magnet is just like the ﬁrst, and brings the three beams back
together again, so that leaves the exit hole along the axis. Finally, we would
like to imagine that in front of the hole at A there is some mechanism which
can get the atoms started from rest and that after the exit hole at B there is a
decelerating mechanism that brings the atoms back to rest at B. That is not
essential, but it will mean that in our analysis we won’t have to worry about
including any eﬀects of the motion as the atoms come out, and can concentrate
on those matters having only to do with the spin. The whole purpose of the
“improved” apparatus is just to bring all the particles to the same place, and
with zero speed.
5-3","480"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","120","(a)
y
z
A
S
N
N
S
S
N
(b)
y
z
A
+
0
−
Fig. 5-3. (a) An imagined modiﬁcation of a Stern-Gerlach apparatus. (b) The paths of
spin-one atoms.
5-4","34"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","121","Now if we want to do an experiment like the one in Fig. 5-2, we can ﬁrst make
a ﬁltered beam by putting a plate in the middle of the apparatus that blocks two
of the beams, as shown in Fig. 5-4. If we now put the polarized atoms through
a second identical apparatus, all the atoms will take the upper path, as can be
veriﬁed by putting similar plates in the way of the various beams of the second
S ﬁlter and seeing whether particles get through.
y
z
S
+
0
−
S
+
0
−
Fig. 5-4. The “improved” Stern-Gerlach apparatus as a ﬁlter.
Suppose we call the ﬁrst apparatus by the name S. (We are going to consider
all sorts of combinations, and we will need labels to keep things straight.) We
will say that the atoms which take the top path in S are in the “plus state
with respect to S”; the ones which take the middle path are in the “zero state
with respect to S”; and the ones which take the lowest path are in the “minus
state with respect to S.” (In the more usual language we would say that the
z-component of the angular momentum was +1ℏ, 0, and −1ℏ, but we are not
using that language now.) Now in Fig. 5-4 the second apparatus is oriented just
like the ﬁrst, so the ﬁltered atoms will all go on the upper path. Or if we had
blocked oﬀthe upper and lower beams in the ﬁrst apparatus and let only the
zero state through, all the ﬁltered atoms would go through the middle path of
the second apparatus. And if we had blocked oﬀall but the lowest beam in the
ﬁrst, there would be only a low beam in the second. We can say that in each
case our ﬁrst apparatus has produced a ﬁltered beam in a pure state with respect
to S (+, 0, or −), and we can test which state is present by putting the atoms
through a second, identical apparatus.
We can make our second apparatus so that it transmits only atoms of a
particular state—by putting masks inside it as we did for the ﬁrst one—and
then we can test the state of the incoming beam just by seeing whether anything
5-5","387"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","122","comes out the far end. For instance, if we block oﬀthe two lower paths in the
second apparatus, 100 percent of the atoms will still come through; but if we
block oﬀthe upper path, nothing will get through.
To make this kind of discussion easier, we are going to invent a shorthand
symbol to represent one of our improved Stern-Gerlach apparatuses. We will let
the symbol



+



0
−
S
(5.1)
stand for one complete apparatus. (This is not a symbol you will ever ﬁnd used
in quantum mechanics; we’ve just invented it for this chapter. It is simply meant
to be a shorthand picture of the apparatus of Fig. 5-3.) Since we are going to
want to use several apparatuses at once, and with various orientations, we will
identify each with a letter underneath. So the symbol in (5.1) stands for the
apparatus S. When we block oﬀone or more of the beams inside, we will show
that by some vertical bars indicating which beam is blocked, like this:



+


.
0
−
S
(5.2)
The various possible combinations we will be using are shown in Fig. 5-5.
If we have two ﬁlters in succession (as in Fig. 5-4), we will put the two symbols
next to each other, like this:



+



0
−
S



+


.
0
−
S
(5.3)
For this setup, everything that comes through the ﬁrst also gets through the
second. In fact, even if we block oﬀthe “zero” and “minus” channels of the
second apparatus, so that we have



+



0
−
S



+


,
0
−
S
(5.4)
5-6","299"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","123","(a)



+



0
−
=
+
0
−
(b)



+ 


0
−
=
(c)



+ 


0
−
=
(d)



+ 


0
−
=
Fig. 5-5. Special shorthand symbols for Stern-Gerlach type ﬁlters.
we still get 100 percent transmission through the second apparatus. On the other
hand, if we have



+



0
−
S



+


,
0
−
S
(5.5)
nothing at all comes out of the far end. Similarly,



+



0
−
S



+



0
−
S
(5.6)
5-7","126"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","124","would give nothing out. On the other hand,



+



0
−
S



+



0
−
S
(5.7)
would be just equivalent to



+



0
−
S
by itself.
Now we want to describe these experiments quantum mechanically. We will
say that an atom is in the (+S) state if it has gone through the apparatus
of Fig. 5-5(b), that it is in a (0S) state if it has gone through (c), and in a
(−S) state if it has gone through (d).† Then we let ⟨b | a⟩be the amplitude that
an atom which is in state a will get through an apparatus into the b state. We
can say: ⟨b | a⟩is the amplitude for an atom in the state a to get into the state b.
The experiment (5.4) gives us that
⟨+S | +S⟩= 1,
whereas (5.5) gives us
⟨−S | +S⟩= 0.
Similarly, the result of (5.6) is
⟨+S | −S⟩= 0,
and of (5.7) is
⟨−S | −S⟩= 1.
As long as we deal only with “pure” states—that is, we have only one channel
open—there are nine such amplitudes, and we can write them in a table:
from
+S
0S
−S
to
+S
1
0
0
0S
0
1
0
−S
0
0
1
(5.8)
† Read: (+S) = “plus-S”; ( 0 S) = “zero-S”; (−S) = “minus-S.”
5-8","242"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","125","This array of nine numbers—called a matrix—summarizes the phenomena we’ve
been describing.
5-2 Experiments with ﬁltered atoms
Now comes the big question: What happens if the second apparatus is tipped
to a diﬀerent angle, so that its ﬁeld axis is no longer parallel to the ﬁrst? It could
be not only tipped, but also pointed in a diﬀerent direction—for instance, it could
take the beam oﬀat 90◦with respect to the original direction. To take it easy at
ﬁrst, let’s ﬁrst think about an arrangement in which the second Stern-Gerlach
experiment is tilted by some angle α about the y-axis, as shown in Fig. 5-6.
We’ll call the second apparatus T. Suppose that we now set up the following
experiment:



+



0
−
S



+


,
0
−
T
or the experiment:



+



0
−
S



+


.
0
−
T
What comes out at the far end in these cases?
The answer is this: If the atoms are in a deﬁnite state with respect to S, they
are not in the same state with respect to T—a (+S) state is not also a (+T) state.
y
z
S
T
α
Fig. 5-6. Two Stern-Gerlach type ﬁlters in series; the second is tilted
at the angle α with respect to the ﬁrst.
5-9","234"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","126","There is, however, a certain amplitude to ﬁnd the atom in a (+T) state—or a
(0T) state or a (−T) state.
In other words, as careful as we have been to make sure that we have the atoms
in a deﬁnite condition, the fact of the matter is that if it goes through an apparatus
which is tilted at a diﬀerent angle it has, so to speak, to “reorient” itself—which it
does, don’t forget, by luck. We can put only one particle through at a time, and
then we can only ask the question: What is the probability that it gets through?
Some of the atoms that have gone through S will end in a (+T) state, some
of them will end in a (0T), and some in a (−T) state—all with diﬀerent odds.
These odds can be calculated by the absolute squares of complex amplitudes;
what we want is some mathematical method, or quantum mechanical description,
for these amplitudes. What we need to know are various quantities like
⟨−T | +S⟩,
by which we mean the amplitude that an atom initially in the (+S) state can get
into the (−T) condition (which is not zero unless T and S are lined up parallel
to each other). There are other amplitudes like
⟨+T | 0S⟩,
or
⟨0T | −S⟩,
etc.
There are, in fact, nine such amplitudes—another matrix—that a theory of
particles should tell us how to calculate. Just as F = ma tells us how to calculate
what happens to a classical particle in any circumstance, the laws of quantum
mechanics permit us to determine the amplitude that a particle will get through a
particular apparatus. The central problem, then, is to be able to calculate—for any
given tilt angle α, or in fact for any orientation whatever—the nine amplitudes:
⟨+T | +S⟩,
⟨+T | 0S⟩,
⟨+T | −S⟩,
⟨0T | +S⟩,
⟨0T | 0S⟩,
⟨0T | −S⟩,
⟨−T | +S⟩,
⟨−T | 0S⟩,
⟨−T | −S⟩.
(5.9)
We can already ﬁgure out some relations among these amplitudes. First,
according to our deﬁnitions, the absolute square
|⟨+T | +S⟩|2
is the probability that an atom in a (+S) state will enter a (+T) state. We will
often ﬁnd it more convenient to write such squares in the equivalent form
⟨+T | +S⟩⟨+T | +S⟩∗.
5-10","386"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","127","In the same notation the number
⟨0T | +S⟩⟨0T | +S⟩∗
is the probability that a particle in the (+S) state will enter the (0T) state, and
⟨−T | +S⟩⟨−T | +S⟩∗
is the probability that it will enter the (−T) state. But the way our apparatuses
are made, every atom which enters the T apparatus must be found in some one
of the three states of the T apparatus—there’s nowhere else for a given kind of
atom to go. So the sum of the three probabilities we’ve just written must be
equal to 100 percent. We have the relation
⟨+T | +S⟩⟨+T | +S⟩∗+ ⟨0T | +S⟩⟨0T | +S⟩∗
+ ⟨−T | +S⟩⟨−T | +S⟩∗= 1.
(5.10)
There are, of course, two other such equations that we get if we start with a (0S)
or a (−S) state. But they are all we can easily get, so we’ll go on to some other
general questions.
5-3 Stern-Gerlach ﬁlters in series
Here is an interesting question: Suppose we had atoms ﬁltered into the
(+S) state, then we put them through a second ﬁlter, say into a (0T) state,
and then through another +S ﬁlter. (We’ll call the last ﬁlter S′ just so we can
distinguish it from the ﬁrst S-ﬁlter.) Do the atoms remember that they were
once in a (+S) state? In other words, we have the following experiment:



+



0
−
S



+



0
−
T



+


.
0
−
S′
(5.11)
We want to know whether all those that get through T also get through S′. They
do not. Once they have been ﬁltered by T, they do not remember in any way
that they were in a (+S) state when they entered T. Note that the second
S apparatus in (5.11) is oriented exactly the same as the ﬁrst, so it is still an
S-type ﬁlter. The states ﬁltered by S′ are, of course, still (+S), (0S), and (−S).
5-11","341"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","128","The important point is this: If the T ﬁlter passes only one beam, the fraction
that gets through the second S ﬁlter depends only on the setup of the T ﬁlter,
and is completely independent of what precedes it. The fact that the same atoms
were once sorted by an S ﬁlter has no inﬂuence whatever on what they will do
once they have been sorted again into a pure beam by a T apparatus. From then
on, the probability for getting into diﬀerent states is the same no matter what
happened before they got into the T apparatus.
As an example, let’s compare the experiment of (5.11) with the following
experiment:



+



0
−
S



+



0
−
T



+



0
−
S′
(5.12)
in which only the ﬁrst S is changed. Let’s say that the angle α (between S and T)
is such that in experiment (5.11) one-third of the atoms that get through T also
get through S′. In experiment (5.12), although there will, in general, be a diﬀerent
number of atoms coming through T, the same fraction of these—one-third—will
also get through S′.
We can, in fact, show from what you have learned earlier that the fraction of
the atoms that come out of T and get through any particular S′ depends only on
T and S′, not on anything that happened earlier. Let’s compare experiment (5.12)
with



+



0
−
S



+



0
−
T



+


.
0
−
S′
(5.13)
The amplitude that an atom that comes out of S will also get through both T
and S′ is, for the experiments of (5.12),
⟨+S | 0T⟩⟨0T | 0S⟩.
The corresponding probability is
|⟨+S | 0T⟩⟨0T | 0S⟩|2 = |⟨+S | 0T⟩|2|⟨0T | 0S⟩|2.
The probability for experiment (5.13) is
|⟨0S | 0T⟩⟨0T | 0S⟩|2 = |⟨0S | 0T⟩|2|⟨0T | 0S⟩|2.
5-12","343"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","129","The ratio is
|⟨0S | 0T⟩|2
|⟨+S | 0T⟩|2 .
and depends only on T and S′, and not at all on which beam (+S), (0S), or (−S)
is selected by S. (The absolute numbers may go up and down together depending
on how much gets through T.) We would, of course, ﬁnd the same result if we
compared the probabilities that the atoms would go into the plus or the minus
states with respect to S′, or the ratio of the probabilities to go into the zero or
minus states.
In fact, since these ratios depend only on which beam is allowed to pass
through T, and not on the selection made by the ﬁrst S ﬁlter, it is clear that
we would get the same result even if the last apparatus were not an S ﬁl-
ter. If we use for the third apparatus—which we will now call R—one rotated
by some arbitrary angle with respect to T, we would ﬁnd that a ratio such
as |⟨0R | 0T⟩|2/|⟨+R | 0T⟩|2 was independent of which beam was passed by the
ﬁrst ﬁlter S.
5-4 Base states
These results illustrate one of the basic principles of quantum mechanics:
Any atomic system can be separated by a ﬁltering process into a certain set of
what we will call base states, and the future behavior of the atoms in any single
given base state depends only on the nature of the base state—it is independent
of any previous history.† The base states depend, of course, on the ﬁlter used;
for instance, the three states (+T), (0T), and (−T) are one set of base states;
the three states (+S), (0S), and (−S) are another. There are any number of
possibilities each as good as any other.
We should be careful to say that we are considering good ﬁlters which do
indeed produce “pure” beams. If, for instance, our Stern-Gerlach apparatus didn’t
produce a good separation of the three beams so that we could not separate
them cleanly by our masks, then we could not make a complete separation into
base states. We can tell if we have pure base states by seeing whether or not the
† We do not intend the word “base state” to imply anything more than what is said here.
They are not to be thought of as “basic” in any sense. We are using the word base with the
thought of a basis for a description, somewhat in the sense that one speaks of “numbers to the
base ten.”
5-13","423"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","130","beams can be split again in another ﬁlter of the same kind. If we have a pure
(+T) state, for instance, all the atoms will go through



+


,
0
−
T
and none will go through



+


,
0
−
T
or through



+


.
0
−
T
Our statement about base states means that it is possible to ﬁlter to some pure
state, so that no further ﬁltering by an identical apparatus is possible.
We must also point out that what we are saying is exactly true only in
rather idealized situations.
In any real Stern-Gerlach apparatus, we would
have to worry about diﬀraction by the slits that could cause some atoms to
go into states corresponding to diﬀerent angles, or about whether the beams
might contain atoms with diﬀerent excitations of their internal states, and so
on.
We have idealized the situation so that we are talking only about the
states that are split in a magnetic ﬁeld; we are ignoring things having to do
with position, momentum, internal excitations, and the like. In general, one
would need to consider also base states which are sorted out with respect to
such things also. But to keep the concepts simple, we are considering only
our set of three states, which is suﬃcient for the exact treatment of the ide-
alized situation in which the atoms don’t get torn up in going through the
apparatus, or otherwise badly treated, and come to rest when they leave the
apparatus.
You will note that we always begin our thought experiments by taking a
ﬁlter with only one channel open, so that we start with some deﬁnite base state.
We do this because atoms come out of a furnace in various states determined
5-14","307"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","131","at random by the accidental happenings inside the furnace.
(It gives what
is called an “unpolarized” beam.) This randomness involves probabilities of
the “classical” kind—as in coin tossing—which are diﬀerent from the quantum
mechanical probabilities we are worrying about now. Dealing with an unpolarized
beam would get us into additional complications that are better to avoid until
after we understand the behavior of polarized beams. So don’t try to consider
at this point what happens if the ﬁrst apparatus lets more than one beam
through. (We will tell you how you can handle such cases at the end of the
chapter.)
Let’s now go back and see what happens when we go from a base state for
one ﬁlter to a base state for a diﬀerent ﬁlter. Suppose we start again with



+



0
−
S



+


.
0
−
T
The atoms which come out of T are in the base state (0T) and have no memory
that they were once in the state (+S). Some people would say that in the ﬁltering
by T we have “lost the information” about the previous state (+S) because we
have “disturbed” the atoms when we separated them into three beams in the
apparatus T. But that is not true. The past information is not lost by the
separation into three beams, but by the blocking masks that are put in—as we
can see by the following set of experiments.
We start with a +S ﬁlter and will call N the number of atoms that come
through it. If we follow this by a 0T ﬁlter, the number of atoms that come out is
some fraction of the original number, say αN. If we then put another +S ﬁlter,
only some fraction β of these atoms will get to the far end. We can indicate this
in the following way:



+



N
−→
0
−
S



+



αN
−→
0
−
T



+



βαN
−→.
0
−
S′
(5.14)
If our third apparatus S′ selected a diﬀerent state, say the (0S) state, a diﬀerent
5-15","372"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","132","fraction, say γ, would get through.† We would have



+



N
−→
0
−
S



+



αN
−→
0
−
T



+



γαN
−→.
0
−
S′
(5.15)
Now suppose we repeat these two experiments but remove all the masks from T.
We would then ﬁnd the remarkable results as follows:



+



N
−→
0
−
S



+



N
−→
0
−
T



+



N
−→,
0
−
S′
(5.16)



+



N
−→
0
−
S



+



N
−→
0
−
T



+



0
−→.
0
−
S′
(5.17)
All the atoms get through S′ in the ﬁrst case, but none in the second case! This
is one of the great laws of quantum mechanics. That nature works this way is not
self-evident, but the results we have given correspond for our idealized situation
to the quantum mechanical behavior observed in innumerable experiments.
5-5 Interfering amplitudes
How can it be that in going from (5.15) to (5.17)—by opening more channels—
we let fewer atoms through? This is the old, deep mystery of quantum mechanics—
the interference of amplitudes. It’s the same kind of thing we ﬁrst saw in the
two-slit interference experiment with electrons. We saw that we could get fewer
electrons at some places with both slits open than we got with one slit open. It
works quantitatively this way. We can write the amplitude that an atom will get
through T and S′ in the apparatus of (5.17) as the sum of three amplitudes, one
for each of the three beams in T; the sum is equal to zero:
⟨0S | +T⟩⟨+T | +S⟩+ ⟨0S | 0T⟩⟨0T | +S⟩+ ⟨0S | −T⟩⟨−T | +S⟩= 0. (5.18)
† In terms of our earlier notation α = |⟨0 T | +S⟩|2, β = |⟨+S | 0 T⟩|2, and γ = |⟨0 S | 0 T⟩|2.
5-16","360"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","133","None of the three individual amplitudes is zero—for example, the absolute square
of the second amplitude is γα, see (5.15)—but the sum is zero. We would have
also the same answer if S′ were set to select the (−S) state. However, in the
setup of (5.16), the answer is diﬀerent. If we call a the amplitude to get through
T and S′, in this case we have†
a = ⟨+S | +T⟩⟨+T | +S⟩+ ⟨+S | 0T⟩⟨0T | +S⟩
+ ⟨+S | −T⟩⟨−T | +S⟩= 1.
(5.19)
In the experiment (5.16) the beam has been split and recombined. Humpty
Dumpty has been put back together again. The information about the original
(+S) state is retained—it is just as though the T apparatus were not there at all.
This is true whatever is put after the “wide-open” T apparatus. We could follow
it with an R ﬁlter—a ﬁlter at some odd angle—or anything we want. The answer
will always be the same as if the atoms were taken directly from the ﬁrst S ﬁlter.
So this is the important principle: A T ﬁlter—or any ﬁlter—with wide-open
masks produces no change at all. We should make one additional condition. The
wide-open ﬁlter must not only transmit all three beams, but it must also not
produce unequal disturbances on the three beams. For instance, it should not
have a strong electric ﬁeld near one beam and not the others. The reason is that
even if this extra disturbance would still let all the atoms through the ﬁlter, it
could change the phases of some of the amplitudes. Then the interference would
be changed, and the amplitudes in Eqs. (5.18) and (5.19) would be diﬀerent. We
will always assume that there are no such extra disturbances.
Let’s rewrite Eqs. (5.18) and (5.19) in an improved notation. We will let i
stand for any one of the three states (+T), (0T), or (−T); then the equations
can be written:
X
all i
⟨0S | i⟩⟨i | +S⟩= 0
(5.20)
and
X
all i
⟨+S | i⟩⟨i | +S⟩= 1.
(5.21)
Similarly, for an experiment where S′ is replaced by a completely arbitrary
† We really cannot conclude from the experiment that a = 1, but only that |a|2 = 1, so a
might be eiδ, but it can be shown that the choice δ = 0 represents no real loss of generality.
5-17","398"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","134","ﬁlter R, we have



+



0
−
S



+



0
−
T



+


.
0
−
R
(5.22)
The results will always be the same as if the T apparatus were left out and we
had only



+



0
−
S



+


.
0
−
R
Or, expressed mathematically,
X
all i
⟨+R | i⟩⟨i | +S⟩= ⟨+R | +S⟩.
(5.23)
This is our fundamental law, and it is generally true so long as i stands for the
three base states of any ﬁlter.
You will notice that in the experiment (5.22) there is no special relation of
S and R to T. Furthermore, the arguments would be the same no matter what
states they selected. To write the equation in a general way, without having
to refer to the speciﬁc states selected by S and R, let’s call φ (“phi”) the state
prepared by the ﬁrst ﬁlter (in our special example, +S) and χ (“khi”) the state
tested by the ﬁnal ﬁlter (in our example, +R). Then we can state our fundamental
law of Eq. (5.23) in the form
⟨χ | φ⟩=
X
all i
⟨χ | i⟩⟨i | φ⟩,
(5.24)
where i is to range over the three base states of some particular ﬁlter.
We want to emphasize again what we mean by base states. They are like
the three states which can be selected by one of our Stern-Gerlach apparatuses.
One condition is that if you have a base state, then the future is independent
of the past. Another condition is that if you have a complete set of base states,
Eq. (5.24) is true for any set of beginning and ending states φ and χ. There is,
however, no unique set of base states. We began by considering base states with
respect to a particular apparatus T. We could equally well consider a diﬀerent
5-18","337"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","135","set of base states with respect to an apparatus S, or with respect to R, etc.† We
usually speak of the base states “in a certain representation.”
Another condition on a set of base states in any particular representation
is that they are all completely diﬀerent. By that we mean that if we have a
(+T) state, there is no amplitude for it to go into a (0T) or a (−T) state. If we
let i and j stand for any two base states of a particular set, the general rules
discussed in connection with (5.8) are that
⟨j | i⟩= 0
for all i and j that are not equal. Of course, we know that
⟨i | i⟩= 1.
These two equations are usually written as
⟨j | i⟩= δji,
(5.25)
where δji (the “Kronecker delta”) is a symbol that is deﬁned to be zero for i ̸= j,
and to be one for i = j.
Equation (5.25) is not independent of the other laws we have mentioned. It
happens that we are not particularly interested in the mathematical problem
of ﬁnding the minimum set of independent axioms that will give all the laws
as consequences.‡ We are satisﬁed if we have a set that is complete and not
apparently inconsistent. We can, however, show that Eqs. (5.25) and (5.24) are
not independent. Suppose we let φ in Eq. (5.24) represent one of the base states
of the same set as i, say the jth state; then we have
⟨χ | j⟩=
X
i
⟨χ | i⟩⟨i | j⟩.
But Eq. (5.25) says that ⟨i | j⟩is zero unless i = j, so the sum becomes just ⟨χ | j⟩
and we have an identity, which shows that the two laws are not independent.
We can see that there must be another relation among the amplitudes if both
Eqs. (5.10) and (5.24) are true. Equation (5.10) is
⟨+T | +S⟩⟨+T | +S⟩∗+ ⟨0T | +S⟩⟨0T | +S⟩∗+ ⟨−T | +S⟩⟨−T | +S⟩∗= 1.
† In fact, for atomic systems with three or more base states, there exist other kinds of
ﬁlters—quite diﬀerent from a Stern-Gerlach apparatus—which can be used to get more choices
for the set of base states (each set with the same number of states).
‡ Redundant truth doesn’t bother us!
5-19","386"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","136","If we write Eq. (5.24), letting both φ and χ be the state (+S), the left-hand side
is ⟨+S | +S⟩, which is clearly = 1; so we get once more Eq. (5.19),
⟨+S | +T⟩⟨+T | +S⟩+ ⟨+S | 0T⟩⟨0T | +S⟩+ ⟨+S | −T⟩⟨−T | +S⟩= 1.
These two equations are consistent (for all relative orientations of the T and S
apparatuses) only if
⟨+S | +T⟩= ⟨+T | +S⟩∗,
⟨+S | 0T⟩= ⟨0T | +S⟩∗,
⟨+S | −T⟩= ⟨−T | +S⟩∗.
And it follows that for any states φ and χ,
⟨φ | χ⟩= ⟨χ | φ⟩∗.
(5.26)
If this were not true, probability wouldn’t be “conserved,” and particles would
get “lost.”
Before going on, we want to summarize the three important general laws
about amplitudes. They are Eqs. (5.24), (5.25), and (5.26):
I
⟨j | i⟩= δji,
II
⟨χ | φ⟩=
X
all i
⟨χ | i⟩⟨i | φ⟩,
III
⟨φ | χ⟩= ⟨χ | φ⟩∗.
(5.27)
In these equations the i and j refer to all the base states of some one representa-
tion, while φ and χ represent any possible states of the atom. It is important
to note that II is valid only if the sum is carried out over all the base states of
the system (in our case, three: +T, 0T, −T). These laws say nothing about
what we should choose for a base for our set of base states. We began by using
a T apparatus, which is a Stern-Gerlach experiment with some arbitrary ori-
entation; but any other orientation, say W, would be just as good. We would
have a diﬀerent set of states to use for i and j, but all the laws would still be
good—there is no unique set. One of the great games of quantum mechanics is
to make use of the fact that things can be calculated in more than one way.
5-20","319"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","137","5-6 The machinery of quantum mechanics
We want to show you why these laws are useful. Suppose we have an atom in
a given condition (by which we mean that it was prepared in a certain way), and
we want to know what will happen to it in some experiment. In other words, we
start with our atom in the state φ and want to know what are the odds that it
will go through some apparatus which accepts atoms only in the condition χ. The
laws say that we can describe the apparatus completely in terms of three complex
numbers ⟨χ | i⟩, the amplitudes for each base state to be in the condition χ; and
that we can tell what will happen if an atom is put into the apparatus if we
describe the state of the atom by giving three numbers ⟨i | φ⟩, the amplitudes for
the atom in its original condition to be found in each of the three base states.
This is an important idea.
Let’s consider another illustration. Think of the following problem: We start
with an S apparatus; then we have a complicated mess of junk, which we can
call A, and then an R apparatus—like this:



+



0
−
S






A



+


.
0
−
R
(5.28)
By A we mean any complicated arrangement of Stern-Gerlach apparatuses with
masks or half-masks, oriented at peculiar angles, with odd electric and magnetic
ﬁelds . . . almost anything you want to put. (It’s nice to do thought experiments—
you don’t have to go to all the trouble of actually building the apparatus!) The
problem then is: With what amplitude does a particle that enters the section A
in a (+S) state come out of A in the (0R) state, so that it will get through the
last R ﬁlter? There is a regular notation for such an amplitude; it is
⟨0R | A | +S⟩.
As usual, it is to be read from right to left (like Hebrew):
⟨ﬁnish | through | start⟩.
If by chance A doesn’t do anything—but is just an open channel—then we write
⟨0R | 1 | +S⟩= ⟨0R | +S⟩;
(5.29)
5-21","381"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","138","the two symbols are equivalent. For a more general problem, we might replace
(+S) by a general starting state φ and (0R) by a general ﬁnishing state χ, and
we would want to know the amplitude
⟨χ | A | φ⟩.
A complete analysis of the apparatus A would have to give the amplitude ⟨χ | A | φ⟩
for every possible pair of states φ and χ—an inﬁnite number of combinations!
How then can we give a concise description of the behavior of the apparatus A?
We can do it in the following way. Imagine that the apparatus of (5.28) is
modiﬁed to be



+



0
−
S



+



0
−
T






A



+



0
−
T



+


.
0
−
R
(5.30)
This is really no modiﬁcation at all since the wide-open T apparatuses don’t
do anything. But they do suggest how we can analyze the problem. There is a
certain set of amplitudes ⟨i | +S⟩that the atoms from S will get into the i state
of T. Then there is another set of amplitudes that an i state (with respect to T)
entering A will come out as a j state (with respect to T). And ﬁnally there is an
amplitude that each j state will get through the last ﬁlter as a (0R) state. For
each possible alternative path, there is an amplitude of the form
⟨0R | j⟩⟨j | A | i⟩⟨i | +S⟩,
and the total amplitude is the sum of the terms we can get with all possible
combinations of i and j. The amplitude we want is
X
ij
⟨0R | j⟩⟨j | A | i⟩⟨i | +S⟩.
(5.31)
If (0R) and (+S) are replaced by general states χ and φ, we would have the
same kind of expression; so we have the general result
⟨χ | A | φ⟩=
X
ij
⟨χ | j⟩⟨j | A | i⟩⟨i | φ⟩.
(5.32)
Now notice that the right-hand side of Eq. (5.32) is really “simpler” than
the left-hand side. The apparatus A is completely described by the nine num-
bers ⟨j | A | i⟩which tell the response of A with respect to the three base states
5-22","395"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","139","of the apparatus T. Once we know these nine numbers, we can handle any two
incoming and outgoing states φ and χ if we deﬁne each in terms of the three
amplitudes for going into, or from, each of the three base states. The result of
an experiment is predicted using Eq. (5.32).
This then is the machinery of quantum mechanics for a spin-one particle.
Every state is described by three numbers which are the amplitudes to be in
each of some selected set of base states. Every apparatus is described by nine
numbers which are the amplitudes to go from one base state to another in the
apparatus. From these numbers anything can be calculated.
The nine amplitudes which describe the apparatus are often written as a
square matrix—called the matrix ⟨j | A | i⟩:
from
+
0
−
to
+
⟨+ | A | +⟩
⟨+ | A | 0⟩
⟨+ | A | −⟩
0
⟨0 | A | +⟩
⟨0 | A | 0⟩
⟨0 | A | −⟩
−
⟨−| A | +⟩
⟨−| A | 0⟩
⟨−| A | −⟩
(5.33)
The mathematics of quantum mechanics is just an extension of this idea. We
will give you a simple illustration. Suppose we have an apparatus C that we wish
to analyze—that is, we want to calculate the various ⟨j | C | i⟩. For instance, we
might want to know what happens in an experiment like



+



0
−
S






C



+


.
0
−
R
(5.34)
But then we notice that C is just built of two pieces of apparatus A and B
in series—the particles go through A and then through B—so we can write
symbolically






C
=






A
·





.
B
(5.35)
We can call the C apparatus the “product” of A and B. Suppose also that we
5-23","343"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","140","already know how to analyze the two parts; so we can get the matrices (with
respect to T) of A and B. Our problem is then solved. We can easily ﬁnd
⟨χ | C | φ⟩
for any input and output states. First we write that
⟨χ | C | φ⟩=
X
k
⟨χ | B | k⟩⟨k | A | φ⟩.
Do you see why? (Hint: Imagine putting a T apparatus between A and B.) Then
if we consider the special case in which φ and χ are also base states (of T), say i
and j, we have
⟨j | C | i⟩=
X
k
⟨j | B | k⟩⟨k | A | i⟩.
(5.36)
This equation gives the matrix for the “product” apparatus C in terms of
the two matrices of the apparatuses A and B. Mathematicians call the new
matrix ⟨j | C | i⟩—formed from two matrices ⟨j | B | i⟩and ⟨j | A | i⟩according to
the sum speciﬁed in Eq. (5.36)—the “product” matrix BA of the two matrices
B and A. (Note that the order is important, AB ̸= BA.) Thus, we can say that
the matrix for a succession of two pieces of apparatus is the matrix product of
the matrices for the two apparatuses (putting the ﬁrst apparatus on the right
in the product). Anyone who knows matrix algebra then understands that we
mean just Eq. (5.36).
5-7 Transforming to a diﬀerent base
We want to make one ﬁnal point about the base states used in the cal-
culations. Suppose we have chosen to work with some particular base—say
the S base—and another fellow decides to do the same calculations with a
diﬀerent base—say the T base.
To keep things straight let’s call our base
states the (iS) states, where i = +, 0, −.
Similarly, we can call his base
states (jT). How can we compare our work with his? The ﬁnal answers for
the result of any measurement should come out the same, but in the calcu-
lations the various amplitudes and matrices used will be diﬀerent. How are
they related? For instance, if we both start with the same φ, we will describe
it in terms of the three amplitudes ⟨iS | φ⟩that φ goes into our base states
5-24","383"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","141","in the S representation, whereas he will describe it by the amplitudes ⟨jT | φ⟩
that the state φ goes into the base states in his T representation. How can
we check that we are really both describing the same state φ? We can do it
with the general rule II in (5.27). Replacing χ by any one of his states jT, we
have
⟨jT | φ⟩=
X
i
⟨jT | iS⟩⟨iS | φ⟩.
(5.37)
To relate the two representations, we need only give the nine complex numbers of
the matrix ⟨jT | iS⟩. This matrix can then be used to convert all of our equations
to his form. It tells us how to transform from one set of base states to another.
(For this reason ⟨jT | iS⟩is sometimes called “the transformation matrix from
representation S to representation T.” Big words!)
For the case of spin-one particles for which we have only three base states (for
higher spins, there are more) the mathematical situation is analogous to what
we have seen in vector algebra. Every vector can be represented by giving three
numbers—the components along the axes x, y, and z. That is, every vector can
be resolved into three “base” vectors which are vectors along the three axes. But
suppose someone else chooses to use a diﬀerent set of axes—x′, y′, and z′. He will
be using diﬀerent numbers to represent any particular vector. His calculations
will look diﬀerent, but the ﬁnal results will be the same. We have considered
this before and know the rules for transforming vectors from one set of axes to
another.
You may want to see how the quantum mechanical transformations work
by trying some out; so we will give here, without proof, the transformation
matrices for converting the spin-one amplitudes in one representation S to an-
other representation T, for various special relative orientations of the S and
T ﬁlters. (We will show you in a later chapter how to derive these same re-
sults.)
First case: The T apparatus has the same y-axis (along which the particles
move) as the S apparatus, but is rotated about the common y-axis by the
angle α (as in Fig. 5-6). (To be speciﬁc, a set of coordinates x′, y′, z′ is ﬁxed
in the T apparatus, related to the x, y, z coordinates of the S apparatus by:
z′ = z cos α + x sin α, x′ = x cos α −z sin α, y′ = y.) Then the transformation
5-25","418"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","142","amplitudes are:
⟨+T | +S⟩= 1
2(1 + cos α),
⟨0T | +S⟩= −1
√
2 sin α,
⟨−T | +S⟩= 1
2(1 −cos α),
⟨+T | 0S⟩= + 1
√
2 sin α,
⟨0T | 0S⟩= cos α,
⟨−T | 0S⟩= −1
√
2 sin α,
⟨+T | −S⟩= 1
2(1 −cos α),
⟨0T | −S⟩= + 1
√
2 sin α,
⟨−T | −S⟩= 1
2(1 + cos α).
(5.38)
Second Case: The T apparatus has the same z-axis as S, but is rotated
around the z-axis by the angle β. (The coordinate transformation is z′ = z,
x′ = x cos β + y sin β, y′ = y cos β −x sin β.) Then the transformation amplitudes
are:
⟨+T | +S⟩= e+iβ,
⟨0T | 0S⟩= 1,
⟨−T | −S⟩= e−iβ,
all others = 0.
(5.39)
Note that any rotations of T whatever can be made up of the two rotations
described.
If a state φ is deﬁned by the three numbers
C+ = ⟨+S | φ⟩,
C0 = ⟨0S | φ⟩,
C−= ⟨−S | φ⟩,
(5.40)
and the same state is described from the point of view of T by the three numbers
C′
+ = ⟨+T | φ⟩,
C′
0 = ⟨0T | φ⟩,
C′
−= ⟨−T | φ⟩,
(5.41)
then the coeﬃcients ⟨jT | iS⟩of (5.38) or (5.39) give the transformation connect-
5-26","229"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","143","ing Ci and C′
i. In other words, the Ci are very much like the components of a
vector that appear diﬀerent from the point of view of S and T.
For a spin-one particle only—because it requires three amplitudes—the corre-
spondence with a vector is very close. In each case, there are three numbers that
must transform with coordinate changes in a certain deﬁnite way. In fact, there
is a set of base states which transform just like the three components of a vector.
The three combinations
Cx = −1
√
2 (C+ −C−),
Cy = −i
√
2 (C+ + C−),
Cz = C0
(5.42)
transform to C′
x, C′
y, and C′
z just the way that x, y, z transform to x′, y′, z′.
[You can check that this is so by using the transformation laws (5.38) and (5.39).]
Now you see why a spin-one particle is often called a “vector particle.”
5-8 Other situations
We began by pointing out that our discussion of spin-one particles would be a
prototype for any quantum mechanical problem. The generalization has only to
do with the numbers of states. Instead of only three base states, any particular
situation may involve n base states.† Our basic laws in Eq. (5.27) have exactly
the same form—with the understanding that i and j must range over all n base
states. Any phenomenon can be analyzed by giving the amplitudes that it starts
in each one of the base states and ends in any other one of the base states, and
then summing over the complete set of base states. Any proper set of base states
can be used, and if someone wishes to use a diﬀerent set, it is just as good; the
two can be connected by using an n by n transformation matrix. We will have
more to say later about such transformations.
Finally, we promised to remark on what to do if atoms come directly from
a furnace, go through some apparatus, say A, and are then analyzed by a ﬁlter
which selects the state χ. You do not know what the state φ is that they start
out in. It is perhaps best if you don’t worry about this problem just yet, but
instead concentrate on problems that always start out with pure states. But if
you insist, here is how the problem can be handled.
First, you have to be able to make some reasonable guess about the way the
states are distributed in the atoms that come from the furnace. For example, if
† The number of base states n may be, and generally is, inﬁnite.
5-27","441"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","144","there were nothing “special” about the furnace, you might reasonably guess that
atoms would leave the furnace with random “orientations.” Quantum mechanically,
that corresponds to saying that you don’t know anything about the states, but
that one-third are in the (+S) state, one-third are in the (0S) state, and one-third
are in the (−S) state. For those that are in the (+S) state the amplitude to get
through is ⟨χ | A | +S⟩and the probability is |⟨χ | A | +S⟩|2, and similarly for the
others. The overall probability is then
1
3|⟨χ | A | +S⟩|2 + 1
3|⟨χ | A | 0S⟩|2 + 1
3|⟨χ | A | −S⟩|2.
Why did we use S rather than, say, T? The answer is, surprisingly, the same no
matter what we choose for our initial resolution—so long as we are dealing with
completely random orientations. It comes about in the same way that
X
i
|⟨χ | iS⟩|2 =
X
j
|⟨χ | jT⟩|2
for any χ. (We leave it for you to prove.)
Note that it is not correct to say that the input state has the amplitudes
p
1/3
to be in (+S),
p
1/3 to be in (0S), and
p
1/3 to be in (−S); that would imply
that certain interferences might be possible. It is simply that you do not know,
what the initial state is; you have to think in terms of the probability that the
system starts out in the various possible initial states, and then you have to take
a weighted average over the various possibilities.
5-28","264"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","145","6
Spin One-Half†
6-1 Transforming amplitudes
In the last chapter, using a system of spin one as an example, we outlined
the general principles of quantum mechanics:
Any state ψ can be described in terms of a set of base states by giving
the amplitudes to be in each of the base states.
The amplitude to go from any state to another can, in general, be written
as a sum of products, each product being the amplitude to go into one of
the base states times the amplitude to go from that base state to the ﬁnal
condition, with the sum including a term for each base state:
⟨χ | ψ⟩=
X
i
⟨χ | i⟩⟨i | ψ⟩.
(6.1)
The base states are orthogonal—the amplitude to be in one if you are in
the other is zero:
⟨i | j⟩= δij.
(6.2)
The amplitude to get from one state to another directly is the complex
conjugate of the reverse:
⟨χ | ψ⟩∗= ⟨ψ | χ⟩.
(6.3)
We also discussed a little bit about the fact that there can be more than one
base for the states and that we can use Eq. (6.1) to convert from one base to
† This chapter is a rather long and abstract side tour, and it does not introduce any idea
which we will not also come to by a diﬀerent route in later chapters. You can, therefore, skip
over it, and come back later if you are interested.
6-1","246"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","146","another. Suppose, for example, that we have the amplitudes ⟨iS | ψ⟩to ﬁnd the
state ψ in every one of the base states i of a base system S, but that we then
decide that we would prefer to describe the state in terms of another set of base
states, say the states j belonging to the base T. In the general formula, Eq. (6.1),
we could substitute jT for χ and obtain this formula:
⟨jT | ψ⟩=
X
i
⟨jT | iS⟩⟨iS | ψ⟩.
(6.4)
The amplitudes for the state (ψ) to be in the base states (jT) are related to the
amplitudes to be in the base states (iS) by the set of coeﬃcients ⟨jT | iS⟩. If
there are N base states, there are N 2 such coeﬃcients. Such a set of coeﬃcients
is often called the “transformation matrix to go from the S-representation to
the T-representation.” This looks rather formidable mathematically, but with a
little renaming we can see that it is really not so bad. If we call Ci the amplitude
that the state ψ is in the base state iS—that is, Ci = ⟨iS | ψ⟩—and call C′
j the
corresponding amplitudes for the base system T—that is, C′
j = ⟨jT | ψ⟩, then
Eq. (6.4) can be written as
C′
j =
X
i
RjiCi,
(6.5)
where Rji means the same thing as ⟨jT | iS⟩. Each amplitude C′
j is equal to a sum
over all i of one of the coeﬃcients Rji times each amplitude Ci. It has the same
form as the transformation of a vector from one coordinate system to another.
In order to avoid being too abstract for too long, we have given you some
examples of these coeﬃcients for the spin-one case, so you can see how to use
them in practice. On the other hand, there is a very beautiful thing in quantum
mechanics—that from the sheer fact that there are three states and from the
symmetry properties of space under rotations, these coeﬃcients can be found
purely by abstract reasoning. Showing you such arguments at this early stage
has a disadvantage in that you are immersed in another set of abstractions before
we get “down to earth.” However, the thing is so beautiful that we are going to
do it anyway.
We will show you in this chapter how the transformation coeﬃcients can
be derived for spin one-half particles. We pick this case, rather than spin one,
because it is somewhat easier. Our problem is to determine the coeﬃcients Rji for
a particle—an atomic system—which is split into two beams in a Stern-Gerlach
apparatus. We are going to derive all the coeﬃcients for the transformation from
6-2","453"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","147","one representation to another by pure reasoning—plus a few assumptions. Some
assumptions are always necessary in order to use “pure” reasoning! Although
the arguments will be abstract and somewhat involved, the result we get will
be relatively simple to state and easy to understand—and the result is the most
important thing. You may, if you wish, consider this as a sort of cultural excursion.
We have, in fact, arranged that all the essential results derived here are also
derived in some other way when they are needed in later chapters. So you need
have no fear of losing the thread of our study of quantum mechanics if you omit this
chapter entirely, or study it at some later time. The excursion is “cultural” in the
sense that it is intended to show that the principles of quantum mechanics are not
only interesting, but are so deep that by adding only a few extra hypotheses about
the structure of space, we can deduce a great many properties of physical systems.
Also, it is important that we know where the diﬀerent consequences of quantum
mechanics come from, because so long as our laws of physics are incomplete—as
we know they are—it is interesting to ﬁnd out whether the places where our
theories fail to agree with experiment is where our logic is the best or where our
logic is the worst. Until now, it appears that where our logic is the most abstract
it always gives correct results—it agrees with experiment. Only when we try to
make speciﬁc models of the internal machinery of the fundamental particles and
their interactions are we unable to ﬁnd a theory that agrees with experiment. The
theory then that we are about to describe agrees with experiment wherever it has
been tested—for the strange particles as well as for electrons, protons, and so on.
One remark on an annoying, but interesting, point before we proceed: It is
not possible to determine the coeﬃcients Rji uniquely, because there is always
some arbitrariness in the probability amplitudes. If you have a set of amplitudes
of any kind, say the amplitudes to arrive at some place by a whole lot of diﬀerent
routes, and if you multiply every single amplitude by the same phase factor—say
by eiδ—you have another set that is just as good. So, it is always possible to
make an arbitrary change in phase of all the amplitudes in any given problem if
you want to.
Suppose you calculate some probability by writing a sum of several amplitudes,
say (A + B + C + · · · ) and taking the absolute square. Then somebody else
calculates the same thing by using the sum of the amplitudes (A′ + B′ + C′ + · · · )
and taking the absolute square. If all the A′, B′, C′, etc., are equal to the
A, B, C, etc., except for a factor eiδ, all probabilities obtained by taking the
absolute squares will be exactly the same, since (A′ + B′ + C′ + · · · ) is then
equal to eiδ(A + B + C + · · · ). Or suppose, for instance, that we were computing
6-3","534"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","148","something with Eq. (6.1), but then we suddenly change all of the phases of a
certain base system. Every one of the amplitudes ⟨i | ψ⟩would be multiplied by
the same factor eiδ. Similarly, the amplitudes ⟨i | χ⟩would also be changed by eiδ,
but the amplitudes ⟨χ | i⟩are the complex conjugates of the amplitudes ⟨i | χ⟩;
therefore, the former gets changed by the factor e−iδ. The plus and minus iδ’s in
the exponents cancel out, and we would have the same expression we had before.
So it is a general rule that if we change all the amplitudes with respect to a given
base system by the same phase—or even if we just change all the amplitudes
in any problem by the same phase—it makes no diﬀerence. There is, therefore,
some freedom to choose the phases in our transformation matrix. Every now and
then we will make such an arbitrary choice—usually following the conventions
that are in general use.
6-2 Transforming to a rotated coordinate system
We consider again the “improved” Stern-Gerlach apparatus described in the
last chapter. A beam of spin one-half particles, entering at the left, would, in
general, be split into two beams, as shown schematically in Fig. 6-1. (There were
three beams for spin one.) As before, the beams are put back together again
unless one or the other of them is blocked oﬀby a “stop” which intercepts the
SIDE VIEW
TOP VIEW
FIELD
GRADIENT
z
y
y
x
+
−
Fig. 6-1. Top and side views of an “improved” Stern-Gerlach apparatus
with beams of a spin one-half particle.
6-4","267"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","149","beam at its half-way point. In the ﬁgure we show an arrow which points in the
direction of the increase of the magnitude of the ﬁeld—say toward the magnet
pole with the sharp edges. This arrow we take to represent the “up” axis of any
particular apparatus. It is ﬁxed relative to the apparatus and will allow us to
indicate the relative orientations when we use several apparatuses together. We
also assume that the direction of the magnetic ﬁeld in each magnet is always the
same with respect to the arrow.
We will say that those atoms which go in the “upper” beam are in the (+) state
with respect to that apparatus and that those in the “lower” beam are in the
(−) state. (There is no “zero” state for spin one-half particles.)
(a)
S
T
α
(b)
S
T
α
Fig. 6-2. Two equivalent experiments.
Now suppose we put two of our modiﬁed Stern-Gerlach apparatuses in se-
quence, as shown in Fig. 6-2(a). The ﬁrst one, which we call S, can be used to
prepare a pure (+S) or a pure (−S) state by blocking one beam or the other. [As
shown it prepares a pure (+S) state.] For each condition, there is some amplitude
for a particle that comes out of S to be in either the (+T) or the (−T) beam of
the second apparatus. There are, in fact, just four amplitudes: the amplitude
6-5","239"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","150","to go from (+S) to (+T), from (+S) to (−T), from (−S) to (+T), from (−S)
to (−T). These amplitudes are just the four coeﬃcients of the transformation
matrix Rji to go from the S-representation to the T-representation. We can
consider that the ﬁrst apparatus “prepares” a particular state in one representa-
tion and that the second apparatus “analyzes” that state in terms of the second
representation. The kind of question we want to answer, then, is this: If an atom
has been prepared in a given condition—say the (+S) state—by blocking one of
the beams in the apparatus S, what is the chance that it will get through the
second apparatus T if this is set for, say, the (−T) state. The result will depend,
of course, on the angles between the two systems S and T.
We should explain why it is that we could have any hope of ﬁnding the
coeﬃcients Rji by deduction. You know that it is almost impossible to believe
that if a particle has its spin lined up in the +z-direction, that there is some
chance of ﬁnding the same particle with its spin pointing in the +x-direction—or
in any other direction at all. In fact, it is almost impossible, but not quite. It is
so nearly impossible that there is only one way it can be done, and that is the
reason we can ﬁnd out what that unique way is.
The ﬁrst kind of argument we can make is this. Suppose we have a setup
like the one in Fig. 6-2(a), in which we have the two apparatuses S and T,
with T cocked at the angle α with respect to S, and we let only the (+) beam
through S and the (−) beam through T. We would observe a certain number for
the probability that the particles coming out of S get through T. Now suppose
we make another measurement with the apparatus of Fig. 6-2(b). The relative
orientation of S and T is the same, but the whole system sits at a diﬀerent angle
in space. We want to assume that both of these experiments give the same
number for the chance that a particle in a pure state with respect to S will get
into some particular state with respect to T. We are assuming, in other words,
that the result of any experiment of this type is the same—that the physics is
the same—no matter how the whole apparatus is oriented in space. (You say,
“That’s obvious.” But it is an assumption, and it is “right” only if it is actually
what happens.) That means that the coeﬃcients Rji depend only on the relation
in space of S and T, and not on the absolute situation of S and T. To say this in
another way, Rji depends only on the rotation which carries S to T, for evidently
what is the same in Fig. 6-2(a) and Fig. 6-2(b) is the three-dimensional rotation
which would carry apparatus S into the orientation of apparatus T. When the
transformation matrix Rji depends only on a rotation, as it does here, it is called
a rotation matrix.
6-6","529"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","151","(a)
S
T
U
(b)
S
U
Fig. 6-3. If T is “wide open,” (b) is equivalent to (a).
For our next step we will need one more piece of information. Suppose we
add a third apparatus which we can call U, which follows T at some arbitrary
angle, as in Fig. 6-3(a). (It’s beginning to look horrible, but that’s the fun of
abstract thinking—you can make the most weird experiments just by drawing
lines!) Now what is the S →T →U transformation? What we really want to
ask for is the amplitude to go from some state with respect to S to some other
state with respect to U, when we know the transformation from S to T and from
T to U. We are then asking about an experiment in which both channels of T
are open. We can get the answer by applying Eq. (6.5) twice in succession. For
going from the S-representation to the T-representation, we have
C′
j =
X
i
RT S
ji Ci,
(6.6)
6-7","172"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","152","where we put the superscripts TS on the R, so that we can distinguish it from
the coeﬃcients RUT we will have for going from T to U.
Assuming the amplitudes to be in the base states of the U-representation C′′
k ,
we can relate them to the T-amplitudes by using Eq. (6.5) once more; we get
C′′
k =
X
j
RUT
kj C′
j.
(6.7)
Now we can combine Eqs. (6.6) and (6.7) to get the transformation to U directly
from S. Substituting C′
j from Eq. (6.6) in Eq. (6.7), we have
C′′
k =
X
j
RUT
kj
X
i
RT S
ji Ci.
(6.8)
Or, since i does not appear in RUT
kj , we can put the i-summation also in front,
and write
C′′
k =
X
i
X
j
RUT
kj RT S
ji Ci.
(6.9)
This is the formula for a double transformation.
Notice, however, that so long as all the beams in T are unblocked, the state
coming out of T is the same as the one that went in. We could just as well have
made a transformation from the S-representation directly to the U-representation.
It should be the same as putting the U apparatus right after S, as in Fig. 6-3(b).
In that case, we would have written
C′′
k =
X
i
RUS
ki Ci,
(6.10)
with the coeﬃcients RUS
ki
belonging to this transformation. Now, clearly, Eqs.
(6.9) and (6.10) should give the same amplitudes C′′
k , and this should be true
no matter what the original state φ was which gave us the amplitudes Ci. So it
must be that
RUS
ki =
X
j
RUT
kj RT S
ji .
(6.11)
In other words, for any rotation S →U of a reference base, which is viewed
as a compounding of two successive rotations S →T and T →U, the rotation
matrix RUS
ki
can be obtained from the matrices of the two partial rotations by
6-8","333"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","153","Eq. (6.11). If you wish, you can ﬁnd Eq. (6.11) directly from Eq. (6.1), for it is
only a diﬀerent notation for ⟨kU | iS⟩= P
j⟨kU | jT⟩⟨jT | iS⟩.
To be thorough, we should add the following parenthetical remarks. They are not
terribly important, however, so you can skip to the next section if you want. What
we have said is not quite right. We cannot really say that Eq. (6.9) and Eq. (6.10)
must give exactly the same amplitudes. Only the physics should be the same; all the
amplitudes could be diﬀerent by some common phase factor like eiδ without changing
the result of any calculation about the real world. So, instead of Eq. (6.11), all we can
say, really, is that
eiδRUS
ki =
X
j
RUT
kj RT S
ji ,
(6.12)
where δ is some real constant. What this extra factor of eiδ means, of course, is that the
amplitudes we get if we use the matrix RUS might all diﬀer by the same phase (e−iδ)
from the amplitude we would get using the two rotations RUT and RT S. We know that
it doesn’t matter if all amplitudes are changed by the same phase, so we could just
ignore this phase factor if we wanted to. It turns out, however, that if we deﬁne all of our
rotation matrices in a particular way, this extra phase factor will never appear—the δ in
Eq. (6.12) will always be zero. Although it is not important for the rest of our arguments,
we can give a quick proof by using a mathematical theorem about determinants. [If
you don’t yet know much about determinants, don’t worry about the proof and just
skip to the deﬁnition of Eq. (6.15).]
First, we should say that Eq. (6.11) is the mathematical deﬁnition of a “product”
of two matrices. (It is just convenient to be able to say: “RUS is the product of RUT
and RT S.”) Second, there is a theorem of mathematics—which you can easily prove for
the two-by-two matrices we have here—which says that the determinant of a “product”
of two matrices is the product of their determinants. Applying this theorem to Eq. (6.12),
we get
ei2δ (Det RUS) = (Det RUT ) · (Det RT S).
(6.13)
(We leave oﬀthe subscripts, because they don’t tell us anything useful.) Yes, the 2δ
is right. Remember that we are dealing with two-by-two matrices; every term in the
matrix RUS
ki
is multiplied by eiδ, so each product in the determinant—which has two
factors—gets multiplied by ei2δ. Now let’s take the square root of Eq. (6.13) and divide
it into Eq. (6.12); we get
RUS
ki
√
Det RUS =
X
j
RUT
kj
√
Det RUT
RT S
ji
√
Det RT S .
(6.14)
The extra phase factor has disappeared.
6-9","473"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","154","Now it turns out that if we want all of our amplitudes in any given representation
to be normalized (which means, you remember, that P
i⟨φ | i⟩⟨i | φ⟩= 1), the rotation
matrices will all have determinants that are pure imaginary exponentials, like eiα. (We
won’t prove it; you will see that it always comes out that way.) So we can, if we wish,
choose to make all our rotation matrices R have a unique phase by making Det R = 1.
It is done like this. Suppose we ﬁnd a rotation matrix R in some arbitrary way. We
make it a rule to “convert” it to “standard form” by deﬁning
Rstandard =
R
√
Det R
.
(6.15)
We can do this because we are just multiplying each term of R by the same phase factor,
to get the phases we want. In what follows, we will always assume that our matrices
have been put in the “standard form”; then we can use Eq. (6.11) without having any
extra phase factors.
6-3 Rotations about the z-axis
We are now ready to ﬁnd the transformation matrix Rji between two diﬀerent
representations. With our rule for compounding rotations and our assumption
that space has no preferred direction, we have the keys we need for ﬁnding the
matrix of any arbitrary rotation. There is only one solution. We begin with
the transformation which corresponds to a rotation about the z-axis. Suppose
we have two apparatuses S and T placed in series along a straight line with
their axes parallel and pointing out of the page, as shown in Fig. 6-4(a). We
take our “z-axis” in this direction. Surely, if the beam goes “up” (toward +z)
in the S apparatus, it will do the same in the T apparatus. Similarly, if it goes
down in S, it will go down in T. Suppose, however, that the T apparatus were
placed at some other angle, but still with its axis parallel to the axis of S, as
in Fig. 6-4(b). Intuitively, you would say that a (+) beam in S would still go
with a (+) beam in T, because the ﬁelds and ﬁeld gradients are still in the same
physical direction. And that would be quite right. Also, a (−) beam in S would
still go into a (−) beam in T. The same result would apply for any orientation
of T in the xy-plane of S. What does this tell us about the relation between
C′
+ = ⟨+T | ψ⟩, C′
−= ⟨−T | ψ⟩and C+ = ⟨+S | ψ⟩, C−= ⟨−S | ψ⟩? You might
conclude that any rotation about the z-axis of the “frame of reference” for base
states leaves the amplitudes to be “up” and “down” the same as before. We
could write C′
+ = C+ and C′
−= C−—but that is wrong. All we can conclude is
6-10","483"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","155","(a)
x
y
S
T
FIELD GRADIENT
P1
(b)
x
y
S
x′
y ′
T
P1
Fig. 6-4. Rotating 90◦about the z-axis.
that for such rotations the probabilities to be in the “up” beam are the same for
the S and T apparatuses. That is,
|C′
+| = |C+|
and
|C′
−| = |C−|.
We cannot say that the phases of the amplitudes referred to the T apparatus
may not be diﬀerent for the two diﬀerent orientations in (a) and (b) of Fig. 6-4.
The two apparatuses in (a) and (b) of Fig. 6-4 are, in fact, diﬀerent, as we
can see in the following way. Suppose that we put an apparatus in front of S
which produces a pure (+x) state. (The x-axis points toward the bottom of the
ﬁgure.) Such particles would be split into (+z) and (−z) beams in S, but the
two beams would be recombined to give a (+x) state again at P1—the exit of S.
The same thing happens again in T. If we follow T by a third apparatus U,
whose axis is in the (+x) direction and, as shown in Fig. 6-5(a), all the particles
would go into the (+) beam of U. Now imagine what happens if T and U are
6-11","212"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","156","(a)
x
y
(+x)
S
T
U
(+x)
P1
(b)
x
y
x′
y ′
(+x)
S
T
U
(+?)
P1
Fig. 6-5. Particle in a (+x) state behaves diﬀerently in (a) and (b).
swung around together by 90◦to the positions shown in Fig. 6-5(b). Again, the
T apparatus puts out just what it takes in, so the particles that enter U are in a
(+x) state with respect to S. But U now analyzes for the (+y) state with respect
to S, which is diﬀerent. (By symmetry, we would now expect only one-half of
the particles to get through.)
What could have changed? The apparatuses T and U are still in the same
physical relationship to each other. Can the physics be changed just because T
and U are in a diﬀerent orientation? Our original assumption is that it should
not. It must be that the amplitudes with respect to T are diﬀerent in the two
cases shown in Fig. 6-5—and, therefore, also in Fig. 6-4. There must be some
way for a particle to know that it has turned the corner at P1. How could it tell?
Well, all we have decided is that the magnitudes of C′
+ and C+ are the same in
6-12","208"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","157","the two cases, but they could—in fact, must—have diﬀerent phases. We conclude
that C′
+ and C+ must be related by
C′
+ = eiλC+,
and that C′
−and C−must be related by
C′
−= eiµC−,
where λ and µ are real numbers which must be related in some way to the angle
between S and T.
The only thing we can say at the moment about λ and µ is that they must
not be equal [except for the special case shown in Fig. 6-5(a), when T is in the
same orientation as S]. We have seen that equal phase changes in all amplitudes
have no physical consequence. For the same reason, we can always add the
same arbitrary amount to both λ and µ without changing anything. So we are
permitted to choose to make λ and µ equal to plus and minus the same number.
That is, we can always take
λ′ = λ −(λ + µ)
2
,
µ′ = µ −(λ + µ)
2
.
Then
λ′ = λ
2 −µ
2 = −µ′.
So we adopt the convention† that µ = −λ. We have then the general rule that
for a rotation of the reference apparatus by some angle about the z-axis, the
transformation is
C′
+ = e+iλC+,
C′
−= e−iλC−.
(6.16)
The absolute values are the same, only the phases are diﬀerent. These phase
factors are responsible for the diﬀerent results in the two experiments of Fig. 6-5.
Now we would like to know the law that relates λ to the angle between S
and T. We already know the answer for one case. If the angle is zero, λ is zero.
Now we will assume that the phase shift λ is a continuous function of angle φ
between S and T (see Fig. 6-4) as φ goes to zero—as only seems reasonable. In
other words, if we rotate T from the straight line through S by the small angle ϵ,
† Looking at it another way, we are just putting the transformation in the “standard form”
described in Section 6-2 by using Eq. (6.15).
6-13","354"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","158","the λ is also a small quantity, say mϵ, where m is some number. We write it
this way because we can show that λ must be proportional to ϵ. Suppose we
were to put after T another apparatus T ′ which makes the angle ϵ with T, and,
therefore, the angle 2ϵ with S. Then, with respect to T, we have
C′
+ = eiλC+,
and with respect to T ′, we have
C′′
+ = eiλC′
+ = ei2λC+.
But we know that we should get the same result if we put T ′ right after S.
Thus, when the angle is doubled, the phase is doubled. We can evidently extend
the argument and build up any rotation at all by a sequence of inﬁnitesimal
rotations. We conclude that for any angle φ, λ is proportional to the angle. We
can, therefore, write λ = mφ.
The general result we get, then, is that for T rotated about the z-axis by the
angle φ with respect to S
C′
+ = eimφC+,
C′
−= e−imφC−.
(6.17)
For the angle φ, and for all rotations we speak of in the future, we adopt the
standard convention that a positive rotation is a right-handed rotation about the
positive direction of the reference axis. A positive φ has the sense of rotation of
a right-handed screw advancing in the positive z-direction.
Now we have to ﬁnd what m must be. First, we might try this argument:
Suppose T is rotated by 360◦; then, clearly, it is right back at zero degrees, and
we should have C′
+ = C+ and C′
−= C−, or, what is the same thing, eim2π = 1.
We get m = 1. This argument is wrong! To see that it is, consider that T is
rotated by 180◦. If m were equal to 1, we would have C′
+ = eiπC+ = −C+
and C′
−= e−iπC−= −C−. However, this is just the original state all over
again. Both amplitudes are just multiplied by −1 which gives back the original
physical system. (It is again a case of a common phase change.) This means
that if the angle between T and S in Fig. 6-5(b) is increased to 180◦, the system
(with respect to T) would be indistinguishable from the zero-degree situation,
and the particles would again go through the (+) state of the U apparatus.
At 180◦, though, the (+) state of the U apparatus is the (−x) state of the original
S apparatus. So a (+x) state would become a (−x) state. But we have done
nothing to change the original state; the answer is wrong. We cannot have m = 1.
6-14","449"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","159","We must have the situation that a rotation by 360◦and no smaller angle
reproduces the same physical state. This will happen if m = 1
2. Then, and only
then, will the ﬁrst angle that reproduces the same physical state be φ = 360◦.†
It gives
C′
+ = −C+
C′
−= −C−


360◦about z-axis.
(6.18)
It is very curious to say that if you turn the apparatus 360◦you get new amplitudes.
They aren’t really new, though, because the common change of sign doesn’t give
any diﬀerent physics. If someone else had decided to change all the signs of the
amplitudes because he thought he had turned 360◦, that’s all right; he gets the
same physics.‡ So our ﬁnal answer is that if we know the amplitudes C+ and C−
for spin one-half particles with respect to a reference frame S, and we then use a
base system referred to T which is obtained from S by a rotation of φ around
the z-axis, the new amplitudes are given in terms of the old by
C′
+ = eiφ/2C+
C′
−= e−iφ/2C−





φ about z.
(6.19)
6-4 Rotations of 180◦and 90◦about y
Next, we will try to guess the transformation for a rotation of T with respect
to S of 180◦around an axis perpendicular to the z-axis—say, about the y-axis.
(We have deﬁned the coordinate axes in Fig. 6-1.) In other words, we start with
two identical Stern-Gerlach equipments, with the second one, T, turned “upside
down” with respect to the ﬁrst one, S, as in Fig. 6-6. Now if we think of our
particles as little magnetic dipoles, a particle that is in the (+S) state—so that it
goes on the “upper” path in the ﬁrst apparatus—will also take the “upper” path
in the second, so that it will be in the minus state with respect to T. (In the
inverted T apparatus, both the gradients and the ﬁeld direction are reversed; for
† It appears that m = −1
2 would also work. However, we see in (6.17) that the change in
sign merely redeﬁnes the notation for a spin-up particle.
‡ Also, if something has been rotated by a sequence of small rotations whose net result
is to return it to the original orientation, it is possible to deﬁne the idea that it has been
rotated 360◦—as distinct from zero net rotation—if you have kept track of the whole history.
(Interestingly enough, this is not true for a net rotation of 720◦.)
6-15","423"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","160","S
T
z
y
Fig. 6-6. A rotation of 180◦about the y-axis.
a particle with its magnetic moment in a given direction, the force is unchanged.)
Anyway, what is “up” with respect to S will be “down” with respect to T. For
these relative positions of S and T, then, we know that the transformation must
give
|C′
+| = |C−|,
|C′
−| = |C+|.
As before, we cannot rule out some additional phase factors; we could have
(for 180◦about the y-axis)
C′
+ = eiβC−
and
C′
−= eiγC+,
(6.20)
where β and γ are still to be determined.
What about a rotation of 360◦about the y-axis? Well, we already know
the answer for a rotation of 360◦about the z-axis—the amplitude to be in any
state changes sign. A rotation of 360◦around any axis always brings us back
to the original position. It must be that for any 360◦rotation, the result is
the same as a 360◦rotation about the z-axis—all amplitudes simply change
sign. Now suppose we imagine two successive rotations of 180◦about y—using
Eq. (6.20)—we should get the result of Eq. (6.18). In other words,
C′′
+ = eiβC′
−= eiβeiγC+ = −C+
and
(6.21)
C′′
−= eiγC′
+ = eiγeiβC−= −C−.
This means that
eiβeiγ = −1
or
eiγ = −e−iβ.
So the transformation for a rotation of 180◦about the y-axis can be written
C′
+ = eiβC−,
C′
−= −e−iβC+.
(6.22)
6-16","236"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","161","The arguments we have just used would apply equally well to a rotation
of 180◦about any axis in the xy-plane, although diﬀerent axes can, of course,
give diﬀerent numbers for β. However, that is the only way they can diﬀer. Now
there is a certain amount of arbitrariness in the number β, but once it is speciﬁed
for one axis of rotation in the xy-plane it is determined for any other axis. It is
conventional to choose to set β = 0 for a 180◦rotation about the y-axis.
To show that we have this choice, suppose we imagine that β was not equal
to zero for a rotation about the y-axis; then we can show that there is some other
axis in the xy-plane, for which the corresponding phase factor will be zero. Let’s
ﬁnd the phase factor βA for an axis A that makes the angle α with the y-axis, as
shown in Fig. 6-7(a). (For clarity, the ﬁgure is drawn with α equal to a negative
number, but that doesn’t matter.) Now if we take a T apparatus which is initially
lined up with the S apparatus and is then rotated 180◦about the axis A, its
axes—which we will call x′′, y′′, and z′′—will be as shown in Fig. 6-7(a). The
amplitudes with respect to T will then be
C′′
+ = eiβAC−,
C′′
−= −e−iβAC+.
(6.23)
We can now think of getting to the same orientation by the two successive
rotations shown in (b) and (c) of the ﬁgure. First, we imagine an apparatus U
which is rotated with respect to S by 180◦about the y-axis. The axes x′, y′,
and z′ of U will be as shown in Fig. 6-7(b), and the amplitudes with respect to U
are given by (6.22).
Now notice that we can go from U to T by a rotation about the “z-axis” of U,
namely about z′, as shown in Fig. 6-7(c). From the ﬁgure you can see that the
angle required is two times the angle α but in the opposite direction (with respect
to z′). Using the transformation of (6.19) with φ = −2α, we get
C′′
+ = e−iαC′
+,
C′′
−= e+iαC′
−.
(6.24)
Combining Eqs. (6.24) and (6.22), we get that
C′′
+ = ei(β−α)C−,
C′′
−= −e−i(β−α)C+.
(6.25)
These amplitudes must, of course, be the same as we got in (6.23). So βA must
be related to α and β by
βA = β −α.
(6.26)
This means that if the angle α between the A-axis and the y-axis (of S) is equal
to β, the transformation for a rotation of 180◦about A will have βA = 0.
6-17","444"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","162","(a)
x
y
z
A
α
α
180◦
x′′
y ′′
z ′′
(b)
x
y, y ′
z
180◦
x′
z ′
(c)
x
y
z
2α
x′′
y ′′
z ′′
Fig. 6-7. A 180◦rotation about the axis A is equivalent to a rotation
of 180◦about y, followed by a rotation about z′.
Now so long as some axis perpendicular to the z-axis is going to have β = 0,
we may as well take it to be the y-axis. It is purely a matter of convention, and
we adopt the one in general use. Our result: For a rotation of 180◦about the
y-axis, we have
C′
+ = C−
C′
−= −C+


180◦about y.
(6.27)
6-18","121"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","163","While we are thinking about the y-axis, let’s next ask for the transformation
matrix for a rotation of 90◦about y. We can ﬁnd it because we know that two
successive 90◦rotations about the same axis must equal one 180◦rotation. We
start by writing the transformation for 90◦in the most general form:
C′
+ = aC+ + bC−,
C′
−= cC+ + dC−.
(6.28)
A second rotation of 90◦about the same axis would have the same coeﬃcients:
C′′
+ = aC′
+ + bC′
−,
C′′
−= cC′
+ + dC′
−.
(6.29)
Combining Eqs. (6.28) and (6.29), we have
C′′
+ = a(aC+ + bC−) + b(cC+ + dC−),
C′′
−= c(aC+ + bC−) + d(cC+ + dC−).
(6.30)
However, from (6.27) we know that
C′′
+ = C−,
C′′
−= −C+,
so that we must have that
ab + bd = 1,
a2 + bc = 0,
ac + cd = −1,
bc + d2 = 0.
(6.31)
These four equations are enough to determine all our unknowns: a, b, c, and d. It
is not hard to do. Look at the second and fourth equations. Deduce that a2 = d2,
which means that a = d or else that a = −d. But a = −d is out, because then the
ﬁrst equation wouldn’t be right. So d = a. Using this, we have immediately that
b = 1/2a and that c = −1/2a. Now we have everything in terms of a. Putting,
say, the second equation all in terms of a, we have
a2 −
1
4a2 = 0
or
a4 = 1
4.
This equation has four diﬀerent solutions, but only two of them give the standard
6-19","282"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","164","value for the determinant. We might as well take a = 1/
√
2; then†
a = 1/
√
2,
b = 1/
√
2,
c = −1/
√
2,
d = 1/
√
2.
In other words, for two apparatuses S and T, with T rotated with respect
to S by 90◦about the y-axis, the transformation is
C′
+ =
1
√
2 (C+ + C−)
C′
−=
1
√
2 (−C+ + C−)











90◦about y.
(6.32)
We can, of course, solve these equations for C+ and C−, which will give us
the transformation for a rotation of minus 90◦about y. Changing the primes
around, we would conclude that
C′
+ =
1
√
2 (C+ −C−)
C′
−=
1
√
2 (C+ + C−)











−90◦about y.
(6.33)
6-5 Rotations about x
You may be thinking: “This is getting ridiculous. What are they going to do
next, 47◦around y, then 33◦about x, and so on, forever?” No, we are almost
ﬁnished. With just two of the transformations we have—90◦about y, and an
arbitrary angle about z (which we did ﬁrst if you remember)—we can generate
any rotation at all.
As an illustration, suppose that we want the angle α around x. We know
how to deal with the angle α around z, but now we want it around x. How do
we get it? First, we turn the axis z down onto x—which is a rotation of +90◦
about y, as shown in Fig. 6-8. Then we turn through the angle α around z′.
† The other solution changes all signs of a, b, c, and d and corresponds to a −270◦rotation.
6-20","296"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","165","(a)
x, z ′
y, y ′
z
90◦
x′
(b)
x, z ′′
y
z
x′′
y ′′
α
α
(c)
x, x′′′
y
z
−90◦
y ′′′
z ′′′
Fig. 6-8. A rotation by α about the x-axis is equivalent to: (a) a
rotation by +90◦about y, followed by (b) a rotation by α about z′,
followed by (c) a rotation of −90◦about y ′′.
Then we rotate −90◦about y′′. The net result of the three rotations is the same
as turning around x by the angle α. It is a property of space.
(These facts of the combinations of rotations, and what they produce, are
hard to grasp intuitively. It is rather strange, because we live in three dimensions,
but it is hard for us to appreciate what happens if we turn this way and then
that way. Perhaps, if we were ﬁsh or birds and had a real appreciation of what
happens when we turn somersaults in space, we could more easily appreciate
such things.)
Anyway, let’s work out the transformation for a rotation by α around the
x-axis by using what we know. From the ﬁrst rotation by +90◦around y the
6-21","197"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","166","amplitudes go according to Eq. (6.32). Calling the rotated axes x′, y′, and z′,
the next rotation by the angle α around z′ takes us to a frame x′′, y′′, z′′, for
which
C′′
+ = eiα/2C′
+,
C′′
−= e−iα/2C′
−.
The last rotation of −90◦about y′′ takes us to x′′′, y′′′, z′′′; by (6.33),
C′′′
+ =
1
√
2 (C′′
+ −C′′
−),
C′′′
−=
1
√
2 (C′′
+ + C′′
−).
Combining these last two transformations, we get
C′′′
+ =
1
√
2 (e+iα/2C′
+ −e−iα/2C′
−),
C′′′
−=
1
√
2 (e+iα/2C′
+ + e−iα/2C′
−).
Using Eqs. (6.32) for C′
+ and C′
−, we get the complete transformation:
C′′′
+ = 1
2{e+iα/2(C+ + C−) −e−iα/2(−C+ + C−)},
C′′′
−= 1
2{e+iα/2(C+ + C−) + e−iα/2(−C+ + C−)}.
We can put these formulas in a simpler form by remembering that
eiθ + e−iθ = 2 cos θ,
and
eiθ −e−iθ = 2i sin θ.
We get
C′′′
+ =

cos α
2

C+ + i

sin α
2

C−
C′′′
−= i

sin α
2

C+ +

cos α
2

C−











α about x.
(6.34)
Here is our transformation for a rotation about the x-axis by any angle α. It is
only a little more complicated than the others.
6-6 Arbitrary rotations
Now we can see how to do any angle at all. First, notice that any relative
orientation of two coordinate frames can be described in terms of three angles,
6-22","270"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","167","x
y
z
β
α
γ
x1
x′
y ′
z′
Fig. 6-9. The orientation of any coordinate frame x′, y ′, z′ relative to
another frame x, y, z can be deﬁned in terms of Euler’s angles α, β, γ.
as shown in Fig. 6-9. If we have a set of axes x′, y′, and z′ oriented in any way
at all with respect to x, y, and z, we can describe the relationship between the
two frames by means of the three Euler angles α, β, and γ, which deﬁne three
successive rotations that will bring the x, y, z frame into the x′, y′, z′ frame.
Starting at x, y, z, we rotate our frame through the angle β about the z-axis,
bringing the x-axis to the line x1. Then, we rotate by α about this temporary
x-axis, to bring z down to z′. Finally, a rotation about the new z-axis (that
is, z′) by the angle γ will bring the x-axis into x′ and the y-axis into y′.† We
know the transformations for each of the three rotations—they are given in (6.19)
and (6.34). Combining them in the proper order, we get
C′
+ = cos α
2 ei(β+γ)/2C+ + isin α
2 e−i(β−γ)/2C−,
C′
−= isin α
2 ei(β−γ)/2C+ + cos α
2 e−i(β+γ)/2C−.
(6.35)
† With a little work you can show that the frame x, y, z can also be brought into the frame
x′, y′, z′ by the following three rotations about the original axes: (1) rotate by the angle γ
around the original z-axis; (2) rotate by the angle α around the original x-axis; (3) rotate by
the angle β around the original z-axis.
6-23","284"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","168","So just starting from some assumptions about the properties of space, we
have derived the amplitude transformation for any rotation at all. That means
that if we know the amplitudes for any state of a spin one-half particle to go into
the two beams of a Stern-Gerlach apparatus S, whose axes are x, y, and z, we
can calculate what fraction would go into either beam of an apparatus T with the
axes x′, y′, and z′. In other words, if we have a state ψ of a spin one-half particle,
whose amplitudes are C+ = ⟨+ | ψ⟩and C−= ⟨−| ψ⟩to be “up” and “down”
with respect to the z-axis of the x, y, z frame, we also know the amplitudes C′
+
and C′
−to be “up” and “down” with respect to the z′-axis of any other frame x′,
y′, z′. The four coeﬃcients in Eqs. (6.35) are the terms of the “transformation
matrix” with which we can project the amplitudes of a spin one-half particle into
any other coordinate system.
We will now work out a few examples to show you how it all works. Let’s
take the following simple question. We put a spin one-half atom through a Stern-
Gerlach apparatus that transmits only the (+z) state. What is the amplitude
that it will be in the (+x) state? The +x axis is the same as the +z′ axis of a
system rotated 90◦about the y-axis. For this problem, then, it is simplest to use
Eqs. (6.32)—although you could, of course, use the complete equations of (6.35).
Since C+ = 1 and C−= 0, we get C′
+ = 1/
√
2. The probabilities are the absolute
square of these amplitudes; there is a 50 percent chance that the particle will
go through an apparatus that selects the (+x) state. If we had asked about the
(−x) state the amplitude would have been −1/
√
2, which also gives a probabil-
ity 1/2—as you would expect from the symmetry of space. So if a particle is in
the (+z) state, it is equally likely to be in (+x) or (−x), but with opposite phase.
There’s no prejudice in y either. A particle in the (+z) state has a 50–50
chance of being in (+y) or in (−y). However, for these (using the formula for
rotating −90◦about x), the amplitudes are 1/
√
2 and −i/
√
2. In this case, the
two amplitudes have a phase diﬀerence of 90◦instead of 180◦, as they did for the
(+x) and (−x). In fact, that’s how the distinction between x and y shows up.
As our ﬁnal example, suppose that we know that a spin one-half particle is in
a state ψ such that it is polarized “up” along some axis A, deﬁned by the angles
θ and φ in Fig. 6-10. We want to know the amplitude C+ that the particle is
“up” along z and the amplitude C−that it is “down” along z. We can ﬁnd these
amplitudes by imagining that A is the z-axis of a system whose x-axis lies in
some arbitrary direction—say in the plane formed by A and z. We can then
bring the frame of A into x, y, z by three rotations. First, we make a rotation
by −π/2 about the axis A, which brings the x-axis into the line B in the ﬁgure.
6-24","564"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","169","O
x
y
z
φ
θ
A
B
Fig. 6-10. An axis A deﬁned by the polar angles θ and φ.
Then we rotate by θ about line B (the new x-axis of frame A) to bring A to the
z-axis. Finally, we rotate by the angle (π/2 −φ) about z. Remembering that we
have only a (+) state with respect to A, we get
C+ = cos θ
2 e−iφ/2,
C−= sin θ
2 e+iφ/2.
(6.36)
We would like, ﬁnally, to summarize the results of this chapter in a form
that will be useful for our later work. First, we remind you that our primary
result in Eqs. (6.35) can be written in another notation. Note that Eqs. (6.35)
mean just the same thing as Eq. (6.4). That is, in Eqs. (6.35) the coeﬃcients of
C+ = ⟨+S | ψ⟩and C−= ⟨−S | ψ⟩are just the amplitudes ⟨jT | iS⟩of Eq. (6.4)—
the amplitudes that a particle in the i-state with respect to S will be in the
j-state with respect to T (when the orientation of T with respect to S is given in
terms of the angles α, β, and γ). We also called them RT S
ji
in Eq. (6.6). (We have
a plethora of notations!) For example, RT S
−+ = ⟨−T | +S⟩is the coeﬃcient of C+
in the formula for C′
−, namely, i sin (α/2) ei(β−γ)/2. We can, therefore, make a
summary of our results in the form of a table, as we have done in Table 6-1.
It will occasionally be handy to have these amplitudes already worked out for
some simple special cases. Let’s let Rz(φ) stand for a rotation by the angle φ
6-25","285"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","170","Table 6-1
The amplitudes ⟨jT | iS⟩for a rotation deﬁned by the
Euler angles α, β, γ of Fig. 6-9
Rji(α, β, γ)
⟨jT | iS⟩
+S
−S
+T
cos α
2 ei(β+γ)/2
i sin α
2 e−i(β−γ)/2
−T
i sin α
2 ei(β−γ)/2
cos α
2 e−i(β+γ)/2
Table 6-2
The amplitudes ⟨jT | iS⟩for a rotation R(φ) by the angle
φ about the z-axis, x-axis, or y-axis
Rz(φ)
⟨jT | iS⟩
+S
−S
+T
eiφ/2
0
−T
0
e−iφ/2
Rx(φ)
⟨jT | iS⟩
+S
−S
+T
cos φ/2
i sin φ/2
−T
i sin φ/2
cos φ/2
Ry(φ)
⟨jT | iS⟩
+S
−S
+T
cos φ/2
sin φ/2
−T
−sin φ/2
cos φ/2
6-26","115"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","171","about the z-axis. We can also let it stand for the corresponding rotation matrix
(omitting the subscripts i and j, which are to be implicitly understood). In
the same spirit Rx(φ) and Ry(φ) will stand for rotations by the angle φ about
the x-axis or the y-axis.
We give in Table 6-2 the matrices—the tables of
amplitudes ⟨jT | iS⟩—which project the amplitudes from the S-frame into the
T-frame, where T is obtained from S by the rotation speciﬁed.
6-27","80"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","172","7
The Dependence of Amplitudes on Time
Review: Chapter 17, Vol. I, Space-Time
Chapter 48, Vol. I, Beats
7-1 Atoms at rest; stationary states
We want now to talk a little bit about the behavior of probability amplitudes
in time. We say a “little bit,” because the actual behavior in time necessarily
involves the behavior in space as well. Thus, we get immediately into the most
complicated possible situation if we are to do it correctly and in detail. We are
always in the diﬃculty that we can either treat something in a logically rigorous
but quite abstract way, or we can do something which is not at all rigorous
but which gives us some idea of a real situation—postponing until later a more
careful treatment. With regard to energy dependence, we are going to take the
second course. We will make a number of statements. We will not try to be
rigorous—but will just be telling you things that have been found out, to give
you some feeling for the behavior of amplitudes as a function of time. As we go
along, the precision of the description will increase, so don’t get nervous that we
seem to be picking things out of the air. It is, of course, all out of the air—the
air of experiment and of the imagination of people. But it would take us too
long to go over the historical development, so we have to plunge in somewhere.
We could plunge into the abstract and deduce everything—which you would not
understand—or we could go through a large number of experiments to justify
each statement. We choose to do something in between.
An electron alone in empty space can, under certain circumstances, have a
certain deﬁnite energy. For example, if it is standing still (so it has no transla-
tional motion, no momentum, or kinetic energy), it has its rest energy. A more
complicated object like an atom can also have a deﬁnite energy when standing
still, but it could also be internally excited to another energy level. (We will
7-1","345"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","173","describe later the machinery of this.) We can often think of an atom in an excited
state as having a deﬁnite energy, but this is really only approximately true. An
atom doesn’t stay excited forever because it manages to discharge its energy by
its interaction with the electromagnetic ﬁeld. So there is some amplitude that a
new state is generated—with the atom in a lower state, and the electromagnetic
ﬁeld in a higher state, of excitation. The total energy of the system is the same
before and after, but the energy of the atom is reduced. So it is not precise to
say an excited atom has a deﬁnite energy; but it will often be convenient and
not too wrong to say that it does.
[Incidentally, why does it go one way instead of the other way? Why does
an atom radiate light? The answer has to do with entropy. When the energy
is in the electromagnetic ﬁeld, there are so many diﬀerent ways it can be—so
many diﬀerent places where it can wander—that if we look for the equilibrium
condition, we ﬁnd that in the most probable situation the ﬁeld is excited with
a photon, and the atom is de-excited. It takes a very long time for the photon
to come back and ﬁnd that it can knock the atom back up again. It’s quite
analogous to the classical problem: Why does an accelerating charge radiate? It
isn’t that it “wants” to lose energy, because, in fact, when it radiates, the energy
of the world is the same as it was before. Radiation or absorption goes in the
direction of increasing entropy.]
Nuclei can also exist in diﬀerent energy levels, and in an approximation which
disregards the electromagnetic eﬀects, we can say that a nucleus in an excited
state stays there. Although we know that it doesn’t stay there forever, it is
often useful to start out with an approximation which is somewhat idealized and
easier to think about. Also it is often a legitimate approximation under certain
circumstances. (When we ﬁrst introduced the classical laws of a falling body,
we did not include friction, but there is almost never a case in which there isn’t
some friction.)
Then there are the subnuclear “strange particles,” which have various masses.
But the heavier ones disintegrate into other light particles, so again it is not
correct to say that they have a precisely deﬁnite energy. That would be true
only if they lasted forever. So when we make the approximation that they have
a deﬁnite energy, we are forgetting the fact that they must blow up. For the
moment, then, we will intentionally forget about such processes and learn later
how to take them into account.
Suppose we have an atom—or an electron, or any particle—which at rest
would have a deﬁnite energy E0. By the energy E0 we mean the mass of the
7-2","484"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","174","whole thing times c2. This mass includes any internal energy; so an excited atom
has a mass which is diﬀerent from the mass of the same atom in the ground state.
(The ground state means the state of lowest energy.) We will call E0 the “energy
at rest.”
For an atom at rest, the quantum mechanical amplitude to ﬁnd an atom at a
place is the same everywhere; it does not depend on position. This means, of
course, that the probability of ﬁnding the atom anywhere is the same. But it
means even more. The probability could be independent of position, and still the
phase of the amplitude could vary from point to point. But for a particle at rest,
the complete amplitude is identical everywhere. It does, however, depend on the
time. For a particle in a state of deﬁnite energy E0, the amplitude to ﬁnd the
particle at (x, y, z) at the time t is
ae−i(E0/ℏ)t,
(7.1)
where a is some constant. The amplitude to be at any point in space is the same
for all points, but depends on time according to (7.1). We shall simply assume
this rule to be true.
Of course, we could also write (7.1) as
ae−iωt,
(7.2)
with
ℏω = E0 = Mc2,
where M is the rest mass of the atomic state, or particle. There are three diﬀerent
ways of specifying the energy: by the frequency of an amplitude, by the energy
in the classical sense, or by the inertia. They are all equivalent; they are just
diﬀerent ways of saying the same thing.
You may be thinking that it is strange to think of a “particle” which has equal
amplitudes to be found throughout all space. After all, we usually imagine a
“particle” as a small object located “somewhere.” But don’t forget the uncertainty
principle. If a particle has a deﬁnite energy, it has also a deﬁnite momentum. If
the uncertainty in momentum is zero, the uncertainty relation, ∆p ∆x = ℏ, tells
us that the uncertainty in the position must be inﬁnite, and that is just what we
are saying when we say that there is the same amplitude to ﬁnd the particle at
all points in space.
If the internal parts of an atom are in a diﬀerent state with a diﬀerent total
energy, then the variation of the amplitude with time is diﬀerent. If you don’t
7-3","401"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","175","know in which state it is, there will be a certain amplitude to be in one state
and a certain amplitude to be in another—and each of these amplitudes will
have a diﬀerent frequency. There will be an interference between these diﬀerent
components—like a beat-note—which can show up as a varying probability.
Something will be “going on” inside of the atom—even though it is “at rest” in
the sense that its center of mass is not drifting. However, if the atom has one
deﬁnite energy, the amplitude is given by (7.1), and the absolute square of this
amplitude does not depend on time. You see, then, that if a thing has a deﬁnite
energy and if you ask any probability question about it, the answer is independent
of time. Although the amplitudes vary with time, if the energy is deﬁnite they
vary as an imaginary exponential, and the absolute value doesn’t change.
That’s why we often say that an atom in a deﬁnite energy level is in a
stationary state. If you make any measurements of the things inside, you’ll
ﬁnd that nothing (in probability) will change in time. In order to have the
probabilities change in time, we have to have the interference of two amplitudes
at two diﬀerent frequencies, and that means that we cannot know what the energy
is. The object will have one amplitude to be in a state of one energy and another
amplitude to be in a state of another energy. That’s the quantum mechanical
description of something when its behavior depends on time.
If we have a “condition” which is a mixture of two diﬀerent states with
diﬀerent energies, then the amplitude for each of the two states varies with time
according to Eq. (7.2), for instance, as
e−i(E1/ℏ)t
and
e−i(E2/ℏ)t.
(7.3)
And if we have some combination of the two, we will have an interference. But
notice that if we added a constant to both energies, it wouldn’t make any diﬀerence.
If somebody else were to use a diﬀerent scale of energy in which all the energies
were increased (or decreased) by a constant amount—say, by the amount A—then
the amplitudes in the two states would, from his point of view, be
e−i(E1+A)t/ℏ
and
e−i(E2+A)t/ℏ.
(7.4)
All of his amplitudes would be multiplied by the same factor e−i(A/ℏ)t, and all
linear combinations, or interferences, would have the same factor. When we take
the absolute squares to ﬁnd the probabilities, all the answers would be the same.
The choice of an origin for our energy scale makes no diﬀerence; we can measure
energy from any zero we want. For relativistic purposes it is nice to measure
7-4","444"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","176","the energy so that the rest mass is included, but for many purposes that aren’t
relativistic it is often nice to subtract some standard amount from all energies
that appear. For instance, in the case of an atom, it is usually convenient to
subtract the energy Msc2, where Ms is the mass of all the separate pieces—the
nucleus and the electrons—which is, of course, diﬀerent from the mass of the
atom. For other problems it may be useful to subtract from all energies the
amount Mgc2, where Mg is the mass of the whole atom in the ground state; then
the energy that appears is just the excitation energy of the atom. So, sometimes
we may shift our zero of energy by some very large constant, but it doesn’t make
any diﬀerence, provided we shift all the energies in a particular calculation by
the same constant. So much for a particle standing still.
7-2 Uniform motion
If we suppose that the relativity theory is right, a particle at rest in one
inertial system can be in uniform motion in another inertial system. In the rest
frame of the particle, the probability amplitude is the same for all x, y, and z but
varies with t. The magnitude of the amplitude is the same for all t, but the phase
depends on t. We can get a kind of a picture of the behavior of the amplitude if
we plot lines of equal phase—say, lines of zero phase—as a function of x and t.
x
t
x′
t′
Fig. 7-1. Relativistic transformation of the amplitude of a particle at
rest in the x-t systems.
7-5","274"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","177","For a particle at rest, these equal-phase lines are parallel to the x-axis and are
equally spaced in the t-coordinate, as shown by the dashed lines in Fig. 7-1.
In a diﬀerent frame—x′, y′, z′, t′—that is moving with respect to the particle
in, say, the x-direction, the x′ and t′ coordinates of any particular point in space
are related to x and t by the Lorentz transformation. This transformation can
be represented graphically by drawing x′ and t′ axes, as is done in Fig. 7-1. (See
Chapter 17, Vol. I, Fig. 17-2) You can see that in the x′-t′ system, points of
equal phase† have a diﬀerent spacing along the t′-axis, so the frequency of the
time variation is diﬀerent. Also there is a variation of the phase with x′, so the
probability amplitude must be a function of x′.
Under a Lorentz transformation for the velocity v, say along the negative
x-direction, the time t is related to the time t′ by
t = t′ −x′v/c2
p
1 −v2/c2 ,
so our amplitude now varies as
e−(i/ℏ)E0t = e−(i/ℏ)(E0t′/√
1−v2/c2−E0vx′/c2√
1−v2/c2).
In the prime system it varies in space as well as in time. If we write the amplitude
as
e−(i/ℏ)(E′
pt′−p′x′),
we see that E′
p = E0/
p
1 −v2/c2 is the energy computed classically for a particle
of rest energy E0 travelling at the velocity v, and p′ = E′
pv/c2 is the corresponding
particle momentum.
You know that xµ = (t, x, y, z) and pµ = (E, px, py, pz) are four-vectors, and
that pµxµ = Et −p · x is a scalar invariant. In the rest frame of the particle,
pµxµ is just Et; so if we transform to another frame, Et will be replaced by
E′t′ −p′ · x′.
Thus, the probability amplitude of a particle which has the momentum p will be
proportional to
e−(i/ℏ)(Ept−p·x),
(7.5)
† We are assuming that the phase should have the same value at corresponding points in
the two systems. This is a subtle point, however, since the phase of a quantum mechanical
amplitude is, to a large extent, arbitrary. A complete justiﬁcation of this assumption requires a
more detailed discussion involving interferences of two or more amplitudes.
7-6","374"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","178","where Ep is the energy of the particle whose momentum is p, that is,
Ep =
q
(pc)2 + E2
0,
(7.6)
where E0 is, as before, the rest energy. For nonrelativistic problems, we can write
Ep = Msc2 + Wp,
(7.7)
where Wp is the energy over and above the rest energy Msc2 of the parts of the
atom. In general, Wp would include both the kinetic energy of the atom as well
as its binding or excitation energy, which we can call the “internal” energy. We
would write
Wp = Wint + p2
2M ,
(7.8)
and the amplitudes would be
e−(i/ℏ)(Wpt−p·x).
(7.9)
Because we will generally be doing nonrelativistic calculations, we will use this
form for the probability amplitudes.
Note that our relativistic transformation has given us the variation of the
amplitude of an atom which moves in space without any additional assumptions.
The wave number of the space variations is, from (7.9),
k = p
ℏ;
(7.10)
so the wavelength is
λ = 2π
k = h
p .
(7.11)
This is the same wavelength we have used before for particles with the momen-
tum p. This formula was ﬁrst arrived at by de Broglie in just this way. For a
moving particle, the frequency of the amplitude variations is still given by
ℏω = Wp.
(7.12)
The absolute square of (7.9) is just 1, so for a particle in motion with a
deﬁnite energy, the probability of ﬁnding it is the same everywhere and does not
change with time. (It is important to notice that the amplitude is a complex
7-7","266"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","179","wave. If we used a real sine wave, the square would vary from point to point,
which would not be right.)
We know, of course, that there are situations in which particles move from
place to place so that the probability depends on position and changes with
time. How do we describe such situations? We can do that by considering
amplitudes which are a superposition of two or more amplitudes for states of
deﬁnite energy. We have already discussed this situation in Chapter 48 of Vol. I—
even for probability amplitudes! We found that the sum of two amplitudes with
diﬀerent wave numbers k (that is, momenta) and frequencies ω (that is, energies)
gives interference humps, or beats, so that the square of the amplitude varies
with space and time. We also found that these beats move with the so-called
“group velocity” given by
vg = ∆ω
∆k ,
where ∆k and ∆ω are the diﬀerences between the wave numbers and frequencies
for the two waves. For more complicated waves—made up of the sum of many
amplitudes all near the same frequency—the group velocity is
vg = dω
dk .
(7.13)
Taking ω = Ep/ℏand k = p/ℏ, we see that
vg = dEp
dp .
(7.14)
Using Eq. (7.6), we have
dEp
dp = c2 p
Ep
.
(7.15)
But Ep = Mc2, so
dEp
dp = p
M ,
(7.16)
which is just the classical velocity of the particle. Alternatively, if we use the
nonrelativistic expressions, we have
ω = Wp
ℏ
and
k = p
ℏ,
7-8","260"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","180","and
dω
dk = dWp
dp
= d
dp
 p2
2M

= p
M ,
(7.17)
which is again the classical velocity.
Our result, then, is that if we have several amplitudes for pure energy states
of nearly the same energy, their interference gives “lumps” in the probability that
move through space with a velocity equal to the velocity of a classical particle
of that energy. We should remark, however, that when we say we can add
two amplitudes of diﬀerent wave number together to get a beat-note that will
correspond to a moving particle, we have introduced something new—something
that we cannot deduce from the theory of relativity. We said what the amplitude
did for a particle standing still and then deduced what it would do if the particle
were moving. But we cannot deduce from these arguments what would happen
when there are two waves moving with diﬀerent speeds. If we stop one, we cannot
stop the other. So we have added tacitly the extra hypothesis that not only
is (7.9) a possible solution, but that there can also be solutions with all kinds
of p’s for the same system, and that the diﬀerent terms will interfere.
7-3 Potential energy; energy conservation
Now we would like to discuss what happens when the energy of a particle can
change. We begin by thinking of a particle which moves in a force ﬁeld described
by a potential. We discuss ﬁrst the eﬀect of a constant potential. Suppose that
M
p
φ
+
−
Fig. 7-2. A particle of mass M and momentum p in a region of
constant potential.
7-9","272"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","181","we have a large metal can which we have raised to some electrostatic potential φ,
as in Fig. 7-2. If there are charged objects inside the can, their potential energy
will be qφ, which we will call V , and will be absolutely independent of position.
Then there can be no change in the physics inside, because the constant potential
doesn’t make any diﬀerence so far as anything going on inside the can is concerned.
Now there is no way we can deduce what the answer should be, so we must make
a guess. The guess which works is more or less what you might expect: For the
energy, we must use the sum of the potential energy V and the energy Ep—which
is itself the sum of the internal and kinetic energies. The amplitude is proportional
to
e−(i/ℏ)[(Ep+V )t−p·x].
(7.18)
The general principle is that the coeﬃcient of t, which we may call ω, is always
given by the total energy of the system: internal (or “mass”) energy, plus kinetic
energy, plus potential energy:
ℏω = Ep + V.
(7.19)
Or, for nonrelativistic situations,
ℏω = Wint + p2
2M + V.
(7.20)
Now what about physical phenomena inside the box? If there are several
diﬀerent energy states, what will we get? The amplitude for each state has the
same additional factor
e−(i/ℏ)V t
over what it would have with V = 0. That is just like a change in the zero of
our energy scale. It produces an equal phase change in all amplitudes, but as we
have seen before, this doesn’t change any of the probabilities. All the physical
phenomena are the same. (We have assumed that we are talking about diﬀerent
states of the same charged object, so that qφ is the same for all. If an object
could change its charge in going from one state to another, we would have quite
another result, but conservation of charge prevents this.)
So far, our assumption agrees with what we would expect for a change of
energy reference level. But if it is really right, it should hold for a potential
energy that is not just a constant. In general, V could vary in any arbitrary way
with both time and space, and the complete result for the amplitude must be
7-10","385"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","182","given in terms of a diﬀerential equation. We don’t want to get concerned with the
general case right now, but only want to get some idea about how some things
happen, so we will think only of a potential that is constant in time and varies
very slowly in space. Then we can make a comparison between the classical and
quantum ideas.
φ1
φ2
DIST
Re(Amp)
λ1
λ2
(FOR φ2 < φ1)
Fig. 7-3. The amplitude for a particle in transit from one potential to
another.
Suppose we think of the situation in Fig. 7-3, which has two boxes held at
the constant potentials φ1 and φ2 and a region in between where we will assume
that the potential varies smoothly from one to the other. We imagine that some
particle has an amplitude to be found in any one of the regions. We also assume
that the momentum is large enough so that in any small region in which there
are many wavelengths, the potential is nearly constant. We would then think
that in any part of the space the amplitude ought to look like (7.18) with the
appropriate V for that part of the space.
Let’s think of a special case in which φ1 = 0, so that the potential energy
there is zero, but in which qφ2 is negative, so that classically the particle would
have more energy in the second box. Classically, it would be going faster in the
second box—it would have more energy and, therefore, more momentum. Let’s
see how that might come out of quantum mechanics.
With our assumption, the amplitude in the ﬁrst box would be proportional to
e−(i/ℏ)[(Wint+p2
1/2M+V1)t−p1·x],
(7.21)
7-11","280"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","183","and the amplitude in the second box would be proportional to
e−(i/ℏ)[(Wint+p2
2/2M+V2)t−p2·x].
(7.22)
(Let’s say that the internal energy is not being changed, but remains the same
in both regions.) The question is: How do these two amplitudes match together
through the region between the boxes?
We are going to suppose that the potentials are all constant in time—so that
nothing in the conditions varies. We will then suppose that the variations of the
amplitude (that is, its phase) have the same frequency everywhere—because, so
to speak, there is nothing in the “medium” that depends on time. If nothing in
the space is changing, we can consider that the wave in one region “generates”
subsidiary waves all over space which will all oscillate at the same frequency—just
as light waves going through materials at rest do not change their frequency. If
the frequencies in (7.21) and (7.22) are the same, we must have that
Wint + p2
1
2M + V1 = Wint + p2
2
2M + V2.
(7.23)
Both sides are just the classical total energies, so Eq. (7.23) is a statement of the
conservation of energy. In other words, the classical statement of the conservation
of energy is equivalent to the quantum mechanical statement that the frequencies
for a particle are everywhere the same if the conditions are not changing with
time. It all ﬁts with the idea that ℏω = E.
In the special example that V1 = 0 and V2 is negative, Eq. (7.23) gives that
p2 is greater than p1, so the wavelength of the waves is shorter in region 2. The
surfaces of equal phase are shown by the dashed lines in Fig. 7-3. We have also
drawn a graph of the real part of the amplitude, which shows again how the
wavelength decreases in going from region 1 to region 2. The group velocity of
the waves, which is p/M, also increases in the way one would expect from the
classical energy conservation, since it is just the same as Eq. (7.23).
There is an interesting special case where V2 gets so large that V2 −V1 is
greater than p2
1/2M. Then p2
2, which is given by
p2
2 = 2M
 p2
1
2M −V2 + V1

,
(7.24)
is negative. That means that p2 is an imaginary number, say, ip′. Classically,
we would say that the particle never gets into region 2—it doesn’t have enough
7-12","409"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","184","energy to climb the potential hill. Quantum mechanically, however, the amplitude
is still given by Eq. (7.22); its space variation still goes as
e(i/ℏ)p2·x.
But if p2 is imaginary, the space dependence becomes a real exponential. Say
that the particle was initially going in the +x-direction; then the amplitude
would vary as
e−p′x/ℏ.
(7.25)
The amplitude decreases rapidly with increasing x.
Imagine that the two regions at diﬀerent potentials were very close together, so
that the potential energy changed suddenly from V1 to V2, as shown in Fig. 7-4(a).
If we plot the real part of the probability amplitude, we get the dependence shown
in part (b) of the ﬁgure. The wave in the ﬁrst region corresponds to a particle
trying to get into the second region, but the amplitude there falls oﬀrapidly.
(b)
x
Re(Amp)
h
p1
¯h
|p2|
(a)
V1
p2
1/2m > 0
V2
p2
2/2m < 0
Fig. 7-4. The amplitude for a particle approaching a strongly repulsive
potential.
7-13","164"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","185","There is some chance that it will be observed in the second region—where it
could never get classically—but the amplitude is very small except right near the
boundary. The situation is very much like what we found for the total internal
reﬂection of light. The light doesn’t normally get out, but we can observe it if
we put something within a wavelength or two of the surface.
You will remember that if we put a second surface close to the boundary where
light was totally reﬂected, we could get some light transmitted into the second
piece of material. The corresponding thing happens to particles in quantum
mechanics. If there is a narrow region with a potential V , so great that the
classical kinetic energy would be negative, the particle would classically never
get past. But quantum mechanically, the exponentially decaying amplitude can
reach across the region and give a small probability that the particle will be found
on the other side where the kinetic energy is again positive. The situation is
illustrated in Fig. 7-5. This eﬀect is called the quantum mechanical “penetration
of a barrier.”
The barrier penetration by a quantum mechanical amplitude gives the ex-
planation—or description—of the α-particle decay of a uranium nucleus. The
x
Re(Amp)
V1
p2
1/2m > 0
V3
p2
3
2m <0
V2
p2
2/2m > 0
Fig. 7-5. The penetration of the amplitude through a potential barrier.
7-14","237"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","186","(a)
r
r1
V (r)
E
(b)
r
r · Re(Amp)
Fig. 7-6. (a) The potential function for an α-particle in a uranium
nucleus. (b) The qualitative form of the probability amplitude.
potential energy of an α-particle, as a function of the distance from the center, is
shown in Fig. 7-6(a). If one tried to shoot an α-particle with the energy E into
the nucleus, it would feel an electrostatic repulsion from the nuclear charge z and
would, classically, get no closer than the distance r1 where its total energy is equal
to the potential energy V . Closer in, however, the potential energy is much lower
because of the strong attraction of the short-range nuclear forces. How is it then
that in radioactive decay we ﬁnd α-particles which started out inside the nucleus
coming out with the energy E? Because they start out with the energy E inside
the nucleus and “leak” through the potential barrier. The probability amplitude
is roughly as sketched in part (b) of Fig. 7-6, although actually the exponential
decay is much larger than shown. It is, in fact, quite remarkable that the mean
life of an α-particle in the uranium nucleus is as long as 4 1
2 billion years, when
the natural oscillations inside the nucleus are so extremely rapid—about 1022 per
sec! How can one get a number like 109 years from 10−22 sec? The answer is that
the exponential gives the tremendously small factor of about e−45—which gives
the very small, though deﬁnite, probability of leakage. Once the α-particle is in
7-15","261"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","187","x
y
px
px
py
δθ
w
LOW V
HIGH V
F =−∂V/∂y
Fig. 7-7. The deﬂection of a particle by a transverse potential gradient.
the nucleus, there is almost no amplitude at all for ﬁnding it outside; however, if
you take many nuclei and wait long enough, you may be lucky and ﬁnd one that
has come out.
7-4 Forces; the classical limit
Suppose that we have a particle moving along and passing through a region
where there is a potential that varies at right angles to the motion. Classically,
we would describe the situation as sketched in Fig. 7-7. If the particle is moving
along the x-direction and enters a region where there is a potential that varies
with y, the particle will get a transverse acceleration from the force F = −∂V/∂y.
If the force is present only in a limited region of width w, the force will act only
for the time w/v. The particle will be given the transverse momentum
py = F w
v .
The angle of deﬂection δθ is then
δθ = py
p = Fw
pv ,
where p is the initial momentum. Using −∂V/∂y for F, we get
δθ = −w
pv
∂V
∂y .
(7.26)
It is now up to us to see if our idea that the waves go as (7.20) will explain
the same result. We look at the same thing quantum mechanically, assuming that
7-16","239"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","188","D
k
WAVE NODE
a
b
w
LOW V
HIGH V
∆x
δθ
δθ
Fig. 7-8. The probability amplitude in a region with a transverse
potential gradient.
everything is on a very large scale compared with a wavelength of our probability
amplitudes. In any small region we can say that the amplitude varies as
e−(i/ℏ)[(W +p2/2M+V )t−p·x].
(7.27)
Can we see that this will also give rise to a deﬂection of the particle when V has a
transverse gradient? We have sketched in Fig. 7-8 what the waves of probability
amplitude will look like. We have drawn a set of “wave nodes” which you can
think of as surfaces where the phase of the amplitude is zero. In every small
region, the wavelength—the distance between successive nodes—is
λ = h
p ,
where p is related to V through
W + p2
2M + V = const.
(7.28)
In the region where V is larger, p is smaller, and the wavelength is longer. So
the angle of the wave nodes gets changed as shown in the ﬁgure.
To ﬁnd the change in angle of the wave nodes we notice that for the two
paths a and b in Fig. 7-8 there is a diﬀerence of potential ∆V = (∂V/∂y)D, so
there is a diﬀerence ∆p in the momentum along the two tracks which can be
obtained from (7.28):
∆
 p2
2M

= p
M ∆p = −∆V.
(7.29)
7-17","241"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","189","The wave number p/ℏis, therefore, diﬀerent along the two paths, which means
that the phase is advancing at a diﬀerent rate. The diﬀerence in the rate of
increase of phase is ∆k = ∆p/ℏ, so the accumulated phase diﬀerence in the total
distance w is
∆(phase) = ∆k · w = ∆p
ℏ· w = −M
pℏ∆V · w.
(7.30)
This is the amount by which the phase on path b is “ahead” of the phase on
path a as the wave leaves the strip. But outside the strip, a phase advance of
this amount corresponds to the wave node being ahead by the amount
∆x = λ
2π ∆(phase) = ℏ
p ∆(phase)
or
∆x = −M
p2 ∆V · w.
(7.31)
Referring to Fig. 7-8, we see that the new wavefronts will be at the angle δθ
given by
∆x = D δθ;
(7.32)
so we have
D δθ = −M
p2 ∆V · w.
(7.33)
This is identical to Eq. (7.26) if we replace p/M by v and ∆V/D by ∂V/∂y.
The result we have just got is correct only if the potential variations are slow
and smooth—in what we call the classical limit. We have shown that under these
conditions we will get the same particle motions we get from F = ma, provided
we assume that a potential contributes a phase to the probability amplitude equal
to V t/ℏ. In the classical limit, the quantum mechanics will agree with Newtonian
mechanics.
7-5 The “precession” of a spin one-half particle
Notice that we have not assumed anything special about the potential energy—
it is just that energy whose derivative gives a force. For instance, in the Stern-
Gerlach experiment we had the energy U = −µ · B, which gives a force if B
7-18","299"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","190","has a spatial variation. If we wanted to give a quantum mechanical description,
we would have said that the particles in one beam had an energy that varied
one way and that those in the other beam had an opposite energy variation.
(We could put the magnetic energy U into the potential energy V or into the
“internal” energy W; it doesn’t matter.) Because of the energy variation, the
waves are refracted, and the beams are bent up or down. (We see now that
quantum mechanics would give us the same bending as we would compute from
the classical mechanics.)
From the dependence of the amplitude on potential energy we would also
expect that if a particle sits in a uniform magnetic ﬁeld along the z-direction, its
probability amplitude must be changing with time according to
e−(i/ℏ)(−µzB)t.
(We can consider that this is, in eﬀect, a deﬁnition of µz.) In other words, if we
place a particle in a uniform ﬁeld B for a time τ, its probability amplitude will
be multiplied by
e−(i/ℏ)(−µzB)τ
over what it would be in no ﬁeld. Since for a spin one-half particle, µz can be
either plus or minus some number, say µ, the two possible states in a uniform
ﬁeld would have their phases changing at the same rate but in opposite directions.
The two amplitudes get multiplied by
e±(i/ℏ)µBτ.
(7.34)
This result has some interesting consequences. Suppose we have a spin one-
half particle in some state that is not purely spin up or spin down. We can
describe its condition in terms of the amplitudes to be in the pure up and pure
down states. But in a magnetic ﬁeld, these two states will have phases changing
at a diﬀerent rate. So if we ask some question about the amplitudes, the answer
will depend on how long it has been in the ﬁeld.
As an example, we consider the disintegration of the muon in a magnetic
ﬁeld. When muons are produced as disintegration products of π-mesons, they
are polarized (in other words, they have a preferred spin direction). The muons,
in turn, disintegrate—in about 2.2 microseconds on the average—emitting an
electron and two neutrinos:
µ →e + ν + ¯ν.
7-19","370"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","191","x
z
µ
A
µ
B
SPIN
e
ELECTRON
COUNTER
Fig. 7-9. A muon-decay experiment.
In this disintegration it turns out that (for at least the highest energies) the
electrons are emitted preferentially in the direction opposite to the spin direction
of the muon.
Suppose then that we consider the experimental arrangement shown in Fig. 7-9.
If polarized muons enter from the left and are brought to rest in a block of material
at A, they will, a little while later, disintegrate. The electrons emitted will, in
general, go oﬀin all possible directions. Suppose, however, that the muons
all enter the stopping block at A with their spins in the x-direction. Without
a magnetic ﬁeld there would be some angular distribution of decay directions;
we would like to know how this distribution is changed by the presence of the
magnetic ﬁeld. We expect that it may vary in some way with time. We can ﬁnd
out what happens by asking, for any moment, what the amplitude is that the
muon will be found in the (+x) state.
We can state the problem in the following way: A muon is known to have
its spin in the +x-direction at t = 0; what is the amplitude that it will be in
the same state at the time τ? Now we do not have any rule for the behavior of
a spin one-half particle in a magnetic ﬁeld at right angles to the spin, but we
do know what happens to the spin up and spin down states with respect to the
ﬁeld—their amplitudes get multiplied by the factor (7.34). Our procedure then is
to choose the representation in which the base states are spin up and spin down
with respect to the z-direction (the ﬁeld direction). Any question can then be
expressed with reference to the amplitudes for these states.
Let’s say that ψ(t) represents the muon state. When it enters the block A,
its state is ψ(0), and we want to know ψ(τ) at the later time τ. If we represent
the two base states by (+z) and (−z) we know the two amplitudes ⟨+z | ψ(0)⟩
and ⟨−z | ψ(0)⟩—we know these amplitudes because we know that ψ(0) represents
7-20","371"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","192","a state with the spin in the (+x) state. From the results of the last chapter,
these amplitudes are†
⟨+z | +x⟩= C+ =
1
√
2
and
(7.35)
⟨−z | +x⟩= C−=
1
√
2.
They happen to be equal. Since these amplitudes refer to the condition at t = 0,
let’s call them C+(0) and C−(0).
Now we know what happens to these two amplitudes with time. Using (7.34),
we have
C+(t) = C+(0)e−(i/ℏ)µBt
and
(7.36)
C−(t) = C−(0)e+(i/ℏ)µBt.
But if we know C+(t) and C−(t), we have all there is to know about the condition
at t. The only trouble is that what we want to know is the probability that at t
the spin will be in the +x-direction. Our general rules can, however, take care
of this problem. We write that the amplitude to be in the (+x) state at time t,
which we may call A+(t), is
A+(t) = ⟨+x | ψ(t)⟩= ⟨+x | +z⟩⟨+z | ψ(t)⟩+ ⟨+x | −z⟩⟨−z | ψ(t)⟩
or
A+(t) = ⟨+x | +z⟩C+(t) + ⟨+x | −z⟩C−(t).
(7.37)
Again using the results of the last chapter—or better the equality ⟨φ | χ⟩= ⟨χ | φ⟩∗
from Chapter 5—we know that
⟨+x | +z⟩=
1
√
2,
⟨+x | −z⟩=
1
√
2.
So we know all the quantities in Eq. (7.37). We get
A+(t) = 1
2e(i/ℏ)µBt + 1
2e−(i/ℏ)µBt,
† If you skipped Chapter 6, you can just take (7.35) as an underived rule for now. We will
give later (in Chapter 10) a more complete discussion of spin precession, including a derivation
of these amplitudes.
7-21","269"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","193","or
A+(t) = cos µB
ℏt.
A particularly simple result! Notice that the answer agrees with what we expect
for t = 0. We get A+(0) = 1, which is right, because we assumed that the muon
was in the (+x) state at t = 0.
The probability P+ that the muon will be found in the (+x) state at t
is (A+)2 or
P+ = cos2 µBt
ℏ.
The probability oscillates between zero and one, as shown in Fig. 7-10. Note that
the probability returns to one for µBt/ℏ= π (not 2π). Because we have squared
the cosine function, the probability repeats itself with the frequency 2µB/ℏ.
µB
¯h
t
π
2π
PROB. TO HAVE
SPIN IN +x DIR.
0
1
Fig. 7-10. Time dependence of the probability that a spin one-half
particle will be in a (+) state with respect to the x-axis.
Thus, we ﬁnd that the chance of catching the decay electron in the electron
counter of Fig. 7-9 varies periodically with the length of time the muon has been
sitting in the magnetic ﬁeld. The frequency depends on the magnetic moment µ.
The magnetic moment of the muon has, in fact, been measured in just this way.
We can, of course, use the same method to answer any other questions about
the muon decay. For example, how does the chance of detecting a decay electron
in the y-direction at 90◦to the x-direction but still at right angles to the ﬁeld
depend on t? If you work it out, the amplitude to be in the (+y) state varies
as cos2{(µBt/ℏ) −π/4}, which oscillates with the same period but reaches its
maximum one-quarter cycle later, when µBt/ℏ= π/4. In fact, what is happening
7-22","287"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","194","is that as time goes on, the muon goes through a succession of states which
correspond to complete polarization in a direction that is continually rotating
about the z-axis. We can describe this by saying that the spin is precessing at
the frequency
ωp = 2µB
ℏ
.
(7.38)
You can begin to see the form that our quantum mechanical description will
take when we are describing how things behave in time.
7-23","73"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","195","8
The Hamiltonian Matrix
Review: Chapter 49, Vol. I, Modes
8-1 Amplitudes and vectors
Before we begin the main topic of this chapter, we would like to describe a
number of mathematical ideas that are used a lot in the literature of quantum
mechanics. Knowing them will make it easier for you to read other books or
papers on the subject. The ﬁrst idea is the close mathematical resemblance
between the equations of quantum mechanics and those of the scalar product
of two vectors. You remember that if χ and φ are two states, the amplitude to
start in φ and end up in χ can be written as a sum over a complete set of base
states of the amplitude to go from φ into one of the base states and then from
that base state out again into χ:
⟨χ | φ⟩=
X
all i
⟨χ | i⟩⟨i | φ⟩.
(8.1)
We explained this in terms of a Stern-Gerlach apparatus, but we remind you
that there is no need to have the apparatus. Equation (8.1) is a mathematical
law that is just as true whether we put the ﬁltering equipment in or not—it is
not always necessary to imagine that the apparatus is there. We can think of it
simply as a formula for the amplitude ⟨χ | φ⟩.
We would like to compare Eq. (8.1) to the formula for the dot product of two
vectors B and A. If B and A are ordinary vectors in three dimensions, we can
write the dot product this way:
X
all i
(B · ei)(ei · A),
(8.2)
8-1","269"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","196","with the understanding that the symbol ei stands for the three unit vectors in
the x, y, and z-directions. Then B · e1 is what we ordinarily call Bx; B · e2 is
what we ordinarily call By; and so on. So Eq. (8.2) is equivalent to
BxAx + ByAy + BzAz,
which is the dot product B · A.
Comparing Eqs. (8.1) and (8.2), we can see the following analogy: The states
χ and φ correspond to the two vectors B and A. The base states i correspond
to the special vectors ei to which we refer all other vectors. Any vector can be
represented as a linear combination of the three “base vectors” ei. Furthermore,
if you know the coeﬃcients of each “base vector” in this combination—that is, its
three components—you know everything about a vector. In a similar way, any
quantum mechanical state can be described completely by the amplitude ⟨i | φ⟩to
go into the base states; and if you know these coeﬃcients, you know everything
there is to know about the state. Because of this close analogy, what we have
called a “state” is often also called a “state vector.”
Since the base vectors ei are all at right angles, we have the relation
ei · ej = δij.
(8.3)
This corresponds to the relations (5.25) among the base states i,
⟨i | j⟩= δij.
(8.4)
You see now why one says that the base states i are all “orthogonal.”
There is one minor diﬀerence between Eq. (8.1) and the dot product. We
have that
⟨φ | χ⟩= ⟨χ | φ⟩∗.
(8.5)
But in vector algebra,
A · B = B · A.
With the complex numbers of quantum mechanics we have to keep straight the
order of the terms, whereas in the dot product, the order doesn’t matter.
Now consider the following vector equation:
A =
X
i
ei(ei · A).
(8.6)
8-2","319"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","197","It’s a little unusual, but correct. It means the same thing as
A =
X
i
Aiei = Axex + Ayey + Azez.
(8.7)
Notice, though, that Eq. (8.6) involves a quantity which is diﬀerent from a dot
product. A dot product is just a number, whereas Eq. (8.6) is a vector equation.
One of the great tricks of vector analysis was to abstract away from the equations
the idea of a vector itself. One might be similarly inclined to abstract a thing that
is the analog of a “vector” from the quantum mechanical formula Eq. (8.1)—and
one can indeed. We remove the ⟨χ | from both sides of Eq. (8.1) and write the
following equation (don’t get frightened—it’s just a notation and in a few minutes
you will ﬁnd out what the symbols mean):
| φ⟩=
X
i
| i⟩⟨i | φ⟩.
(8.8)
One thinks of the bracket ⟨χ | φ⟩as being divided into two pieces. The second
piece | φ⟩is often called a ket, and the ﬁrst piece ⟨χ | is called a bra (put together,
they make a “bra-ket”—a notation proposed by Dirac); the half-symbols | φ⟩
and ⟨χ | are also called state vectors. In any case, they are not numbers, and, in
general, we want the results of our calculations to come out as numbers; so such
“unﬁnished” quantities are only part-way steps in our calculations.
It happens that until now we have written all our results in terms of numbers.
How have we managed to avoid vectors? It is amusing to note that even in
ordinary vector algebra we could make all equations involve only numbers. For
instance, instead of a vector equation like
F = ma,
we could always have written
C · F = C · (ma).
We have then an equation between dot products that is true for any vector C.
But if it is true for any C, it hardly makes sense at all to keep writing the C!
Now look at Eq. (8.1). It is an equation that is true for any χ. So to save
writing, we should just leave out the χ and write Eq. (8.8) instead. It has the
same information provided we understand that it should always be “ﬁnished” by
8-3","376"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","198","“multiplying on the left by”—which simply means reinserting—some ⟨χ | on both
sides. So Eq. (8.8) means exactly the same thing as Eq. (8.1)—no more, no less.
When you want numbers, you put in the ⟨χ | you want.
Maybe you have already wondered about the φ in Eq. (8.8). Since the equation
is true for any φ, why do we keep it? Indeed, Dirac suggests that the φ also can
just as well be abstracted away, so that we have only
| =
X
i
| i⟩⟨i |.
(8.9)
And this is the great law of quantum mechanics! (There is no analog in vector
analysis.) It says that if you put in any two states χ and φ on the left and right
of both sides, you get back Eq. (8.1). It is not really very useful, but it’s a nice
reminder that the equation is true for any two states.
8-2 Resolving state vectors
Let’s look at Eq. (8.8) again; we can think of it in the following way. Any state
vector | φ⟩can be represented as a linear combination with suitable coeﬃcients of
a set of base “vectors”—or, if you prefer, as a superposition of “unit vectors” in
suitable proportions. To emphasize that the coeﬃcients ⟨i | φ⟩are just ordinary
(complex) numbers, suppose we write
⟨i | φ⟩= Ci.
Then Eq. (8.8) is the same as
| φ⟩=
X
i
| i⟩Ci.
(8.10)
We can write a similar equation for any other state vector, say | χ⟩, with, of
course, diﬀerent coeﬃcients—say Di. Then we have
| χ⟩=
X
i
| i⟩Di.
(8.11)
The Di are just the amplitudes ⟨i | χ⟩.
Suppose we had started by abstracting the φ from Eq. (8.1). We would have
had
⟨χ | =
X
i
⟨χ | i⟩⟨i |.
(8.12)
8-4","301"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","199","Remembering that ⟨χ | i⟩= ⟨i | χ⟩∗, we can write this as
⟨χ | =
X
i
D∗
i ⟨i |.
(8.13)
Now the interesting thing is that we can just multiply Eq. (8.13) and Eq. (8.10)
to get back ⟨χ | φ⟩. When we do that, we have to be careful of the summation
indices, because they are quite distinct in the two equations. Let’s ﬁrst rewrite
Eq. (8.13) as
⟨χ | =
X
j
D∗
j ⟨j |,
which changes nothing. Then putting it together with Eq. (8.10), we have
⟨χ | φ⟩=
X
ij
D∗
j ⟨j | i⟩Ci.
(8.14)
Remember, though, that ⟨j | i⟩= δij, so that in the sum we have left only the
terms with j = i. We get
⟨χ | φ⟩=
X
i
D∗
i Ci,
(8.15)
where, of course, D∗
i = ⟨i | χ⟩∗= ⟨χ | i⟩, and Ci = ⟨i | φ⟩. Again we see the close
analogy with the dot product
B · A =
X
i
BiAi.
The only diﬀerence is the complex conjugate on Di. So Eq. (8.15) says that if
the state vectors ⟨χ | and | φ⟩are expanded in terms of the base vectors ⟨i | or | i⟩,
the amplitude to go from φ to χ is given by the kind of dot product in Eq. (8.15).
This equation is, of course, just Eq. (8.1) written with diﬀerent symbols. So we
have just gone in a circle to get used to the new symbols.
We should perhaps emphasize again that while space vectors in three dimen-
sions are described in terms of three orthogonal unit vectors, the base vectors | i⟩
of the quantum mechanical states must range over the complete set applicable to
any particular problem. Depending on the situation, two, or three, or ﬁve, or an
inﬁnite number of base states may be involved.
We have also talked about what happens when particles go through an
apparatus. If we start the particles out in a certain state φ, then send them
8-5","343"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","200","through an apparatus, and afterward make a measurement to see if they are in
state χ, the result is described by the amplitude
⟨χ | A | φ⟩.
(8.16)
Such a symbol doesn’t have a close analog in vector algebra. (It is closer to
tensor algebra, but the analogy is not particularly useful.) We saw in Chapter 5,
Eq. (5.32), that we could write (8.16) as
⟨χ | A | φ⟩=
X
ij
⟨χ | i⟩⟨i | A | j⟩⟨j | φ⟩.
(8.17)
This is just an example of the fundamental rule Eq. (8.9), used twice.
We also found that if another apparatus B was added in series with A, then
we could write
⟨χ | BA | φ⟩=
X
ijk
⟨χ | i⟩⟨i | B | j⟩⟨j | A | k⟩⟨k | φ⟩.
(8.18)
Again, this comes directly from Dirac’s method of writing Eq. (8.9)—remember
that we can always place a bar (|), which is just like the factor 1, between B
and A.
Incidentally, we can think of Eq. (8.17) in another way. Suppose we think
of the particle entering apparatus A in the state φ and coming out of A in the
state ψ, (“psi”). In other words, we could ask ourselves this question: Can we
ﬁnd a ψ such that the amplitude to get from ψ to χ is always identically and
everywhere the same as the amplitude ⟨χ | A | φ⟩? The answer is yes. We want
Eq. (8.17) to be replaced by
⟨χ | ψ⟩=
X
i
⟨χ | i⟩⟨i | ψ⟩.
(8.19)
We can clearly do this if
⟨i | ψ⟩=
X
j
⟨i | A | j⟩⟨j | φ⟩= ⟨i | A | φ⟩,
(8.20)
which determines ψ. “But it doesn’t determine ψ,” you say; “it only deter-
mines ⟨i | ψ⟩.” However, ⟨i | ψ⟩does determine ψ, because if you have all the
8-6","314"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","201","coeﬃcients that relate ψ to the base states i, then ψ is uniquely deﬁned. In fact,
we can play with our notation and write the last term of Eq. (8.20) as
⟨i | ψ⟩=
X
j
⟨i | j⟩⟨j | A | φ⟩.
(8.21)
Then, since this equation is true for all i, we can write simply
| ψ⟩=
X
j
| j⟩⟨j | A | φ⟩.
(8.22)
Then we can say: “The state ψ is what we get if we start with φ and go through
the apparatus A.”
One ﬁnal example of the tricks of the trade. We start again with Eq. (8.17).
Since it is true for any χ and φ, we can drop them both! We then get†
A =
X
ij
| i⟩⟨i | A | j⟩⟨j |.
(8.23)
What does it mean? It means no more, no less, than what you get if you put
back the φ and χ. As it stands, it is an “open” equation and incomplete. If we
multiply it “on the right” by | φ⟩, it becomes
A | φ⟩=
X
ij
| i⟩⟨i | A | j⟩⟨j | φ⟩,
(8.24)
which is just Eq. (8.22) all over again. In fact, we could have just dropped the j’s
from that equation and written
| ψ⟩= A | φ⟩.
(8.25)
The symbol A is neither an amplitude, nor a vector; it is a new kind of thing
called an operator. It is something which “operates on” a state to produce a new
state—Eq. (8.25) says that | ψ⟩is what results if A operates on | φ⟩. Again, it is
still an open equation until it is completed with some bra like ⟨χ | to give
⟨χ | ψ⟩= ⟨χ | A | φ⟩.
(8.26)
† You might think we should write | A | instead of just A. But then it would look like the
symbol for “absolute value of A,” so the bars are usually dropped. In general, the bar (|)
behaves much like the factor one.
8-7","339"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","202","The operator A is, of course, described completely if we give the matrix of
amplitudes ⟨i | A | j⟩—also written Aij—in terms of any set of base vectors.
We have really added nothing new with all of this new mathematical notation.
One reason for bringing it all up was to show you the way of writing pieces of
equations, because in many books you will ﬁnd the equations written in the
incomplete forms, and there’s no reason for you to be paralyzed when you come
across them. If you prefer, you can always add the missing pieces to make an
equation between numbers that will look like something more familiar.
Also, as you will see, the “bra” and “ket” notation is a very convenient one.
For one thing, we can from now on identify a state by giving its state vector.
When we want to refer to a state of deﬁnite momentum p we can say: “the
state | p⟩.” Or we may speak of some arbitrary state | ψ⟩. For consistency we will
always use the ket, writing | ψ⟩, to identify a state. (It is, of course an arbitrary
choice; we could equally well have chosen to use the bra, ⟨ψ |.)
8-3 What are the base states of the world?
We have discovered that any state in the world can be represented as a
superposition—a linear combination with suitable coeﬃcients—of base states.
You may ask, ﬁrst of all, what base states?
Well, there are many diﬀerent
possibilities. You can, for instance, project a spin in the z-direction or in some
other direction. There are many, many diﬀerent representations, which are the
analogs of the diﬀerent coordinate systems one can use to represent ordinary
vectors. Next, what coeﬃcients? Well, that depends on the physical circumstances.
Diﬀerent sets of coeﬃcients correspond to diﬀerent physical conditions. The
important thing to know about is the “space” in which you are working—in other
words, what the base states mean physically. So the ﬁrst thing you have to know
about, in general, is what the base states are like. Then you can understand how
to describe a situation in terms of these base states.
We would like to look ahead a little and speak a bit about what the general
quantum mechanical description of nature is going to be—in terms of the now
current ideas of physics, anyway. First, one decides on a particular representation
for the base states—diﬀerent representations are always possible. For example,
for a spin one-half particle we can use the plus and minus states with respect to
the z-axis. But there’s nothing special about the z-axis—you can take any other
axis you like. For consistency we’ll always pick the z-axis, however. Suppose we
begin with a situation with one electron. In addition to the two possibilities for
8-8","470"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","203","the spin (“up” and “down” along the z-direction), there is also the momentum of
the electron. We pick a set of base states, each corresponding to one value of
the momentum. What if the electron doesn’t have a deﬁnite momentum? That’s
all right; we’re just saying what the base states are. If the electron hasn’t got a
deﬁnite momentum, it has some amplitude to have one momentum and another
amplitude to have another momentum, and so on. And if it is not necessarily
spinning up, it has some amplitude to be spinning up going at this momentum,
and some amplitude to be spinning down going at that momentum, and so on.
The complete description of an electron, so far as we know, requires only that the
base states be described by the momentum and the spin. So one acceptable set
of base states | i⟩for a single electron refer to diﬀerent values of the momentum
and whether the spin is up or down. Diﬀerent mixtures of amplitudes—that is,
diﬀerent combinations of the C’s describe diﬀerent circumstances. What any
particular electron is doing is described by telling with what amplitude it has an
up-spin or a down-spin and one momentum or another—for all possible momenta.
So you can see what is involved in a complete quantum mechanical description
of a single electron.
What about systems with more than one electron? Then the base states get
more complicated. Let’s suppose that we have two electrons. We have, ﬁrst of
all, four possible states with respect to spin: both electrons spinning up, the ﬁrst
one down and the second one up, the ﬁrst one up and the second one down, or
both down. Also we have to specify that the ﬁrst electron has the momentum p1,
and the second electron, the momentum p2. The base states for two electrons
require the speciﬁcation of two momenta and two spin characters. With seven
electrons, we have to specify seven of each.
If we have a proton and an electron, we have to specify the spin direction
of the proton and its momentum, and the spin direction of the electron and its
momentum. At least that’s approximately true. We do not really know what
the correct representation is for the world. It is all very well to start out by
supposing that if you specify the spin in the electron and its momentum, and
likewise for a proton, you will have the base states; but what about the “guts” of
the proton? Let’s look at it this way. In a hydrogen atom which has one proton
and one electron, we have many diﬀerent base states to describe—up and down
spins of the proton and electron and the various possible momenta of the proton
and electron. Then there are diﬀerent combinations of amplitudes Ci which
together describe the character of the hydrogen atom in diﬀerent states. But
suppose we look at the whole hydrogen atom as a “particle.” If we didn’t know
8-9","496"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","204","that the hydrogen atom was made out of a proton and an electron, we might have
started out and said: “Oh, I know what the base states are—they correspond to
a particular momentum of the hydrogen atom.” No, because the hydrogen atom
has internal parts. It may, therefore, have various states of diﬀerent internal
energy, and describing the real nature requires more detail.
The question is: Does a proton have internal parts? Do we have to describe a
proton by giving all possible states of protons, and mesons, and strange particles?
We don’t know. And even though we suppose that the electron is simple, so that
all we have to tell about it is its momentum and its spin, maybe tomorrow we will
discover that the electron also has inner gears and wheels. It would mean that
our representation is incomplete, or wrong, or approximate—in the same way
that a representation of the hydrogen atom which describes only its momentum
would be incomplete, because it disregarded the fact that the hydrogen atom
could have become excited inside. If an electron could become excited inside and
turn into something else like, for instance, a muon, then it would be described
not just by giving the states of the new particle, but presumably in terms of
some more complicated internal wheels. The main problem in the study of the
fundamental particles today is to discover what are the correct representations
for the description of nature. At the present time, we guess that for the electron
it is enough to specify its momentum and spin. We also guess that there is
an idealized proton which has its π-mesons, and K-mesons, and so on, that
all have to be speciﬁed. Several dozen particles—that’s crazy! The question
of what is a fundamental particle and what is not a fundamental particle—a
subject you hear so much about these days—is the question of what is the ﬁnal
representation going to look like in the ultimate quantum mechanical description
of the world. Will the electron’s momentum still be the right thing with which
to describe nature? Or even, should the whole question be put this way at all!
This question must always come up in any scientiﬁc investigation. At any rate,
we see a problem—how to ﬁnd a representation. We don’t know the answer. We
don’t even know whether we have the “right” problem, but if we do, we must
ﬁrst attempt to ﬁnd out whether any particular particle is “fundamental” or not.
In the nonrelativistic quantum mechanics—if the energies are not too high,
so that you don’t disturb the inner workings of the strange particles and so
forth—you can do a pretty good job without worrying about these details. You
can just decide to specify the momenta and spins of the electrons and of the nuclei;
then everything will be all right. In most chemical reactions and other low-energy
happenings, nothing goes on in the nuclei; they don’t get excited. Furthermore,
8-10","495"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","205","if a hydrogen atom is moving slowly and bumping quietly against other hydrogen
atoms—never getting excited inside, or radiating, or anything complicated like
that, but staying always in the ground state of energy for internal motion—you
can use an approximation in which you talk about the hydrogen atom as one
object, or particle, and not worry about the fact that it can do something inside.
This will be a good approximation as long as the kinetic energy in any collision
is well below 10 electron volts—the energy required to excite the hydrogen atom
to a diﬀerent internal state. We will often be making an approximation in which
we do not include the possibility of inner motion, thereby decreasing the number
of details that we have to put into our base states. Of course, we then omit
some phenomena which would appear (usually) at some higher energy, but by
making such approximations we can simplify very much the analysis of physical
problems. For example, we can discuss the collision of two hydrogen atoms at
low energy—or any chemical process—without worrying about the fact that the
atomic nuclei could be excited. To summarize, then, when we can neglect the
eﬀects of any internal excited states of a particle we can choose a base set which
are the states of deﬁnite momentum and z-component of angular momentum.
One problem then in describing nature is to ﬁnd a suitable representation
for the base states. But that’s only the beginning. We still want to be able to
say what “happens.” If we know the “condition” of the world at one moment, we
would like to know the condition at a later moment. So we also have to ﬁnd the
laws that determine how things change with time. We now address ourselves to
this second part of the framework of quantum mechanics—how states change
with time.
8-4 How states change with time
We have already talked about how we can represent a situation in which we
put something through an apparatus. Now one convenient, delightful “apparatus”
to consider is merely a wait of a few minutes; that is, you prepare a state φ,
and then before you analyze it, you just let it sit. Perhaps you let it sit in some
particular electric or magnetic ﬁeld—it depends on the physical circumstances
in the world. At any rate, whatever the conditions are, you let the object sit
from time t1 to time t2. Suppose that it is let out of your ﬁrst apparatus in the
condition φ at t1. And then it goes through an “apparatus,” but the “apparatus”
consists of just delay until t2. During the delay, various things could be going on—
external forces applied or other shenanigans—so that something is happening. At
8-11","459"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","206","the end of the delay, the amplitude to ﬁnd the thing in some state χ is no longer
exactly the same as it would have been without the delay. Since “waiting” is just
a special case of an “apparatus,” we can describe what happens by giving an
amplitude with the same form as Eq. (8.17). Because the operation of “waiting”
is especially important, we’ll call it U instead of A, and to specify the starting
and ﬁnishing times t1 and t2, we’ll write U(t2, t1). The amplitude we want is
⟨χ | U(t2, t1) | φ⟩.
(8.27)
Like any other such amplitude, it can be represented in some base system or
other by writing it
X
ij
⟨χ | i⟩⟨i | U(t2, t1) | j⟩⟨j | φ⟩.
(8.28)
Then U is completely described by giving the whole set of amplitudes—the matrix
⟨i | U(t2, t1) | j⟩.
(8.29)
We can point out, incidentally, that the matrix ⟨i | U(t2, t1) | j⟩gives much
more detail than may be needed. The high-class theoretical physicist working in
high-energy physics considers problems of the following general nature (because
it’s the way experiments are usually done). He starts with a couple of particles,
like a proton and a proton, coming together from inﬁnity. (In the lab, usually
one particle is standing still, and the other comes from an accelerator that is
practically at inﬁnity on atomic level.) The things go crash and out come, say,
two K-mesons, six π-mesons, and two neutrons in certain directions with certain
momenta. What’s the amplitude for this to happen? The mathematics looks
like this: The φ-state speciﬁes the spins and momenta of the incoming particles.
The χ would be the question about what comes out. For instance, with what
amplitude do you get the six mesons going in such-and-such directions, and the
two neutrons going oﬀin these directions, with their spins so-and-so. In other
words, χ would be speciﬁed by giving all the momenta, and spins, and so on of the
ﬁnal products. Then the job of the theorist is to calculate the amplitude (8.27).
However, he is really only interested in the special case that t1 is −∞and t2
is +∞. (There is no experimental evidence on the details of the process, only on
what comes in and what goes out.) The limiting case of U(t2, t1) as t1 →−∞
and t2 →+∞is called S, and what he wants is
⟨χ | S | φ⟩.
8-12","409"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","207","Or, using the form (8.28), he would calculate the matrix
⟨i | S | j⟩,
which is called the S-matrix. So if you see a theoretical physicist pacing the ﬂoor
and saying, “All I have to do is calculate the S-matrix,” you will know what he
is worried about.
How to analyze—how to specify the laws for—the S-matrix is an interesting
question. In relativistic quantum mechanics for high energies, it is done one way,
but in nonrelativistic quantum mechanics it can be done another way, which is
very convenient. (This other way can also be done in the relativistic case, but
then it is not so convenient.) It is to work out the U-matrix for a small interval
of time—in other words for t2 and t1 close together. If we can ﬁnd a sequence
of such U’s for successive intervals of time we can watch how things go as a
function of time. You can appreciate immediately that this way is not so good
for relativity, because you don’t want to have to specify how everything looks
“simultaneously” everywhere. But we won’t worry about that—we’re just going
to worry about nonrelativistic mechanics.
Suppose we think of the matrix U for a delay from t1 until t3 which is greater
than t2. In other words, let’s take three successive times: t1 less than t2 less
than t3. Then we claim that the matrix that goes between t1 and t3 is the product
in succession of what happens when you delay from t1 until t2 and then from t2
until t3. It’s just like the situation when we had two apparatuses B and A in
series. We can then write, following the notation of Section 5-6,
U(t3, t1) = U(t3, t2) · U(t2, t1).
(8.30)
In other words, we can analyze any time interval if we can analyze a sequence of
short time intervals in between. We just multiply together all the pieces; that’s
the way that quantum mechanics is analyzed nonrelativistically.
Our problem, then, is to understand the matrix U(t2, t1) for an inﬁnitesimal
time interval—for t2 = t1 + ∆t. We ask ourselves this: If we have a state φ now,
what does the state look like an inﬁnitesimal time ∆t later? Let’s see how we
write that out. Call the state at the time t, | ψ(t)⟩(we show the time dependence
of ψ to be perfectly clear that we mean the condition at the time t). Now we
ask the question: What is the condition after the small interval of time ∆t later?
The answer is
| ψ(t + ∆t)⟩= U(t + ∆t, t) | ψ(t)⟩.
(8.31)
8-13","439"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","208","This means the same as we meant by (8.25), namely, that the amplitude to ﬁnd
χ at the time t + ∆t, is
⟨χ | ψ(t + ∆t)⟩= ⟨χ | U(t + ∆t, t) | ψ(t)⟩.
(8.32)
Since we’re not yet too good at these abstract things, let’s project our am-
plitudes into a deﬁnite representation. If we multiply both sides of Eq. (8.31)
by ⟨i |, we get
⟨i | ψ(t + ∆t)⟩= ⟨i | U(t + ∆t, t) | ψ(t)⟩.
(8.33)
We can also resolve the | ψ(t)⟩into base states and write
⟨i | ψ(t + ∆t)⟩=
X
j
⟨i | U(t + ∆t, t) | j⟩⟨j | ψ(t)⟩.
(8.34)
We can understand Eq. (8.34) in the following way. If we let Ci(t) = ⟨i | ψ(t)⟩
stand for the amplitude to be in the base state i at the time t, then we can think
of this amplitude (just a number, remember!) varying with time. Each Ci becomes
a function of t. And we also have some information on how the amplitudes Ci
vary with time. Each amplitude at (t + ∆t) is proportional to all of the other
amplitudes at t multiplied by a set of coeﬃcients. Let’s call the U-matrix Uij,
by which we mean
Uij = ⟨i | U | j⟩.
Then we can write Eq. (8.34) as
Ci(t + ∆t) =
X
j
Uij(t + ∆t, t)Cj(t).
(8.35)
This, then, is how the dynamics of quantum mechanics is going to look.
We don’t know much about the Uij yet, except for one thing. We know that
if ∆t goes to zero, nothing can happen—we should get just the original state.
So, Uii →1 and Uij →0, if i ̸= j. In other words, Uij →δij for ∆t →0. Also,
we can suppose that for small ∆t, each of the coeﬃcients Uij should diﬀer from
δij by amounts proportional to ∆t; so we can write
Uij = δij + Kij ∆t.
(8.36)
8-14","329"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","209","However, it is usual to take the factor (−i/ℏ)† out of the coeﬃcients Kij, for
historical and other reasons; we prefer to write
Uij(t + ∆t, t) = δij −i
ℏHij(t) ∆t.
(8.37)
It is, of course, the same as Eq. (8.36) and, if you wish, just deﬁnes the coeﬃ-
cients Hij(t). The terms Hij are just the derivatives with respect to t2 of the
coeﬃcients Uij(t2, t1), evaluated at t2 = t1 = t.
Using this form for U in Eq. (8.35), we have
Ci(t + ∆t) =
X
j

δij −i
ℏHij(t) ∆t

Cj(t).
(8.38)
Taking the sum over the δij term, we get just Ci(t), which we can put on the
other side of the equation. Then dividing by ∆t, we have what we recognize as a
derivative
Ci(t + ∆t) −Ci(t)
∆t
= −i
ℏ
X
j
Hij(t)Cj(t)
or
iℏdCi(t)
dt
=
X
j
Hij(t)Cj(t).
(8.39)
You remember that Ci(t) is the amplitude ⟨i | ψ⟩to ﬁnd the state ψ in one
of the base states i (at the time t). So Eq. (8.39) tells us how each of the
coeﬃcients ⟨i | ψ⟩varies with time. But that is the same as saying that Eq. (8.39)
tells us how the state ψ varies with time, since we are describing ψ in terms
of the amplitudes ⟨i | ψ⟩. The variation of ψ in time is described in terms of
the matrix Hij, which has to include, of course, the things we are doing to the
system to cause it to change. If we know the Hij—which contains the physics
of the situation and can, in general, depend on the time—we have a complete
description of the behavior in time of the system. Equation (8.39) is then the
quantum mechanical law for the dynamics of the world.
(We should say that we will always take a set of base states which are ﬁxed
and do not vary with time. There are people who use base states that also vary.
† We are in a bit of trouble here with notation. In the factor (−i/ℏ), the i means the
imaginary unit √−1, and not the index i that refers to the ith base state! We hope that you
won’t ﬁnd it too confusing.
8-15","378"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","210","However, that’s like using a rotating coordinate system in mechanics, and we
don’t want to get involved in such complications.)
8-5 The Hamiltonian matrix
The idea, then, is that to describe the quantum mechanical world we need to
pick a set of base states i and to write the physical laws by giving the matrix of
coeﬃcients Hij. Then we have everything—we can answer any question about
what will happen. So we have to learn what the rules are for ﬁnding the H’s
to go with any physical situation—what corresponds to a magnetic ﬁeld, or an
electric ﬁeld, and so on. And that’s the hardest part. For instance, for the new
strange particles, we have no idea what Hij’s to use. In other words, no one
knows the complete Hij for the whole world. (Part of the diﬃculty is that one
can hardly hope to discover the Hij when no one even knows what the base states
are!) We do have excellent approximations for nonrelativistic phenomena and for
some other special cases. In particular, we have the forms that are needed for
the motions of electrons in atoms—to describe chemistry. But we don’t know
the full true H for the whole universe.
The coeﬃcients Hij are called the Hamiltonian matrix or, for short, just the
Hamiltonian. (How Hamilton, who worked in the 1830’s, got his name on a
quantum mechanical matrix is a tale of history.) It would be much better called
the energy matrix, for reasons that will become apparent as we work with it. So
the problem is: Know your Hamiltonian!
The Hamiltonian has one property that can be deduced right away, namely,
that
H∗
ij = Hji.
(8.40)
This follows from the condition that the total probability that the system is
in some state does not change. If you start with a particle—an object or the
world—then you’ve still got it as time goes on. The total probability of ﬁnding it
somewhere is
X
i
|Ci(t)|2,
which must not vary with time. If this is to be true for any starting condition φ,
then Eq. (8.40) must also be true.
As our ﬁrst example, we take a situation in which the physical circumstances
are not changing with time; we mean the external physical conditions, so that
8-16","380"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","211","H is independent of time. Nobody is turning magnets on and oﬀ. We also pick
a system for which only one base state is required for the description; it is an
approximation we could make for a hydrogen atom at rest, or something similar.
Equation (8.39) then says
iℏdC1
dt = H11C1.
(8.41)
Only one equation—that’s all! And if H11 is constant, this diﬀerential equation
is easily solved to give
C1 = (const)e−(i/ℏ)H11t.
(8.42)
This is the time dependence of a state with a deﬁnite energy E = H11. You see
why Hij ought to be called the energy matrix. It is the generalization of the
energy for more complex situations.
Next, to understand a little more about what the equations mean, we look at
a system which has two base states. Then Eq. (8.39) reads
iℏdC1
dt = H11C1 + H12C2,
iℏdC2
dt = H21C1 + H22C2.
(8.43)
If the H’s are again independent of time, you can easily solve these equations.
We leave you to try for fun, and we’ll come back and do them later. Yes, you
can solve the quantum mechanics without knowing the H’s, so long as they are
independent of time.
8-6 The ammonia molecule
We want now to show you how the dynamical equation of quantum mechanics
can be used to describe a particular physical circumstance. We have picked an
interesting but simple example in which, by making some reasonable guesses
about the Hamiltonian, we can work out some important—and even practical—
results. We are going to take a situation describable by two states: the ammonia
molecule.
The ammonia molecule has one nitrogen atom and three hydrogen atoms
located in a plane below the nitrogen so that the molecule has the form of a
pyramid, as drawn in Fig. 8-1(a). Now this molecule, like any other, has an
inﬁnite number of states. It can spin around any possible axis; it can be moving in
8-17","322"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","212","(a)
H
H
H
N
| 1⟩
(b)
H
H
H
N
| 2⟩
Fig. 8-1. Two equivalent geometric arrangements of the ammonia
molecule.
any direction; it can be vibrating inside, and so on, and so on. It is, therefore, not
a two-state system at all. But we want to make an approximation that all other
states remain ﬁxed, because they don’t enter into what we are concerned with at
the moment. We will consider only that the molecule is spinning around its axis
of symmetry (as shown in the ﬁgure), that it has zero translational momentum,
and that it is vibrating as little as possible. That speciﬁes all conditions except
one: there are still the two possible positions for the nitrogen atom—the nitrogen
may be on one side of the plane of hydrogen atoms or on the other, as shown in
Fig. 8-1(a) and (b). So we will discuss the molecule as though it were a two-state
system. We mean that there are only two states we are going to really worry
about, all other things being assumed to stay put. You see, even if we know that
it is spinning with a certain angular momentum around the axis and that it is
8-18","205"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","213","moving with a certain momentum and vibrating in a deﬁnite way, there are still
two possible states. We will say that the molecule is in the state | 1⟩when the
nitrogen is “up,” as in Fig. 8-1(a), and is in the state | 2⟩when the nitrogen is
“down,” as in (b). The states | 1⟩and | 2⟩will be taken as the set of base states
for our analysis of the behavior of the ammonia molecule. At any moment, the
actual state | ψ⟩of the molecule can be represented by giving C1 = ⟨1 | ψ⟩, the
amplitude to be in state | 1⟩, and C2 = ⟨2 | ψ⟩, the amplitude to be in state | 2⟩.
Then, using Eq. (8.8) we can write the state vector | ψ⟩as
| ψ⟩= | 1⟩⟨1 | ψ⟩+ | 2⟩⟨2 | ψ⟩
or
| ψ⟩= | 1⟩C1 + | 2⟩C2.
(8.44)
Now the interesting thing is that if the molecule is known to be in some state
at some instant, it will not be in the same state a little while later. The two C-
coeﬃcients will be changing with time according to the equations (8.43)—which
hold for any two-state system. Suppose, for example, that you had made some
observation—or had made some selection of the molecules—so that you know
that the molecule is initially in the state | 1⟩. At some later time, there is some
chance that it will be found in state | 2⟩. To ﬁnd out what this chance is, we
have to solve the diﬀerential equation which tells us how the amplitudes change
with time.
The only trouble is that we don’t know what to use for the coeﬃcients Hij in
Eq. (8.43). There are some things we can say, however. Suppose that once the
molecule was in the state | 1⟩there was no chance that it could ever get into | 2⟩,
and vice versa. Then H12 and H21 would both be zero, and Eq. (8.43) would
read
iℏdC1
dt = H11C1,
iℏdC2
dt = H22C2.
We can easily solve these two equations; we get
C1 = (const)e−(i/ℏ)H11t,
C2 = (const)e−(i/ℏ)H22t.
(8.45)
These are just the amplitudes for stationary states with the energies E1 = H11
and E2 = H22. We note, however, that for the ammonia molecule the two states
| 1⟩and | 2⟩have a deﬁnite symmetry. If nature is at all reasonable, the matrix
elements H11 and H22 must be equal. We’ll call them both E0, because they
8-19","414"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","214","correspond to the energy the states would have if H12 and H21 were zero. But
Eqs. (8.45) do not tell us what ammonia really does. It turns out that it is
possible for the nitrogen to push its way through the three hydrogens and ﬂip
to the other side. It is quite diﬃcult; to get half-way through requires a lot of
energy. How can it get through if it hasn’t got enough energy? There is some
amplitude that it will penetrate the energy barrier. It is possible in quantum
mechanics to sneak quickly across a region which is illegal energetically. There
is, therefore, some small amplitude that a molecule which starts in | 1⟩will get
to the state | 2⟩. The coeﬃcients H12 and H21 are not really zero. Again, by
symmetry, they should both be the same—at least in magnitude. In fact, we
already know that, in general, Hij must be equal to the complex conjugate of Hji,
so they can diﬀer only by a phase. It turns out, as you will see, that there is no
loss of generality if we take them equal to each other. For later convenience we
set them equal to a negative number; we take H12 = H21 = −A. We then have
the following pair of equations:
iℏdC1
dt = E0C1 −AC2,
(8.46)
iℏdC2
dt = E0C2 −AC1.
(8.47)
These equations are simple enough and can be solved in any number of ways.
One convenient way is the following. Taking the sum of the two, we get
iℏd
dt (C1 + C2) = (E0 −A)(C1 + C2),
whose solution is
C1 + C2 = ae−(i/ℏ)(E0−A)t.
(8.48)
Then, taking the diﬀerence of (8.46) and (8.47), we ﬁnd that
iℏd
dt (C1 −C2) = (E0 + A)(C1 −C2),
which gives
C1 −C2 = be−(i/ℏ)(E0+A)t.
(8.49)
We have called the two integration constants a and b; they are, of course, to
be chosen to give the appropriate starting condition for any particular physical
8-20","329"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","215","problem. Now, by adding and subtracting (8.48) and (8.49), we get C1 and C2:
C1(t) = a
2 e−(i/ℏ)(E0−A)t + b
2 e−(i/ℏ)(E0+A)t,
(8.50)
C2(t) = a
2 e−(i/ℏ)(E0−A)t −b
2 e−(i/ℏ)(E0+A)t.
(8.51)
They are the same except for the sign of the second term.
We have the solutions; now what do they mean? (The trouble with quan-
tum mechanics is not only in solving the equations but in understanding what
the solutions mean!) First, notice that if b = 0, both terms have the same fre-
quency ω = (E0 −A)/ℏ. If everything changes at one frequency, it means that
the system is in a state of deﬁnite energy—here, the energy (E0 −A). So there is
a stationary state of this energy in which the two amplitudes C1 and C2 are equal.
We get the result that the ammonia molecule has a deﬁnite energy (E0 −A) if
there are equal amplitudes for the nitrogen atom to be “up” and to be “down.”
There is another stationary state possible if a = 0; both amplitudes then
have the frequency (E0 + A)/ℏ. So there is another state with the deﬁnite
energy (E0 + A) if the two amplitudes are equal but with the opposite sign;
C2 = −C1. These are the only two states of deﬁnite energy. We will discuss
the states of the ammonia molecule in more detail in the next chapter; we will
mention here only a couple of things.
We conclude that because there is some chance that the nitrogen atom can ﬂip
from one position to the other, the energy of the molecule is not just E0, as we
would have expected, but that there are two energy levels (E0 + A) and (E0 −A).
Every one of the possible states of the molecule, whatever energy it has, is “split”
into two levels. We say every one of the states because, you remember, we picked
out one particular state of rotation, and internal energy, and so on. For each
possible condition of that kind there is a doublet of energy levels because of the
ﬂip-ﬂop of the molecule.
Let’s now ask the following question about an ammonia molecule. Suppose
that at t = 0, we know that a molecule is in the state | 1⟩or, in other words,
that C1(0) = 1 and C2(0) = 0. What is the probability that the molecule will
be found in the state | 2⟩at the time t, or will still be found in state | 1⟩at the
time t? Our starting condition tells us what a and b are in Eqs. (8.50) and (8.51).
Letting t = 0, we have that
C1(0) = a + b
2
= 1,
C2(0) = a −b
2
= 0.
8-21","458"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","216","Clearly, a = b = 1. Putting these values into the formulas for C1(t) and C2(t)
and rearranging some terms, we have
C1(t) = e−(i/ℏ)E0t
e(i/ℏ)At + e−(i/ℏ)At
2

,
C2(t) = e−(i/ℏ)E0t
e(i/ℏ)At −e−(i/ℏ)At
2

.
We can rewrite these as
C1(t) = e−(i/ℏ)E0t cos At
ℏ,
(8.52)
C2(t) = ie−(i/ℏ)E0t sin At
ℏ.
(8.53)
The two amplitudes have a magnitude that varies harmonically with time.
The probability that the molecule is found in state | 2⟩at the time t is the
absolute square of C2(t):
|C2(t)|2 = sin2 At
ℏ.
(8.54)
The probability starts at zero (as it should), rises to one, and then oscillates back
and forth between zero and one, as shown in the curve marked P2 of Fig. 8-2.
The probability of being in the | 1⟩state does not, of course, stay at one. It
“dumps” into the second state until the probability of ﬁnding the molecule in the
ﬁrst state is zero, as shown by the curve P1 of Fig. 8-2. The probability sloshes
back and forth between the two.
A long time ago we saw what happens when we have two equal pendulums
with a slight coupling. (See Chapter 49, Vol. I.) When we lift one back and let
go, it swings, but then gradually the other one starts to swing. Pretty soon
the second pendulum has picked up all the energy. Then, the process reverses,
and pendulum number one picks up the energy. It is exactly the same kind of
a thing. The speed at which the energy is swapped back and forth depends on
the coupling between the two pendulums—the rate at which the “oscillation” is
able to leak across. Also, you remember, with the two pendulums there are two
special motions—each with a deﬁnite frequency—which we call the fundamental
modes. If we pull both pendulums out together, they swing together at one
frequency. On the other hand, if we pull one out one way and the other out the
other way, there is another stationary mode also at a deﬁnite frequency.
8-22","344"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","217","t

units of ¯h
A

π
4
π
2
3π
4
π
5π
4
P
0
0.5
1.0
P1
P2
Fig. 8-2. The probability P1 that an ammonia molecule in state | 1⟩
at t = 0 will be found in state | 1⟩at t. The probability P2 that it will
be found in state | 2⟩.
Well, here we have a similar situation—the ammonia molecule is mathemat-
ically like the pair of pendulums. These are the two frequencies—(E0 −A)/ℏ
and (E0 + A)/ℏ—for when they are oscillating together, or oscillating opposite.
The pendulum analogy is not much deeper than the principle that the same
equations have the same solutions. The linear equations for the amplitudes (8.39)
are very much like the linear equations of harmonic oscillators. (In fact, this is
the reason behind the success of our classical theory of the index of refraction, in
which we replaced the quantum mechanical atom by a harmonic oscillator, even
though, classically, this is not a reasonable view of electrons circulating about
a nucleus.) If you pull the nitrogen to one side, then you get a superposition of
these two frequencies, and you get a kind of beat note, because the system is not
in one or the other states of deﬁnite frequency. The splitting of the energy levels
of the ammonia molecule is, however, strictly a quantum mechanical eﬀect.
The splitting of the energy levels of the ammonia molecule has important
practical applications which we will describe in the next chapter. At long last we
have an example of a practical physical problem that you can understand with
the quantum mechanics!
8-23","273"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","218","9
The Ammonia Maser
MASER = Microwave Ampliﬁcation by Stimulated Emission of Radiation
9-1 The states of an ammonia molecule
In this chapter we are going to discuss the application of quantum mechanics
to a practical device, the ammonia maser. You may wonder why we stop our
formal development of quantum mechanics to do a special problem, but you
will ﬁnd that many of the features of this special problem are quite common
in the general theory of quantum mechanics, and you will learn a great deal
by considering this one problem in detail. The ammonia maser is a device for
generating electromagnetic waves, whose operation is based on the properties of
the ammonia molecule which we discussed brieﬂy in the last chapter. We begin
by summarizing what we found there.
The ammonia molecule has many states, but we are considering it as a two-
state system, thinking now only about what happens when the molecule is in any
speciﬁc state of rotation or translation. A physical model for the two states can be
visualized as follows. If the ammonia molecule is considered to be rotating about
an axis passing through the nitrogen atom and perpendicular to the plane of the
hydrogen atoms, as shown in Fig. 9-1, there are still two possible conditions—the
nitrogen may be on one side of the plane of hydrogen atoms or on the other. We
call these two states | 1⟩and | 2⟩. They are taken as a set of base states for our
analysis of the behavior of the ammonia molecule.
In a system with two base states, any state | ψ⟩of the system can always
be described as a linear combination of the two base states; that is, there is a
certain amplitude C1 to be in one base state and an amplitude C2 to be in the
other. We can write its state vector as
| ψ⟩= | 1⟩C1 + | 2⟩C2,
(9.1)
9-1","323"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","219","| 1⟩
H
H
H
N
E
µ
| 2⟩
H
H
H
N
µ
Center
of
Mass
Dipole
Moment
Fig. 9-1.
A physical model of two base states for the ammonia
molecule. These states have the electric dipole moments µ.
where
C1 = ⟨1 | ψ⟩
and
C2 = ⟨2 | ψ⟩.
These two amplitudes change with time according to the Hamiltonian equa-
tions, Eq. (8.43). Making use of the symmetry of the two states of the ammonia
molecule, we set H11 = H22 = E0, and H12 = H21 = −A, and get the solution
[see Eqs. (8.50) and (8.51)]
C1 = a
2 e−(i/ℏ)(E0−A)t + b
2 e−(i/ℏ)(E0+A)t,
(9.2)
C2 = a
2 e−(i/ℏ)(E0−A)t −b
2 e−(i/ℏ)(E0+A)t.
(9.3)
We want now to take a closer look at these general solutions. Suppose that
the molecule was initially put into a state | ψII⟩for which the coeﬃcient b was
equal to zero. Then at t = 0 the amplitudes to be in the states | 1⟩and | 2⟩are
identical, and they stay that way for all time. Their phases both vary with time
in the same way—with the frequency (E0 −A)/ℏ. Similarly, if we were to put
the molecule into a state | ψI⟩for which a = 0, the amplitude C2 is the negative
of C1, and this relationship would stay that way forever. Both amplitudes would
9-2","229"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","220","now vary with time with the frequency (E0 + A)/ℏ. These are the only two
possibilities of states for which the relation between C1 and C2 is independent of
time.
We have found two special solutions in which the two amplitudes do not vary
in magnitude and, furthermore, have phases which vary at the same frequencies.
These are stationary states as we deﬁned them in Section 7-1, which means that
they are states of deﬁnite energy. The state | ψII⟩has the energy EII = E0 −A,
and the state | ψI⟩has the energy EI = E0 +A. They are the only two stationary
states that exist, so we ﬁnd that the molecule has two energy levels, with the
energy diﬀerence 2A. (We mean, of course, two energy levels for the assumed
state of rotation and vibration which we referred to in our initial assumptions.)†
If we hadn’t allowed for the possibility of the nitrogen ﬂipping back and forth,
we would have taken A equal to zero and the two energy levels would be on top
of each other at energy E0. The actual levels are not this way; their average
energy is E0, but they are split apart by ±A, giving a separation of 2A between
the energies of the two states. Since A is, in fact, very small, the diﬀerence in
energy is also very small.
In order to excite an electron inside an atom, the energies involved are
relatively very high—requiring photons in the optical or ultraviolet range. To
excite the vibrations of the molecules involves photons in the infrared. If you
talk about exciting rotations, the energy diﬀerences of the states correspond
to photons in the far infrared. But the energy diﬀerence 2A is lower than any
of those and is, in fact, below the infrared and well into the microwave region.
Experimentally, it has been found that there is a pair of energy levels with a
separation of 10−4 electron volt—corresponding to a frequency 24,000 megacycles.
Evidently this means that 2A = hf, with f = 24,000 megacycles (corresponding
to a wavelength of 1 1
4 cm). So here we have a molecule that has a transition
which does not emit light in the ordinary sense, but emits microwaves.
For the work that follows we need to describe these two states of deﬁnite
energy a little bit better. Suppose we were to construct an amplitude CII by
taking the sum of the two numbers C1 and C2:
CII = C1 + C2 = ⟨1 | Φ⟩+ ⟨2 | Φ⟩.
(9.4)
† In what follows it is helpful—in reading to yourself or in talking to someone else—to have
a handy way of distinguishing between the Arabic 1 and 2 and the Roman I and II. We ﬁnd it
convenient to reserve the names “one” and “two” for the Arabic numbers, and to call I and II
by the names “eins” and “zwei” (although “unus” and “duo” might be more logical!).
9-3","494"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","221","What would that mean? Well, this is just the amplitude to ﬁnd the state | Φ⟩in a
new state | II⟩in which the amplitudes of the original base states are equal. That
is, writing CII = ⟨II | Φ⟩, we can abstract the | Φ⟩away from Eq. (9.4)—because
it is true for any Φ—and get
⟨II | = ⟨1 | + ⟨2 |,
which means the same as
| II⟩= | 1⟩+ | 2⟩.
(9.5)
The amplitude for the state | II⟩to be in the state | 1⟩is
⟨1 | II⟩= ⟨1 | 1⟩+ ⟨1 | 2⟩,
which is, of course, just 1, since | 1⟩and | 2⟩are base states. The amplitude for
the state | II⟩to be in the state | 2⟩is also 1, so the state | II⟩is one which has
equal amplitudes to be in the two base states | 1⟩and | 2⟩.
We are, however, in a bit of trouble. The state | II⟩has a total probability
greater than one of being in some base state or other. That simply means,
however, that the state vector is not properly “normalized.” We can take care of
that by remembering that we should have ⟨II | II⟩= 1, which must be so for any
state. Using the general relation that
⟨χ | Φ⟩=
X
i
⟨χ | i⟩⟨i | Φ⟩,
letting both Φ and χ be the state II, and taking the sum over the base states
| 1⟩and | 2⟩, we get that
⟨II | II⟩= ⟨II | 1⟩⟨1 | II⟩+ ⟨II | 2⟩⟨2 | II⟩.
This will be equal to one as it should if we change our deﬁnition of CII—in
Eq. (9.4)—to read
CII =
1
√
2 [C1 + C2].
In the same way we can construct an amplitude
CI =
1
√
2 [C1 −C2],
9-4","303"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","222","or
CI =
1
√
2 [⟨1 | Φ⟩−⟨2 | Φ⟩].
(9.6)
This amplitude is the projection of the state | Φ⟩into a new state | I⟩which has
opposite amplitudes to be in the states | 1⟩and | 2⟩. Namely, Eq. (9.6) means
the same as
⟨I | =
1
√
2 [⟨1 | −⟨2 |],
or
| I⟩=
1
√
2 [| 1⟩−| 2⟩],
(9.7)
from which it follows that
⟨1 | I⟩=
1
√
2 = −⟨2 | I⟩.
Now the reason we have done all this is that the states | I⟩and | II⟩can be
taken as a new set of base states which are especially convenient for describing the
stationary states of the ammonia molecule. You remember that the requirement
for a set of base states is that
⟨i | j⟩= δij.
We have already ﬁxed things so that
⟨I | I⟩= ⟨II | II⟩= 1.
You can easily show from Eqs. (9.5) and (9.7) that
⟨I | II⟩= ⟨II | I⟩= 0.
The amplitudes CI = ⟨I | Φ⟩and CII = ⟨II | Φ⟩for any state Φ to be in our
new base states | I⟩and | II⟩must also satisfy a Hamiltonian equation with the
form of Eq. (8.39). In fact, if we just subtract the two equations (9.2) and (9.3)
and diﬀerentiate with respect to t, we see that
iℏdCI
dt = (E0 + A)CI = EICI.
(9.8)
And taking the sum of Eqs. (9.2) and (9.3), we see that
iℏdCII
dt
= (E0 −A)CII = EIICII.
(9.9)
9-5","255"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","223","Using | I⟩and | II⟩for base states, the Hamiltonian matrix has the simple form
HI,I = EI,
HI,II = 0,
HII,I = 0,
HII,II = EII.
Note that each of the Eqs. (9.8) and (9.9) look just like what we had in Section 8-6
for the equation of a one-state system. They have a simple exponential time
dependence corresponding to a single energy. As time goes on, the amplitudes to
be in each state act independently.
The two stationary states | ψI⟩and | ψII⟩we found above are, of course,
solutions of Eqs. (9.8) and (9.9). The state | ψI⟩(for which C1 = −C2) has
CI = e−(i/ℏ)(E0+A)t,
CII = 0.
(9.10)
And the state | ψII⟩(for which C1 = C2) has
CI = 0,
CII = e−(i/ℏ)(E0−A)t.
(9.11)
Remember that the amplitudes in Eq. (9.10) are
CI = ⟨I | ψI⟩,
and
CII = ⟨II | ψI⟩;
so Eq. (9.10) means the same thing as
| ψI⟩= | I⟩e−(i/ℏ)(E0+A)t.
That is, the state vector of the stationary state | ψI⟩is the same as the state
vector of the base state | I⟩except for the exponential factor appropriate to the
energy of the state. In fact at t = 0
| ψI⟩= | I⟩;
the state | I⟩has the same physical conﬁguration as the stationary state of
energy E0 + A. In the same way, we have for the second stationary state that
| ψII⟩= | II⟩e−(i/ℏ)(E0−A)t.
The state | II⟩is just the stationary state of energy E0 −A at t = 0. Thus
our two new base states | I⟩and | II⟩have physically the form of the states of
9-6","270"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","224","deﬁnite energy, with the exponential time factor taken out so that they can be
time-independent base states. (In what follows we will ﬁnd it convenient not
to have to distinguish always between the stationary states | ψI⟩and | ψII⟩and
their base states | I⟩and | II⟩, since they diﬀer only by the obvious time factors.)
In summary, the state vectors | I⟩and | II⟩are a pair of base vectors which
are appropriate for describing the deﬁnite energy states of the ammonia molecule.
They are related to our original base vectors by
| I⟩=
1
√
2 [| 1⟩−| 2⟩],
| II⟩=
1
√
2 [| 1⟩+ | 2⟩].
(9.12)
The amplitudes to be in | I⟩and | II⟩are related to C1 and C2 by
CI =
1
√
2 [C1 −C2],
CII =
1
√
2 [C1 + C2].
(9.13)
Any state at all can be represented by a linear combination of | 1⟩and | 2⟩—with
the coeﬃcients C1 and C2—or by a linear combination of the deﬁnite energy base
states | I⟩and | II⟩—with the coeﬃcients CI and CII. Thus,
| Φ⟩= | 1⟩C1 + | 2⟩C2
or
| Φ⟩= | I⟩CI + | II⟩CII.
The second form gives us the amplitudes for ﬁnding the state | Φ⟩in a state with
the energy EI = E0 + A or in a state with the energy EII = E0 −A.
9-2 The molecule in a static electric ﬁeld
If the ammonia molecule is in either of the two states of deﬁnite energy and
we disturb it at a frequency ω such that ℏω = EI −EII = 2A, the system may
make a transition from one state to the other. Or, if it is in the upper state,
it may change to the lower state and emit a photon. But in order to induce
such transitions you must have a physical connection to the states—some way of
disturbing the system. There must be some external machinery for aﬀecting the
states, such as magnetic or electric ﬁelds. In this particular case, these states are
sensitive to an electric ﬁeld. We will, therefore, look next at the problem of the
behavior of the ammonia molecule in an external electric ﬁeld.
9-7","369"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","225","To discuss the behavior in an electric ﬁeld, we will go back to the original
base system | 1⟩and | 2⟩, rather than using | I⟩and | II⟩. Suppose that there is
an electric ﬁeld in a direction perpendicular to the plane of the hydrogen atoms.
Disregarding for the moment the possibility of ﬂipping back and forth, would
it be true that the energy of this molecule is the same for the two positions
of the nitrogen atom? Generally, no. The electrons tend to lie closer to the
nitrogen than to the hydrogen nuclei, so the hydrogens are slightly positive. The
actual amount depends on the details of electron distribution. It is a complicated
problem to ﬁgure out exactly what this distribution is, but in any case the net
result is that the ammonia molecule has an electric dipole moment, as indicated
in Fig. 9-1. We can continue our analysis without knowing in detail the direction
or amount of displacement of the charge. However, to be consistent with the
notation of others, let’s suppose that the electric dipole moment is µ, with its
direction pointing from the nitrogen atom and perpendicular to the plane of the
hydrogen atoms.
Now, when the nitrogen ﬂips from one side to the other, the center of mass will
not move, but the electric dipole moment will ﬂip over. As a result of this moment,
the energy in an electric ﬁeld E will depend on the molecular orientation.† With
the assumption made above, the potential energy will be higher if the nitrogen
atom points in the direction of the ﬁeld, and lower if it is in the opposite direction;
the separation in the two energies will be 2µE.
In the discussion up to this point, we have assumed values of E0 and A
without knowing how to calculate them. According to the correct physical theory,
it should be possible to calculate these constants in terms of the positions and
motions of all the nuclei and electrons. But nobody has ever done it. Such a
system involves ten electrons and four nuclei and that’s just too complicated a
problem. As a matter of fact, there is no one who knows much more about this
molecule than we do. All anyone can say is that when there is an electric ﬁeld,
the energy of the two states is diﬀerent, the diﬀerence being proportional to the
electric ﬁeld. We have called the coeﬃcient of proportionality 2µ, but its value
must be determined experimentally. We can also say that the molecule has the
amplitude A to ﬂip over, but this will have to be measured experimentally. Nobody
can give us accurate theoretical values of µ and A, because the calculations are
too complicated to do in detail.
† We are sorry that we have to introduce a new notation. Since we have been using p and E
for momentum and energy, we don’t want to use them again for dipole moment and electric
ﬁeld. Remember, in this section µ is the electric dipole moment.
9-8","506"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","226","For the ammonia molecule in an electric ﬁeld, our description must be changed.
If we ignored the amplitude for the molecule to ﬂip from one conﬁguration to the
other, we would expect the energies of the two states | 1⟩and | 2⟩to be (E0 ±µE).
Following the procedure of the last chapter, we take
H11 = E0 + µE,
H22 = E0 −µE.
(9.14)
Also we will assume that for the electric ﬁelds of interest the ﬁeld does not aﬀect
appreciably the geometry of the molecule and, therefore, does not aﬀect the
amplitude that the nitrogen will jump from one position to the other. We can
then take that H12 and H21 are not changed; so
H12 = H21 = −A.
(9.15)
We must now solve the Hamiltonian equations, Eq. (8.43), with these new values
of Hij. We could solve them just as we did before, but since we are going to
have several occasions to want the solutions for two-state systems, let’s solve the
equations once and for all in the general case of arbitrary Hij—assuming only
that they do not change with time.
We want the general solution of the pair of Hamiltonian equations
iℏdC1
dt = H11C1 + H12C2,
(9.16)
iℏdC2
dt = H21C1 + H22C2.
(9.17)
Since these are linear diﬀerential equations with constant coeﬃcients, we can
always ﬁnd solutions which are exponential functions of the dependent variable t.
We will ﬁrst look for a solution in which C1 and C2 both have the same time
dependence; we can use the trial functions
C1 = a1e−iωt,
C2 = a2e−iωt.
Since such a solution corresponds to a state of energy E = ℏω, we may as well
write right away
C1 = a1e−(i/ℏ)Et,
(9.18)
C2 = a2e−(i/ℏ)Et,
(9.19)
9-9","292"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","227","where E is as yet unknown and to be determined so that the diﬀerential equations
(9.16) and (9.17) are satisﬁed.
When we substitute C1 and C2 from (9.18) and (9.19) in the diﬀerential
equations (9.16) and (9.17), the derivatives give us just −iE/ℏtimes C1 or C2,
so the left sides become just EC1 and EC2. Cancelling the common exponential
factors, we get
Ea1 = H11a1 + H12a2,
Ea2 = H21a1 + H22a2.
Or, rearranging the terms, we have
(E −H11)a1 −H12a2 = 0,
(9.20)
−H21a1 + (E −H22)a2 = 0.
(9.21)
With such a set of homogeneous algebraic equations, there will be nonzero
solutions for a1 and a2 only if the determinant of the coeﬃcients of a1 and a2 is
zero, that is, if
Det
 
E −H11
−H12
−H21
E −H22
!
= 0.
(9.22)
However, when there are only two equations and two unknowns, we don’t
need such a sophisticated idea. The two equations (9.20) and (9.21) each give
a ratio for the two coeﬃcients a1 and a2, and these two ratios must be equal.
From (9.20) we have that
a1
a2
=
H12
E −H11
,
(9.23)
and from (9.21) that
a1
a2
= E −H22
H21
.
(9.24)
Equating these two ratios, we get that E must satisfy
(E −H11)(E −H22) −H12H21 = 0.
This is the same result we would get by solving Eq. (9.22). Either way, we have
a quadratic equation for E which has two solutions:
E = H11 + H22
2
±
r
(H11 −H22)2
4
+ H12H21.
(9.25)
9-10","257"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","228","There are two possible values for the energy E. Note that both solutions give
real numbers for the energy, because H11 and H22 are real, and H12H21 is equal
to H12H∗
12 = |H12|2, which is both real and positive.
Using the same convention we took before, we will call the upper energy EI
and the lower energy EII. We have
EI = H11 + H22
2
+
r
(H11 −H22)2
4
+ H12H21,
(9.26)
EII = H11 + H22
2
−
r
(H11 −H22)2
4
+ H12H21.
(9.27)
Using each of these two energies separately in Eqs. (9.18) and (9.19), we have
the amplitudes for the two stationary states (the states of deﬁnite energy). If
there are no external disturbances, a system initially in one of these states will
stay that way forever—only its phase changes.
We can check our results for two special cases. If H12 = H21 = 0, we have
that EI = H11 and EII = H22. This is certainly correct, because then Eqs. (9.16)
and (9.17) are uncoupled, and each represents a state of energy H11 and H22.
Next, if we set H11 = H22 = E0 and H21 = H12 = −A, we get the solution we
found before:
EI = E0 + A
and
EII = E0 −A.
For the general case, the two solutions EI and EII refer to two states—which
we can again call the states
| ψI⟩= | I⟩e−(i/ℏ)EIt
and
| ψII⟩= | II⟩e−(i/ℏ)EIIt.
These states will have C1 and C2 as given in Eqs. (9.18) and (9.19), where a1
and a2 are still to be determined. Their ratio is given by either Eq. (9.23) or
Eq. (9.24). They must also satisfy one more condition. If the system is known
to be in one of the stationary states, the sum of the probabilities that it will be
found in | 1⟩or | 2⟩must equal one. We must have that
|C1|2 + |C2|2 = 1,
(9.28)
or, equivalently,
|a1|2 + |a2|2 = 1.
(9.29)
9-11","335"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","229","These conditions do not uniquely specify a1 and a2; they are still undetermined
by an arbitrary phase—in other words, by a factor like eiδ. Although general
solutions for the a’s can be written down,† it is usually more convenient to work
them out for each special case.
Let’s go back now to our particular example of the ammonia molecule in an
electric ﬁeld. Using the values for H11, H22, and H12 given in (9.14) and (9.15),
we get for the energies of the two stationary states
EI = E0 +
p
A2 + µ2E2,
EII = E0 −
p
A2 + µ2E2.
(9.30)
These two energies are plotted as a function of the electric ﬁeld strength E in
µE/A
1
2
3
4
E
E0 + A
E0
E0 −A
E0 + µE
E0 −µE
E0 +
p
A2 + µ2E2
E0 −
p
A2 + µ2E2
Fig. 9-2. Energy levels of the ammonia molecule in an electric ﬁeld.
† For example, the following set is one acceptable solution, as you can easily verify:
a1 =
H12
[(E −H11)2 + H12H21]1/2 ,
a2 =
E −H11
[(E −H11)2 + H12H21]1/2 .
9-12","192"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","230","Fig. 9-2. When the electric ﬁeld is zero, the two energies are, of course, just E0±A.
When an electric ﬁeld is applied, the splitting between the two levels increases.
The splitting increases at ﬁrst slowly with E, but eventually becomes proportional
to E. (The curve is a hyperbola.) For enormously strong ﬁelds, the energies are
just
EI = E0 + µE = H11,
EII = E0 −µE = H22.
(9.31)
The fact that there is an amplitude for the nitrogen to ﬂip back and forth has little
eﬀect when the two positions have very diﬀerent energies. This is an interesting
point which we will come back to again later.
We are at last ready to understand the operation of the ammonia maser. The
idea is the following. First, we ﬁnd a way of separating molecules in the state | I⟩
from those in the state | II⟩.† Then the molecules in the higher energy state | I⟩
are passed through a cavity which has a resonant frequency of 24,000 megacycles.
The molecules can deliver energy to the cavity—in a way we will discuss later—
and leave the cavity in the state | II⟩. Each molecule that makes such a transition
will deliver the energy E = EI −EII to the cavity. The energy from the molecules
will appear as electrical energy in the cavity.
How can we separate the two molecular states? One method is as follows.
The ammonia gas is let out of a little jet and passed through a pair of slits to give
a narrow beam, as shown in Fig. 9-3. The beam is then sent through a region in
which there is a large transverse electric ﬁeld. The electrodes to produce the ﬁeld
are shaped so that the electric ﬁeld varies rapidly across the beam. Then the
NH3
SLITS
INCREASING E2
I
II
Fig. 9-3. The ammonia beam may be separated by an electric ﬁeld in
which E2 has a gradient perpendicular to the beam.
† From now on we will write | I⟩and | II⟩instead of | ψI⟩and | ψII⟩. You must remember
that the actual states | ψI⟩and | ψII⟩are the energy base states multiplied by the appropriate
exponential factor.
9-13","367"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","231","square of the electric ﬁeld E · E will have a large gradient perpendicular to the
beam. Now a molecule in state | I⟩has an energy which increases with E2, and
therefore this part of the beam will be deﬂected toward the region of lower E2.
A molecule in state | II⟩will, on the other hand, be deﬂected toward the region
of larger E2, since its energy decreases as E2 increases.
Incidentally, with the electric ﬁelds which can be generated in the laboratory,
the energy µE is always much smaller than A. In such cases, the square root in
Eqs. (9.30) can be approximated by
A

1 + 1
2
µ2E2
A2

.
(9.32)
So the energy levels are, for all practical purposes,
EI = E0 + A + µ2E2
2A
(9.33)
and
EII = E0 −A −µ2E2
2A .
(9.34)
And the energies vary approximately linearly with E2. The force on the molecules
is then
F = µ2
2A ∇E2.
(9.35)
Many molecules have an energy in an electric ﬁeld which is proportional to E2.
The coeﬃcient is the polarizability of the molecule. Ammonia has an unusually
high polarizability because of the small value of A in the denominator. Thus,
ammonia molecules are unusually sensitive to an electric ﬁeld. (What would you
expect for the dielectric coeﬃcient of NH3 gas?)
9-3 Transitions in a time-dependent ﬁeld
In the ammonia maser, the beam with molecules in the state | I⟩and with
the energy EI is sent through a resonant cavity, as shown in Fig. 9-4. The other
beam is discarded. Inside the cavity, there will be a time-varying electric ﬁeld, so
the next problem we must discuss is the behavior of a molecule in an electric ﬁeld
that varies with time. We have a completely diﬀerent kind of a problem—one
9-14","303"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","232","v
∇E2
I
II
MASER CAVITY
FREQUENCY ω0
vT
electric ﬁeld E
ALL
II
Fig. 9-4. Schematic diagram of the ammonia maser.
with a time-varying Hamiltonian. Since Hij depends upon E, the Hij vary with
time, and we must determine the behavior of the system in this circumstance.
To begin with, we write down the equations to be solved:
iℏdC1
dt = (E0 + µE)C1 −AC2,
iℏdC2
dt = −AC1 + (E0 −µE)C2.
(9.36)
To be deﬁnite, let’s suppose that the electric ﬁeld varies sinusoidally; then we
can write
E = 2E0 cos ωt = E0(eiωt + e−iωt).
(9.37)
In actual operation the frequency ω will be very nearly equal to the resonant
frequency of the molecular transition ω0 = 2A/ℏ, but for the time being we want
to keep things general, so we’ll let it have any value at all. The best way to solve
our equations is to form linear combinations of C1 and C2 as we did before. So
we add the two equations, divide by the square root of 2, and use the deﬁnitions
of CI and CII that we had in Eq. (9.13). We get
iℏdCII
dt
= (E0 −A)CII + µECI.
(9.38)
You’ll note that this is the same as Eq. (9.9) with an extra term due to the
9-15","216"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","233","electric ﬁeld. Similarly, if we subtract the two equations (9.36), we get
iℏdCI
dt = (E0 + A)CI + µECII.
(9.39)
Now the question is, how to solve these equations? They are more diﬃcult
than our earlier set, because E depends on t; and, in fact, for a general E(t) the
solution is not expressible in elementary functions. However, we can get a good
approximation so long as the electric ﬁeld is small. First we will write
CI = γIe−i(E0+A)t/ℏ= γIe−i(EI)t/ℏ,
CII = γIIe−i(E0−A)t/ℏ= γIIe−i(EII)t/ℏ.
(9.40)
If there were no electric ﬁeld, these solutions would be correct with γI and γII
just chosen as two complex constants. In fact, since the probability of being in
state | I⟩is the absolute square of CI and the probability of being in state | II⟩is
the absolute square of CII, the probability of being in state | I⟩or in state | II⟩is
just |γI|2 or |γII|2. For instance, if the system were to start originally in state | II⟩
so that γI was zero and |γII|2 was one, this condition would go on forever. There
would be no chance, if the molecule were originally in state | II⟩, ever to get into
state | I⟩.
Now the idea of writing our equations in the form of Eq. (9.40) is that if
µE is small in comparison with A, the solutions can still be written in this way,
but then γI and γII become slowly varying functions of time—where by “slowly
varying” we mean slowly in comparison with the exponential functions. That
is the trick. We use the fact that γI and γII vary slowly to get an approximate
solution.
We want now to substitute CI from Eq. (9.40) in the diﬀerential equa-
tion (9.39), but we must remember that γI is also a function of t. We have
iℏdCI
dt = EIγIe−iEIt/ℏ+ iℏdγI
dt e−iEIt/ℏ.
The diﬀerential equation becomes

EIγI + iℏdγI
dt

e−(i/ℏ)EIt = EIγIe−(i/ℏ)EIt + µEγIIe−(i/ℏ)EIIt.
(9.41)
9-16","329"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","234","Similarly, the equation in dCII/dt becomes

EIIγII + iℏdγII
dt

e−(i/ℏ)EIIt = EIIγIIe−(i/ℏ)EIIt + µEγIe−(i/ℏ)EIt.
(9.42)
Now you will notice that we have equal terms on both sides of each equation. We
cancel these terms, and we also multiply the ﬁrst equation by e+iEIt/ℏand the
second by e+iEIIt/ℏ. Remembering that (EI −EII) = 2A = ℏω0, we have ﬁnally,
iℏdγI
dt = µE(t)eiω0tγII,
iℏdγII
dt
= µE(t)e−iω0tγI.
(9.43)
Now we have an apparently simple pair of equations—and they are still
exact, of course. The derivative of one variable is a function of time µE(t)eiω0t,
multiplied by the second variable; the derivative of the second is a similar time
function, multiplied by the ﬁrst. Although these simple equations cannot be
solved in general, we will solve them for some special cases.
We are, for the moment at least, interested only in the case of an oscillating
electric ﬁeld. Taking E(t) as given in Eq. (9.37), we ﬁnd that the equations for
γI and γII become
iℏdγI
dt = µE0[ei(ω+ω0)t + e−i(ω−ω0)t]γII,
iℏdγII
dt
= µE0[ei(ω−ω0)t + e−i(ω+ω0)t]γI.
(9.44)
Now if E0 is suﬃciently small, the rates of change of γI and γII are also small.
The two γ’s will not vary much with t, especially in comparison with the rapid
variations due to the exponential terms. These exponential terms have real and
imaginary parts that oscillate at the frequency ω + ω0 or ω −ω0. The terms
with ω + ω0 oscillate very rapidly about an average value of zero and, therefore,
do not contribute very much on the average to the rate of change of γ. So we can
make a reasonably good approximation by replacing these terms by their average
9-17","284"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","235","value, namely, zero. We will just leave them out, and take as our approximation:
iℏdγI
dt = µE0e−i(ω−ω0)tγII,
iℏdγII
dt
= µE0ei(ω−ω0)tγI.
(9.45)
Even the remaining terms, with exponents proportional to (ω −ω0), will also vary
rapidly unless ω is near ω0. Only then will the right-hand side vary slowly enough
that any appreciable amount will accumulate when we integrate the equations
with respect to t. In other words, with a weak electric ﬁeld the only signiﬁcant
frequencies are those near ω0.
With the approximation made in getting Eq. (9.45), the equations can be
solved exactly, but the work is a little elaborate, so we won’t do that until later
when we take up another problem of the same type. Now we’ll just solve them
approximately—or rather, we’ll ﬁnd an exact solution for the case of perfect
resonance, ω = ω0, and an approximate solution for frequencies near resonance.
9-4 Transitions at resonance
Let’s take the case of perfect resonance ﬁrst. If we take ω = ω0, the exponen-
tials are equal to one in both equations of (9.45), and we have just
dγI
dt = −iµE0
ℏ
γII,
dγII
dt
= −iµE0
ℏ
γI.
(9.46)
If we eliminate ﬁrst γI and then γII from these equations, we ﬁnd that each
satisﬁes the diﬀerential equation of simple harmonic motion:
d2γ
dt2 = −
µE0
ℏ
2
γ.
(9.47)
The general solutions for these equations can be made up of sines and cosines.
As you can easily verify, the following equations are a solution:
γI = a cos
µE0
ℏ

t + b sin
µE0
ℏ

t,
γII = ib cos
µE0
ℏ

t −ia sin
µE0
ℏ

t,
(9.48)
9-18","284"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","236","where a and b are constants to be determined to ﬁt any particular physical
situation.
For instance, suppose that at t = 0 our molecular system was in the upper
energy state | I⟩, which would require—from Eq. (9.40)—that γI = 1 and γII = 0
at t = 0. For this situation we would need a = 1 and b = 0. The probability that
the molecule is in the state | I⟩at some later t is the absolute square of γI, or
PI = |γI|2 = cos2
µE0
ℏ

t.
(9.49)
Similarly, the probability that the molecule will be in the state | II⟩is given by
the absolute square of γII,
PII = |γII|2 = sin2
µE0
ℏ

t.
(9.50)
So long as E is small and we are on resonance, the probabilities are given by
simple oscillating functions. The probability to be in state | I⟩falls from one
to zero and back again, while the probability to be in the state | II⟩rises from
zero to one and back. The time variation of the two probabilities is shown in
Fig. 9-5. Needless to say, the sum of the two probabilities is always equal to one;
the molecule is always in some state!
t
t in units of π¯h/2µE0
1
2
P
0
1
PI
PII
Fig. 9-5. Probabilities for the two states of the ammonia molecule in
a sinusoidal electric ﬁeld.
9-19","237"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","237","Let’s suppose that it takes the molecule the time T to go through the cavity.
If we make the cavity just long enough so that µE0T/ℏ= π/2, then a molecule
which enters in state | I⟩will certainly leave it in state | II⟩. If it enters the cavity
in the upper state, it will leave the cavity in the lower state. In other words, its
energy is decreased, and the loss of energy can’t go anywhere else but into the
machinery which generates the ﬁeld. The details by which you can see how the
energy of the molecule is fed into the oscillations of the cavity are not simple;
however, we don’t need to study these details, because we can use the principle
of conservation of energy. (We could study them if we had to, but then we would
have to deal with the quantum mechanics of the ﬁeld in the cavity in addition to
the quantum mechanics of the atom.)
In summary: the molecule enters the cavity, the cavity ﬁeld—oscillating at
exactly the right frequency—induces transitions from the upper to the lower state,
and the energy released is fed into the oscillating ﬁeld. In an operating maser
the molecules deliver enough energy to maintain the cavity oscillations—not only
providing enough power to make up for the cavity losses but even providing small
amounts of excess power that can be drawn from the cavity. Thus, the molecular
energy is converted into the energy of an external electromagnetic ﬁeld.
Remember that before the beam enters the cavity, we have to use a ﬁlter which
separates the beam so that only the upper state enters. It is easy to demonstrate
that if you were to start with molecules in the lower state, the process will go the
other way and take energy out of the cavity. If you put the unﬁltered beam in, as
many molecules are taking energy out as are putting energy in, so nothing much
would happen. In actual operation it isn’t necessary, of course, to make (µE0T/ℏ)
exactly π/2. For any other value (except an exact integral multiple of π), there
is some probability for transitions from state | I⟩to state | II⟩. For other values,
however, the device isn’t 100 percent eﬃcient; many of the molecules which leave
the cavity could have delivered some energy to the cavity but didn’t.
In actual use, the velocity of all the molecules is not the same; they have
some kind of Maxwell distribution. This means that the ideal periods of time
for diﬀerent molecules will be diﬀerent, and it is impossible to get 100 percent
eﬃciency for all the molecules at once. In addition, there is another complication
which is easy to take into account, but we don’t want to bother with it at this
stage. You remember that the electric ﬁeld in a cavity usually varies from place
to place across the cavity. Thus, as the molecules drift across the cavity, the
electric ﬁeld at the molecule varies in a way that is more complicated than the
simple sinusoidal oscillation in time that we have assumed. Clearly, one would
9-20","521"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","238","E
E′
EI
EII
¯hω1
¯hω2
¯hω0
Fig. 9-6. The energy levels of a “three-state” maser.
have to use a more complicated integration to do the problem exactly, but the
general idea is still the same.
There are other ways of making masers. Instead of separating the atoms
in state | I⟩from those in state | II⟩by a Stern-Gerlach apparatus, one can
have the atoms already in the cavity (as a gas or a solid) and shift atoms from
state | II⟩to state | I⟩by some means. One way is one used in the so-called
three-state maser. For it, atomic systems are used which have three energy levels,
as shown in Fig. 9-6, with the following special properties. The system will
absorb radiation (say, light) of frequency ℏω1 and go from the lowest energy
level EII, to some high-energy level E′, and then will quickly emit photons of
frequency ℏω2 and go to the state | I⟩with energy EI. The state | I⟩has a long
lifetime so its population can be raised, and the conditions are then appropriate
for maser operation between states | I⟩and | II⟩. Although such a device is called
a “three-state” maser, the maser operation really works just as a two-state system
such as we are describing.
A laser (Light Ampliﬁcation by Stimulated Emission of Radiation) is just a
maser working at optical frequencies. The “cavity” for a laser usually consists of
just two plane mirrors between which standing waves are generated.
9-5 Transitions oﬀresonance
Finally, we would like to ﬁnd out how the states vary in the circumstance
that the cavity frequency is nearly, but not exactly, equal to ω0. We could solve
this problem exactly, but instead of trying to do that, we’ll take the important
9-21","293"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","239","case that the electric ﬁeld is small and also the period of time T is small, so
that µE0T/ℏis much less than one. Then, even in the case of perfect resonance
which we have just worked out, the probability of making a transition is small.
Suppose that we start again with γI = 1 and γII = 0. During the time T we
would expect γI to remain nearly equal to one, and γII to remain very small
compared with unity. Then the problem is very easy. We can calculate γII from
the second equation in (9.45), taking γI equal to one and integrating from t = 0
to t = T. We get
γII = µE0
ℏ
1 −ei(ω−ω0)T
ω −ω0

.
(9.51)
This γII, used with Eq. (9.40), gives the amplitude to have made a transition from
the state | I⟩to the state | II⟩during the time interval T. The probability P(I →
II) to make the transition is |γII|2, or
P(I →II) = |γII|2 =
µE0T
ℏ
2 sin2[(ω −ω0)T/2]
[(ω −ω0)T/2]2 .
(9.52)
It is interesting to plot this probability for a ﬁxed length of time as a function
of the frequency of the cavity in order to see how sensitive it is to frequencies
near the resonant frequency ω0. We show such a plot of P(I →II) in Fig. 9-7.
(The vertical scale has been adjusted to be 1 at the peak by dividing by the value
of the probability when ω = ω0.) We have seen a curve like this in the diﬀraction
theory, so you should already be familiar with it. The curve falls rather abruptly
to zero for (ω −ω0) = 2π/T and never regains signiﬁcant size for large frequency
deviations. In fact, by far the greatest part of the area under the curve lies
within the range ±π/T. It is possible to show† that the area under the curve is
just 2π/T and is equal to the area of the shaded rectangle drawn in the ﬁgure.
Let’s examine the implication of our results for a real maser. Suppose that
the ammonia molecule is in the cavity for a reasonable length of time, say for
one millisecond. Then for f0 = 24,000 megacycles, we can calculate that the
probability for a transition falls to zero for a frequency deviation of (f −f0)/f0 =
1/f0T, which is ﬁve parts in 108. Evidently the frequency must be very close
to ω0 to get a signiﬁcant transition probability. Such an eﬀect is the basis of the
great precision that can be obtained with “atomic” clocks, which work on the
maser principle.
† Using the formula R ∞
−∞(sin2 x/x2) dx = π.
9-22","449"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","240","ω
ω0
PI→II(ω)/PI→II(ω0)
0
1/2
1
2π/T
π/T
Fig. 9-7. Transition probability for the ammonia molecule as function
of frequency.
9-6 The absorption of light
Our treatment above applies to a more general situation than the ammonia
maser. We have treated the behavior of a molecule under the inﬂuence of an
electric ﬁeld, whether that ﬁeld was conﬁned in a cavity or not. So we could be
simply shining a beam of “light”—at microwave frequencies—at the molecule and
ask for the probability of emission or absorption. Our equations apply equally
well to this case, but let’s rewrite them in terms of the intensity of the radiation
rather than the electric ﬁeld. If we deﬁne the intensity I to be the average energy
ﬂow per unit area per second, then from Chapter 27 of Volume II, we can write
I = ϵ0c2|E × B|ave = 1
2ϵ0c2|E × B|max = 2ϵ0cE2
0.
(The maximum value of E is 2E0.) The transition probability now becomes:
P(I →II) = 2π

µ2
4πϵ0ℏ2c

IT 2 sin2[(ω −ω0)T/2]
[(ω −ω0)T/2]2 .
(9.53)
Ordinarily the light shining on such a system is not exactly monochromatic.
It is, therefore, interesting to solve one more problem—that is, to calculate the
9-23","205"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","241","transition probability when the light has intensity I(ω) per unit frequency interval,
covering a broad range which includes ω0. Then, the probability of going from
| I⟩to | II⟩will become an integral:
P(I →II) = 2π

µ2
4πϵ0ℏ2c

T 2
Z ∞
0
I(ω) sin2[(ω −ω0)T/2]
[(ω −ω0)T/2]2
dω.
(9.54)
In general, I(ω) will vary much more slowly with ω than the sharp resonance
term. The two functions might appear as shown in Fig. 9-8. In such cases, we
can replace I(ω) by its value I(ω0) at the center of the sharp resonance curve and
take it outside of the integral. What remains is just the integral under the curve
of Fig. 9-7, which is, as we have seen, just equal to 2π/T. We get the result that
P(I →II) = 4π2

µ2
4πϵ0ℏ2c

I(ω0)T.
(9.55)
This is an important result, because it is the general theory of the absorption
of light by any molecular or atomic system. Although we began by considering
a case in which state | I⟩) had a higher energy than state | II⟩, none of our
ω
ω0
I(ω)
I(ω)
I(ω0)
Fig. 9-8. The spectral intensity I(ω) can be approximated by its value
at ω0.
9-24","205"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","242","arguments depended on that fact. Equation (9.55) still holds if the state | I⟩has
a lower energy than the state | II⟩; then P(I →II) represents the probability for
a transition with the absorption of energy from the incident electromagnetic wave.
The absorption of light by any atomic system always involves the amplitude for
a transition in an oscillating electric ﬁeld between two states separated by an
energy E = ℏω0. For any particular case, it is always worked out in just the way we
have done here and gives an expression like Eq. (9.55). We, therefore, emphasize
the following features of this result. First, the probability is proportional to T.
In other words, there is a constant probability per unit time that transitions
will occur. Second, this probability is proportional to the intensity of the light
incident on the system. Finally, the transition probability is proportional to µ2,
where, you remember, µE deﬁned the shift in energy due to the electric ﬁeld E.
Because of this, µE also appeared in Eqs. (9.38) and (9.39) as the coupling term
that is responsible for the transition between the otherwise stationary states | I⟩
and | II⟩. In other words, for the small E we have been considering, µE is the
so-called “perturbation term” in the Hamiltonian matrix element which connects
the states | I⟩and | II⟩. In the general case, we would have that µE gets replaced
by the matrix element ⟨II | H | I⟩(see Section 5-6).
In Volume I (Section 42-5) we talked about the relations among light absorp-
tion, induced emission, and spontaneous emission in terms of the Einstein A-
and B-coeﬃcients. Here, we have at last the quantum mechanical procedure for
computing these coeﬃcients. What we have called P(I →II) for our two-state
ammonia molecule corresponds precisely to the absorption coeﬃcient Bnm of the
Einstein radiation theory. For the complicated ammonia molecule—which is too
diﬃcult for anyone to calculate—we have taken the matrix element ⟨II | H | I⟩
as µE, saying that µ is to be gotten from experiment. For simpler atomic systems,
the µmn which belongs to any particular transition can be calculated from the
deﬁnition
µmnE = ⟨m | H | n⟩= Hmn,
(9.56)
where Hmn is the matrix element of the Hamiltonian which includes the eﬀects
of a weak electric ﬁeld. The µmn calculated in this way is called the electric
dipole matrix element. The quantum mechanical theory of the absorption and
emission of light is, therefore, reduced to a calculation of these matrix elements
for particular atomic systems.
Our study of a simple two-state system has thus led us to an understanding
of the general problem of the absorption and emission of light.
9-25","452"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","243","10
Other Two-State Systems
10-1 The hydrogen molecular ion
In the last chapter we discussed some aspects of the ammonia molecule under
the approximation that it can be considered as a two-state system. It is, of
course, not really a two-state system—there are many states of rotation, vibration,
translation, and so on—but each of these states of motion must be analyzed in
terms of two internal states because of the ﬂip-ﬂop of the nitrogen atom. Here we
are going to consider other examples of systems which, to some approximation or
other, can be considered as two-state systems. Lots of things will be approximate
because there are always many other states, and in a more accurate analysis they
would have to be taken into account. But in each of our examples we will be able
to understand a great deal by just thinking about two states.
Since we will only be dealing with two-state systems, the Hamiltonian we need
will look just like the one we used in the last chapter. When the Hamiltonian is
independent of time, we know that there are two stationary states with deﬁnite—
and usually diﬀerent—energies. Generally, however, we start our analysis with a
set of base states which are not these stationary states, but states which may,
perhaps, have some other simple physical meaning. Then, the stationary states
of the system will be represented by a linear combination of these base states.
For convenience, we will summarize the important equations from Chapter 9.
Let the original choice of base states be | 1⟩and | 2⟩. Then any state | ψ⟩is
represented by the linear combination
| ψ⟩= | 1⟩⟨1 | ψ⟩+ | 2⟩⟨2 | ψ⟩= | 1⟩C1 + | 2⟩C2.
(10.1)
The amplitudes Ci (by which we mean either C1 or C2) satisfy the two linear
diﬀerential equations
iℏdCi
dt =
X
j
HijCj,
(10.2)
where both i and j take on the values 1 and 2.
10-1","323"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","244","When the terms of the Hamiltonian Hij do not depend on t, the two states
of deﬁnite energy (the stationary states), which we call
| ψI⟩= | I⟩e−(i/ℏ)EIt
and
| ψII⟩= | II⟩e−(i/ℏ)EIIt,
have the energies
EI = H11 + H22
2
+
sH11 −H22
2
2
+ H12H21,
EII = H11 + H22
2
−
sH11 −H22
2
2
+ H12H21.
(10.3)
The two C’s for each of these states have the same time dependence. The state
vectors | I⟩and | II⟩which go with the stationary states are related to our original
base states | 1⟩and | 2⟩by
| I⟩= | 1⟩a1 + | 2⟩a2,
| II⟩= | 1⟩a′
1 + | 2⟩a′
2.
(10.4)
The a’s are complex constants, which satisfy
|a1|2 + |a2|2 = 1,
a1
a2
=
H12
EI −H11
,
(10.5)
|a′
1|2 + |a′
2|2 = 1,
a′
1
a′
2
=
H12
EII −H11
.
(10.6)
If H11 and H22 are equal—say both are equal to E0—and H12 = H21 = −A,
then EI = E0 + A, EII = E0 −A, and the states | I⟩and | II⟩are particularly
simple:
| I⟩=
1
√
2

| 1⟩−| 2⟩

,
| II⟩=
1
√
2

| 1⟩+ | 2⟩

.
(10.7)
10-2","213"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","245","Now we will use these results to discuss a number of interesting examples
taken from the ﬁelds of chemistry and physics. The ﬁrst example is the hydrogen
molecular ion. A positively ionized hydrogen molecule consists of two protons
with one electron worming its way around them. If the two protons are very far
apart, what states would we expect for this system? The answer is pretty clear:
The electron will stay close to one proton and form a hydrogen atom in its lowest
state, and the other proton will remain alone as a positive ion. So, if the two
protons are far apart, we can visualize one physical state in which the electron is
“attached” to one of the protons. There is, clearly, another state symmetric to
that one in which the electron is near the other proton, and the ﬁrst proton is the
one that is an ion. We will take these two as our base states, and we’ll call them
| 1⟩and | 2⟩. They are sketched in Fig. 10-1. Of course, there are really many
states of an electron near a proton, because the combination can exist as any one
of the excited states of the hydrogen atom. We are not interested in that variety
of states now; we will consider only the situation in which the hydrogen atom is
in the lowest state—its ground state—and we will, for the moment, disregard
spin of the electron. We can just suppose that for all our states the electron has
its spin “up” along the z-axis.†
| 1⟩
+
+
ELECTRON
PROTONS
| 2⟩
+
+
Fig. 10-1. A set of base states for two protons and an electron.
Now to remove an electron from a hydrogen atom requires 13.6 electron volts
of energy. So long as the two protons of the hydrogen molecular ion are far apart,
it still requires about this much energy—which is for our present considerations a
† This is satisfactory so long as there are no important magnetic ﬁelds. We will discuss the
eﬀects of magnetic ﬁelds on the electron later in this chapter, and the very small eﬀects of spin
in the hydrogen atom in Chapter 12.
10-3","365"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","246","great deal of energy—to get the electron somewhere near the midpoint between
the protons. So it is impossible, classically, for the electron to jump from one
proton to the other. However, in quantum mechanics it is possible—though not
very likely. There is some small amplitude for the electron to move from one
proton to the other. As a ﬁrst approximation, then, each of our base states | 1⟩
and | 2⟩will have the energy E0, which is just the energy of one hydrogen atom
plus one proton. We can take that the Hamiltonian matrix elements H11 and H22
are both approximately equal to E0. The other matrix elements H12 and H21,
which are the amplitudes for the electron to go back and forth, we will again
write as −A.
You see that this is the same game we played in the last two chapters. If we
disregard the fact that the electron can ﬂip back and forth, we have two states
of exactly the same energy. This energy will, however, be split into two energy
levels by the possibility of the electron going back and forth—the greater the
probability of the transition, the greater the split. So the two energy levels of the
system are E0 + A and E0 −A; and the states which have these deﬁnite energies
are given by Eqs. (10.7).
From our solution we see that if a proton and a hydrogen atom are put
anywhere near together, the electron will not stay on one of the protons but will
ﬂip back and forth between the two protons. If it starts on one of the protons, it
will oscillate back and forth between the states | 1⟩and | 2⟩—giving a time-varying
solution. In order to have the lowest energy solution (which does not vary with
time), it is necessary to start the system with equal amplitudes for the electron
to be around each proton. Remember, there are not two electrons—we are not
saying that there is an electron around each proton. There is only one electron,
and it has the same amplitude—1/
√
2 in magnitude—to be in either position.
Now the amplitude A for an electron which is near one proton to get to the
other one depends on the separation between the protons. The closer the protons
are together, the larger the amplitude. You remember that we talked in Chapter 7
about the amplitude for an electron to “penetrate a barrier,” which it could not
do classically. We have the same situation here. The amplitude for an electron to
get across decreases roughly exponentially with the distance—for large distances.
Since the transition probability, and therefore A, gets larger when the protons
are closer together, the separation of the energy levels will also get larger. If the
system is in the state | I⟩, the energy E0 + A increases with decreasing distance,
so these quantum mechanical eﬀects make a repulsive force tending to keep the
protons apart. On the other hand, if the system is in the state | II⟩, the total
10-4","509"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","247","D
DISTANCE
BETWEEN
PROTONS
E
EI = E0 + A
EII = E0 −A
E0
Fig. 10-2. The energies of the two stationary states of the H+
2 ion as
a function of the distance between the two protons.
energy decreases if the protons are brought closer together; there is an attractive
force pulling the protons together. The variation of the two energies with the
distance between the two protons should be roughly as shown in Fig. 10-2. We
have, then, a quantum-mechanical explanation of the binding force that holds
the H+
2 ion together.
We have, however, forgotten one thing. In addition to the force we have
just described, there is also an electrostatic repulsive force between the two
protons. When the two protons are far apart—as in Fig. 10-1—the “bare” proton
sees only a neutral atom, so there is a negligible electrostatic force. At very
close distances, however, the “bare” proton begins to get “inside” the electron
distribution—that is, it is closer to the proton on the average than to the electron.
So there begins to be some extra electrostatic energy which is, of course, positive.
This energy—which also varies with the separation—should be included in E0. So
for E0 we should take something like the broken-line curve in Fig. 10-2 which rises
rapidly for distances less than the radius of a hydrogen atom. We should add and
subtract the ﬂip-ﬂop energy A from this E0. When we do that, the energies EI
10-5","246"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","248","D(˚A)
1
2
3
4
∆E
EH
−0.2
−0.1
0
0.1
0.2
0.3
EI
EII
Fig. 10-3. The energy levels of the H+
2 ion as a function of the
interproton distance D. (EH = 13.6 eV.)
and EII will vary with the interproton distance D as shown in Fig. 10-3. [In this
ﬁgure, we have plotted the results of a more detailed calculation. The interproton
distance is given in units of 1 Å (10−8 cm), and the excess energy over a proton
plus a hydrogen atom is given in units of the binding energy of the hydrogen
atom—the so-called “Rydberg” energy, 13.6 eV.] We see that the state | II⟩
has a minimum-energy point. This will be the equilibrium conﬁguration—the
lowest energy condition—for the H+
2 ion. The energy at this point is lower than
the energy of a separated proton and hydrogen ion, so the system is bound. A
single electron acts to hold the two protons together. A chemist would call it a
“one-electron bond.”
This kind of chemical binding is also often called “quantum mechanical
resonance” (by analogy with the two coupled pendulums we have described
before). But that really sounds more mysterious than it is, it’s only a “resonance”
if you start out by making a poor choice for your base states—as we did also! If
you picked the state | II⟩, you would have the lowest energy state—that’s all.
We can see in another way why such a state should have a lower energy
than a proton and a hydrogen atom. Let’s think about an electron near two
10-6","265"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","249","protons with some ﬁxed, but not too large, separation. You remember that with a
single proton the electron is “spread out” because of the uncertainty principle. It
seeks a balance between having a low Coulomb potential energy and not getting
conﬁned into too small a space, which would make a high kinetic energy (because
of the uncertainty relation ∆p ∆x ≈ℏ). Now if there are two protons, there is
more space where the electron can have a low potential energy. It can spread
out—lowering its kinetic energy—without increasing its potential energy. The
net result is a lower energy than a proton and a hydrogen atom. Then why does
the other state | I⟩have a higher energy? Notice that this state is the diﬀerence
of the states | 1⟩and | 2⟩. Because of the symmetry of | 1⟩and | 2⟩, the diﬀerence
must have zero amplitude to ﬁnd the electron half-way between the two protons.
This means that the electron is somewhat more conﬁned, which leads to a larger
energy.
We should say that our approximate treatment of the H+
2 ion as a two-state
system breaks down pretty badly once the protons get as close together as they
are at the minimum in the curve of Fig. 10-3, and so, will not give a good value for
the actual binding energy. For small separations, the energies of the two “states”
we imagined in Fig. 10-1 are not really equal to E0; a more reﬁned quantum
mechanical treatment is needed.
Suppose we ask now what would happen if instead of two protons, we had two
diﬀerent objects—as, for example, one proton and one lithium positive ion (both
particles still with a single positive charge). In such a case, the two terms H11
and H22 of the Hamiltonian would no longer be equal; they would, in fact, be
quite diﬀerent. If it should happen that the diﬀerence (H11 −H22) is, in absolute
value, much greater than A = −H12, the attractive force gets very weak, as we
can see in the following way.
If we put H12H21 = A2 into Eqs. (10.3) we get
E = H11 + H22
2
± H11 −H22
2
s
1 +
4A2
(H11 −H22)2 .
When H11 −H22 is much greater than A2, the square root is very nearly equal to
1 +
2A2
(H11 −H22)2 .
10-7","392"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","250","The two energies are then
EI = H11 +
A2
(H11 −H22),
EII = H22 −
A2
(H11 −H22).
(10.8)
They are now very nearly just the energies H11 and H22 of the isolated atoms,
pushed apart only slightly by the ﬂip-ﬂop amplitude A.
The energy diﬀerence EI −EII is
(H11 −H22) +
2A2
H11 −H22
.
The additional separation from the ﬂip-ﬂop of the electron is no longer equal
to 2A; it is smaller by the factor A/(H11 −H22), which we are now taking to
be much less than one. Also, the dependence of EI −EII on the separation of
the two nuclei is much smaller than for the H+
2 ion—it is also reduced by the
factor A/(H11 −H22). We can now see why the binding of unsymmetric diatomic
molecules is generally very weak.
In our theory of the H+
2 ion we have discovered an explanation for the
mechanism by which an electron shared by two protons provides, in eﬀect, an
attractive force between the two protons which can be present even when the
protons are at large distances. The attractive force comes from the reduced energy
of the system due to the possibility of the electron jumping from one proton to
the other. In such a jump the system changes from the conﬁguration (hydrogen
atom, proton) to the conﬁguration (proton, hydrogen atom), or switches back.
We can write the process symbolically as
(H, p) ⇌(p, H).
The energy shift due to this process is proportional to the amplitude A that an
electron whose energy is −WH (its binding energy in the hydrogen atom) can
get from one proton to the other.
For large distances R between the two protons, the electrostatic potential
energy of the electron is nearly zero over most of the space it must go when
it makes its jump. In this space, then, the electron moves nearly like a free
particle in empty space—but with a negative energy! We have seen in Chapter 3
10-8","332"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","251","[Eq. (3.7)] that the amplitude for a particle of deﬁnite energy to get from one
place to another a distance r away is proportional to
e(i/ℏ)pr
r
,
where p is the momentum corresponding to the deﬁnite energy. In the present
case (using the nonrelativistic formula), p is given by
p2
2m = −WH.
(10.9)
This means that p is an imaginary number,
p = i
p
2mWH
(the other sign for the radical gives nonsense here).
We should expect, then, that the amplitude A for the H+
2 ion will vary as
A ∝e−(√2mWH/ℏ)R
R
(10.10)
for large separations R between the two protons. The energy shift due to the
electron binding is proportional to A, so there is a force pulling the two protons
together which is proportional—for large R—to the derivative of (10.10) with
respect to R.
Finally, to be complete, we should remark that in the two-proton, one-electron
system there is still one other eﬀect which gives a dependence of the energy on R.
We have neglected it until now because it is usually rather unimportant—the
exception is just for those very large distances where the energy of the exchange
term A has decreased exponentially to very small values. The new eﬀect we
are thinking of is the electrostatic attraction of the proton for the hydrogen
atom, which comes about in the same way any charged object attracts a neutral
object. The bare proton makes an electric ﬁeld E (varying as 1/R2) at the
neutral hydrogen atom. The atom becomes polarized, taking on an induced
dipole moment µ proportional to E. The energy of the dipole is µE, which is
proportional to E2—or to 1/R4. So there is a term in the energy of the system
which decreases with the fourth power of the distance. (It is a correction to E0.)
This energy falls oﬀwith distance more slowly than the shift A given by (10.10);
10-9","320"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","252","at some large separation R it becomes the only remaining important term giving
a variation of energy with R—and, therefore, the only remaining force. Note
that the electrostatic term has the same sign for both of the base states (the
force is attractive, so the energy is negative) and so also for the two stationary
states, whereas the electron exchange term A gives opposite signs for the two
stationary states.
10-2 Nuclear forces
We have seen that the system of a hydrogen atom and a proton has an energy
of interaction due to the exchange of the single electron which varies at large
separations R as
e−αR
R
,
(10.11)
with α = √2mWH/ℏ. (One usually says that there is an exchange of a “virtual”
electron when—as here—the electron has to jump across a space where it would
have a negative energy. More speciﬁcally, a “virtual exchange” means that the
phenomenon involves a quantum mechanical interference between an exchanged
state and a nonexchanged state.)
Now we might ask the following question: Could it be that forces between
other kinds of particles have an analogous origin? What about, for example,
the nuclear force between a neutron and a proton, or between two protons? In
an attempt to explain the nature of nuclear forces, Yukawa proposed that the
force between two nucleons is due to a similar exchange eﬀect—only, in this case,
due to the virtual exchange, not of an electron, but of a new particle, which he
called a “meson.” Today, we would identify Yukawa’s meson with the π-meson
(or “pion”) produced in high-energy collisions of protons or other particles.
Let’s see, as an example, what kind of a force we would expect from the
exchange of a positive pion (π+) of mass mπ between a proton and a neutron.
Just as a hydrogen atom H0 can go into a proton p+ by giving up an electron e−,
H0 →p+ + e−,
(10.12)
a proton p+ can go into a neutron n0 by giving up a π+ meson:
p+ →n0 + π+.
(10.13)
10-10","343"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","253","So if we have a proton at a and a neutron at b separated by the distance R, the
proton can become a neutron by emitting a π+, which is then absorbed by the
neutron at b, turning it into a proton. There is an energy of interaction of the
two-nucleon (plus pion) system which depends on the amplitude A for the pion
exchange—just as we found for the electron exchange in the H+
2 ion.
In the process (10.12), the energy of the H0 atom is less than that of the
proton by WH (calculating nonrelativistically, and omitting the rest energy mc2
of the electron), so the electron has a negative kinetic energy—or imaginary
momentum—as in Eq. (10.9). In the nuclear process (10.13), the proton and
neutron have almost equal masses, so the π+ will have zero total energy. The
relation between the total energy E and the momentum p for a pion of mass mπ
is
E2 = p2c2 + m2
πc4.
Since E is zero (or at least negligible in comparison with mπ), the momentum is
again imaginary:
p = imπc.
Using the same arguments we gave for the amplitude that a bound electron
would penetrate the barrier in the space between two protons, we get for the
nuclear case an exchange amplitude A which should—for large R—go as
e−(mπc/ℏ)R
R
.
(10.14)
The interaction energy is proportional to A, and so varies in the same way. We
get an energy variation in the form of the so-called Yukawa potential between
two nucleons. Incidentally, we obtained this same formula earlier directly from
the diﬀerential equation for the motion of a pion in free space [see Chapter 28,
Vol. II, Eq. (28.18)].
We can, following the same line of argument, discuss the interaction between
two protons (or between two neutrons) which results from the exchange of a
neutral pion (π0). The basic process is now
p+ →p+ + π0.
(10.15)
A proton can emit a virtual π0, but then it remains still a proton. If we have two
protons, proton No. 1 can emit a virtual π0 which is absorbed by proton No. 2.
At the end, we still have two protons. This is somewhat diﬀerent from the H+
2
10-11","373"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","254","ion. There the H0 went into a diﬀerent condition—the proton—after emitting
the electron. Now we are assuming that a proton can emit a π0 without changing
its character. Such processes are, in fact, observed in high-energy collisions. The
process is analogous to the way that an electron emits a photon and ends up still
an electron:
e →e + photon.
(10.16)
We do not “see” the photons inside the electrons before they are emitted or
after they are absorbed, and their emission does not change the “nature” of the
electron.
Going back to the two protons, there is an interaction energy which arises
from the amplitude A that one proton emits a neutral pion which travels across
(with imaginary momentum) to the other proton and is absorbed there. This
amplitude is again proportional to (10.14), with mπ the mass of the neutral
pion. All the same arguments give an equal interaction energy for two neutrons.
Since the nuclear forces (disregarding electrical eﬀects) between neutron and
proton, between proton and proton, between neutron and neutron are the same,
we conclude that the masses of the charged and neutral pions should be the
same. Experimentally, the masses are indeed very nearly equal, and the small
diﬀerence is about what one would expect from electric self-energy corrections
(see Chapter 28, Vol. II).
There are other kinds of particles—like K-mesons—which can be exchanged
between two nucleons. It is also possible for two pions to be exchanged at the
same time. But all of these other exchanged “objects” have a rest mass mx higher
than the pion mass mπ, and lead to terms in the exchange amplitude which vary
as
e−(mxc/ℏ)R
R
.
These terms die out faster with increasing R than the one-meson term. No one
knows, today, how to calculate these higher-mass terms, but for large enough
values of R only the one-pion term survives. And, indeed, those experiments which
involve nuclear interactions only at large distances do show that the interaction
energy is as predicted from the one-pion exchange theory.
In the classical theory of electricity and magnetism, the Coulomb electrostatic
interaction and the radiation of light by an accelerating charge are closely related—
both come out of the Maxwell equations. We have seen in the quantum theory that
light can be represented as the quantum excitations of the harmonic oscillations
10-12","390"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","255","of the classical electromagnetic ﬁelds in a box. Alternatively, the quantum theory
can be set up by describing light in terms of particles—photons—which obey
Bose statistics. We emphasized in Section 4-5 that the two alternative points of
view always give identical predictions. Can the second point of view be carried
through completely to include all electromagnetic eﬀects? In particular, if we
want to describe the electromagnetic ﬁeld purely in terms of Bose particles—that
is, in terms of photons—what is the Coulomb force due to?
From the “particle” point of view the Coulomb interaction between two
electrons comes from the exchange of a virtual photon. One electron emits a
photon—as in reaction (10.16)—which goes over to the second electron, where
it is absorbed in the reverse of the same reaction. The interaction energy is
again given by a formula like (10.14), but now with mπ replaced by the rest
mass of the photon—which is zero. So the virtual exchange of a photon between
two electrons gives an interaction energy that varies simply inversely as R, the
distance between the two electrons—just the normal Coulomb potential energy!
In the “particle” theory of electromagnetism, the process of a virtual photon
exchange gives rise to all the phenomena of electrostatics.
10-3 The hydrogen molecule
As our next two-state system we will look at the neutral hydrogen molecule
H2. It is, naturally, more complicated to understand because it has two electrons.
Again, we start by thinking of what happens when the two protons are well
separated. Only now we have two electrons to add. To keep track of them, we’ll
call one of them “electron a” and the other “electron b.” We can again imagine two
possible states. One possibility is that “electron a” is around the ﬁrst proton and
“electron b” is around the second, as shown in the top half of Fig. 10-4. We have
simply two hydrogen atoms. We will call this state | 1⟩. There is also another
possibility: that “electron b” is around the ﬁrst proton and that “electron a” is
around the second. We call this state | 2⟩. From the symmetry of the situation,
those two possibilities should be energetically equivalent, but, as we will see, the
energy of the system is not just the energy of two hydrogen atoms. We should
mention that there are many other possibilities. For instance, “electron a” might
be near the ﬁrst proton and “electron b” might be in another state around the
same proton. We’ll disregard such a case, since it will certainly have higher energy
(because of the large Coulomb repulsion between the two electrons). For greater
accuracy, we would have to include such states, but we can get the essentials of
10-13","453"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","256","| 1⟩
+
+
a
b
ELECTRONS
PROTONS
| 2⟩
+
+
b
a
Fig. 10-4. A set of base states for the H2 molecule.
the molecular binding by considering just the two states of Fig. 10-4. To this
approximation we can describe any state by giving the amplitude ⟨1 | φ⟩to be
in the state | 1⟩and an amplitude ⟨2 | φ⟩to be in state | 2⟩. In other words, the
state vector | φ⟩can be written as the linear combination
| φ⟩=
X
i
| i⟩⟨i | φ⟩.
To proceed, we assume—as usual—that there is some amplitude A that the
electrons can move through the intervening space and exchange places. This
possibility of exchange means that the energy of the system is split, as we have
seen for other two-state systems. As for the hydrogen molecular ion, the splitting
is very small when the distance between the protons is large. As the protons
approach each other, the amplitude for the electrons to go back and forth increases,
so the splitting increases. The decrease of the lower energy state means that there
is an attractive force which pulls the atoms together. Again the energy levels
rise when the protons get very close together because of the Coulomb repulsion.
The net ﬁnal result is that the two stationary states have energies which vary
with the separation as shown in Fig. 10-5. At a separation of about 0.74 Å, the
lower energy level reaches a minimum; this is the proton-proton distance of the
true hydrogen molecule.
10-14","256"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","257","D(˚A)
0
1
2
3
∆E
EH
−0.4
−0.2
0
0.2
0.4
Fig. 10-5. The energy levels of the H2 molecule for diﬀerent interpro-
ton distances D. (EH = 13.6 eV.)
Now you have probably been thinking of an objection. What about the
fact that the two electrons are identical particles? We have been calling them
“electron a” and “electron b,” but there really is no way to tell which is which.
And we have said in Chapter 4 that for electrons—which are Fermi particles—if
there are two ways something can happen by exchanging the electrons, the two
amplitudes will interfere with a negative sign. This means that if we switch which
electron is which, the sign of the amplitude must reverse. We have just concluded,
however, that the bound state of the hydrogen molecule would be (at t = 0)
| II⟩=
1
√
2 (| 1⟩+ | 2⟩).
However, according to our rules of Chapter 4, this state is not allowed. If we
reverse which electron is which, we get the state
1
√
2 (| 2⟩+ | 1⟩).
and we get the same sign instead of the opposite one.
10-15","193"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","258","These arguments are correct if both electrons have the same spin. It is true
that if both electrons have spin up (or both have spin down), the only state that
is permitted is
| I⟩=
1
√
2 (| 1⟩−| 2⟩).
For this state, an interchange of the two electrons gives
1
√
2 (| 2⟩−| 1⟩).
which is −| I⟩, as required. So if we bring two hydrogen atoms near to each other
with their electrons spinning in the same direction, they can go into the state | I⟩
and not state | II⟩. But notice that state | I⟩is the upper energy state. Its curve
of energy versus separation has no minimum. The two hydrogens will always
repel and will not form a molecule. So we conclude that the hydrogen molecule
cannot exist with parallel electron spins. And that is right.
On the other hand, our state | II⟩is perfectly symmetric for the two electrons.
In fact, if we interchange which electron we call a and which we call b we get
back exactly the same state. We saw in Section 4-7 that if two Fermi particles
are in the same state, they must have opposite spins. So, the bound hydrogen
molecule must have one electron with spin up and one with spin down.
The whole story of the hydrogen molecule is really somewhat more complicated
if we want to include the proton spins. It is then no longer right to think of the
molecule as a two-state system. It should really be looked at as an eight-state
system—there are four possible spin arrangements for each of our states | 1⟩
and | 2⟩—so we were cutting things a little short by neglecting the spins. Our
ﬁnal conclusions are, however, correct.
We ﬁnd that the lowest energy state—the only bound state—of the H2 molecule
has the two electrons with spins opposite. The total spin angular momentum
of the electrons is zero. On the other hand, two nearby hydrogen atoms with
spins parallel—and so with a total angular momentum ℏ—must be in a higher
(unbound) energy state; the atoms repel each other. There is an interesting
correlation between the spins and the energies. It gives another illustration of
something we mentioned before, which is that there appears to be an “interaction”
energy between two spins because the case of parallel spins has a higher energy
than the opposite case. In a certain sense you could say that the spins try to
reach an antiparallel condition and, in doing so, have the potential to liberate
10-16","426"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","259","energy—not because there is a large magnetic force, but because of the exclusion
principle.
We saw in Section 10-1 that the binding of two diﬀerent ions by a single
electron is likely to be quite weak. This is not true for binding by two electrons.
Suppose the two protons in Fig. 10-4 were replaced by any two ions (with closed
inner electron shells and a single ionic charge), and that the binding energies of
an electron at the two ions are diﬀerent. The energies of states | 1⟩and | 2⟩would
still be equal because in each of these states we have one electron bound to each
ion. Therefore, we always have the splitting proportional to A. Two-electron
binding is ubiquitous—it is the most common valence bond. Chemical binding
usually involves this ﬂip-ﬂop game played by two electrons. Although two atoms
can be bound together by only one electron, it is relatively rare—because it
requires just the right conditions.
Finally, we want to mention that if the energy of attraction for an electron to
one nucleus is much greater than to the other, then what we have said earlier
about ignoring other possible states is no longer right. Suppose nucleus a (or
it may be a positive ion) has a much stronger attraction for an electron than
does nucleus b. It may then happen that the total energy is still fairly low
even when both electrons are at nucleus a, and no electron is at nucleus b. The
strong attraction may more than compensate for the mutual repulsion of the two
electrons. If it does, the lowest energy state may have a large amplitude to ﬁnd
both electrons at a (making a negative ion) and a small amplitude to ﬁnd any
electron at b. The state looks like a negative ion with a positive ion. This is, in
fact, what happens in an “ionic” molecule like NaCl. You can see that all the
gradations between covalent binding and ionic binding are possible.
You can now begin to see how it is that many of the facts of chemistry can
be most clearly understood in terms of a quantum mechanical description.
10-4 The benzene molecule
Chemists have invented nice diagrams to represent complicated organic
molecules. Now we are going to discuss one of the most interesting of them—the
benzene molecule shown in Fig. 10-6. It contains six carbon and six hydrogen
atoms in a symmetrical array. Each bar of the diagram represents a pair of
electrons, with spins opposite, doing the covalent bond dance. Each hydrogen
atom contributes one electron and each carbon atom contributes four electrons
to make up the total of 30 electrons involved. (There are two more electrons
10-17","450"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","260","C
C
C
C
C
C
H
H
H
H
H
H
Fig. 10-6. The benzene molecule, C6H6.
close to the nucleus of the carbon which form the ﬁrst, or K, shell. These are
not shown since they are so tightly bound that they are not appreciably involved
in the covalent binding.) So each bar in the ﬁgure represents a bond, or pair
of electrons, and the double bonds mean that there are two pairs of electrons
between alternate pairs of carbon atoms.
There is a mystery about this benzene molecule. We can calculate what
energy should be required to form this chemical compound, because the chemists
have measured the energies of various compounds which involve pieces of the
ring—for instance, they know the energy of a double bond by studying ethylene,
and so on. We can, therefore, calculate the total energy we should expect for the
benzene molecule. The actual energy of the benzene ring, however, is much lower
(a)
C
C
C
C
C
C
H
H
H
H
Br
Br
(b)
C
C
C
C
C
C
H
H
H
H
Br
Br
Fig. 10-7.
Two possibilities of orthodibromobenzene.
The two
bromines could be separated by a single bond or by a double bond.
10-18","208"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","261","than we get by such a calculation; it is more tightly bound than we would expect
from what is called an “unsaturated double bond system.” Usually a double bond
system which is not in such a ring is easily attacked chemically because it has a
relatively high energy—the double bonds can be easily broken by the addition of
other hydrogens. But in benzene the ring is quite permanent and hard to break
up. In other words, benzene has a much lower energy than you would calculate
from the bond picture.
Then there is another mystery. Suppose we replace two adjacent hydrogens
by bromine atoms to make ortho-dibromobenzene. There are two ways to do
this, as shown in Fig. 10-7. The bromines could be on the opposite ends of a
double bond as shown in part (a) of the ﬁgure, or could be on the opposite ends
of a single bond as in (b). One would think that ortho-dibromobenzene should
have two diﬀerent forms, but it doesn’t. There is only one such chemical.†
Now we want to resolve these mysteries—and perhaps you have already
guessed how: by noticing, of course, that the “ground state” of the benzene ring
is really a two-state system. We could imagine that the bonds in benzene could
be in either of the two arrangements shown in Fig. 10-8. You say, “But they
are really the same; they should have the same energy.” Indeed, they should.
And for that reason they must be analyzed as a two-state system. Each state
| 1⟩
C
C
C
C
C
C
H
H
H
H
H
H
| 2⟩
C
C
C
C
C
C
H
H
H
H
H
H
Fig. 10-8. A set of base states for the benzene molecule.
† We are oversimplifying a little.
Originally, the chemists thought that there should
be four forms of dibromobenzene: two forms with the bromines on adjacent carbon atoms
(ortho-dibromobenzene), a third form with the bromines on next-nearest carbons (meta-
dibromobenzene), and a fourth form with the bromines opposite to each other (para-
dibromobenzene).
However, they found only three forms—there is only one form of the
ortho-molecule.
10-19","358"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","262","represents a diﬀerent conﬁguration of the whole set of electrons, and there is
some amplitude A that the whole bunch can switch from one arrangement to the
other—there is a chance that the electrons can ﬂip from one dance to the other.
As we have seen, this chance of ﬂipping makes a mixed state whose energy is
lower than you would calculate by looking separately at either of the two pictures
in Fig. 10-8. Instead, there are two stationary states—one with an energy above
and one with an energy below the expected value. So actually, the true normal
state (lowest energy) of benzene is neither of the possibilities shown in Fig. 10-8,
but it has the amplitude 1/
√
2 to be in each of the states shown. It is the
only state that is involved in the chemistry of benzene at normal temperatures.
Incidentally, the upper state also exists; we can tell it is there because benzene
has a strong absorption for ultraviolet light at the frequency ω = (EI −EII)/ℏ.
You will remember that in ammonia, where the object ﬂipping back and forth was
three protons, the energy separation was in the microwave region. In benzene,
the objects are electrons, and because they are much lighter, they ﬁnd it easier to
ﬂip back and forth, which makes the coeﬃcient A very much larger. The result
is that the energy diﬀerence is much larger—about 1.5 eV, which is the energy of
an ultraviolet photon.†
What happens if we substitute bromine? Again the two “possibilities” (a)
and (b) in Fig. 10-7 represent the two diﬀerent electron conﬁgurations. The only
diﬀerence is that the two base states we start with would have slightly diﬀerent
energies. The lowest energy stationary state will still involve a linear combination
of the two states, but with unequal amplitudes. The amplitude for state | 1⟩
might have a value something like
p
2/3, say, whereas state | 2⟩would have the
magnitude
p
1/3. We can’t say for sure without more information, but once the
two energies H11 and H22 are no longer equal, then the amplitudes C1 and C2
no longer have equal magnitudes. This means, of course, that one of the two
possibilities in the ﬁgure is more likely than the other, but the electrons are
mobile enough so that there is some amplitude for both. The other state has
† What we have said is a little misleading. Absorption of ultraviolet light would be very
weak in the two-state system we have taken for benzene, because the dipole moment matrix
element between the two states is zero. [The two states are electrically symmetric, so in our
formula Eq. (9.55) for the probability of a transition, the dipole moment µ is zero and no light
is absorbed.] If these were the only states, the existence of the upper state would have to be
shown in other ways. A more complete theory of benzene, however, which begins with more
base states (such as those having adjacent double bonds) shows that the true stationary states
of benzene are slightly distorted from the ones we have found. The resulting dipole moments
permit the transition we mentioned in the text to occur by the absorption of ultraviolet light.
10-20","540"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","263","diﬀerent amplitudes (like
p
1/3 and −
p
2/3) but lies at a higher energy. There
is only one lowest state, not two as the naive theory of ﬁxed chemical bonds
would suggest.
10-5 Dyes
We will give you one more chemical example of the two-state phenomenon—
this time on a larger molecular scale. It has to do with the theory of dyes. Many
dyes—in fact, most artiﬁcial dyes—have an interesting characteristic; they have
a kind of symmetry. Figure 10-9 shows an ion of a particular dye called magenta,
which has a purplish red color. The molecule has three ring structures—two of
which are benzene rings. The third is not exactly the same as a benzene ring
because it has only two double bonds inside the ring. The ﬁgure shows two
equally satisfactory pictures, and we would guess that they should have equal
energies. But there is a certain amplitude that all the electrons can ﬂip from
one condition to the other, shifting the position of the “unﬁlled” position to
the opposite end. With so many electrons involved, the ﬂipping amplitude is
somewhat lower than it is in the case of benzene, and the diﬀerence in energy
between the two stationary states is smaller. There are, nevertheless, the usual
two stationary states | I⟩and | II⟩which are the sum and diﬀerence combinations
of the two base states shown in the ﬁgure. The energy separation of | I⟩and | II⟩
comes out to be equal to the energy of a photon in the optical region. If one
| 1⟩
| 1⟩
H2N
C
NH2
+
| 2⟩
| 2⟩
H2N
C
NH2
+
Fig. 10-9. Two base states for the molecule of the dye magenta.
10-21","285"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","264","shines light on the molecule, there is a very strong absorption at one frequency,
and it appears to be brightly colored. That’s why it’s a dye!
Another interesting feature of such a dye molecule is that in the two base
states shown, the center of electric charge is located at diﬀerent places. As a
result, the molecule should be strongly aﬀected by an external electric ﬁeld. We
had a similar eﬀect in the ammonia molecule. Evidently we can analyze it by
using exactly the same mathematics, provided we know the numbers E0 and A.
Generally, these are obtained by gathering experimental data. If one makes
measurements with many dyes, it is often possible to guess what will happen
with some related dye molecule. Because of the large shift in the position of the
center of electric charge the value of µ in formula (9.55) is large and the material
has a high probability for absorbing light of the characteristic frequency 2A/ℏ.
Therefore, it is not only colored but very strongly so—a small amount of substance
absorbs a lot of light.
The rate of ﬂipping—and, therefore, A—is very sensitive to the complete
structure of the molecule. By changing A, the energy splitting, and with it
the color of the dye, can be changed. Also, the molecules do not have to be
perfectly symmetrical. We have seen that the same basic phenomenon exists
with slight modiﬁcations, even if there is some small asymmetry present. So, one
can get some modiﬁcation of the colors by introducing slight asymmetries in the
molecules. For example, another important dye, malachite green, is very similar
to magenta, but has two of the hydrogens replaced by CH3. It’s a diﬀerent color
because the A is shifted and the ﬂip-ﬂop rate is changed.
10-6 The Hamiltonian of a spin one-half particle in a magnetic ﬁeld
Now we would like to discuss a two-state system involving an object of spin
one-half. Some of what we will say has been covered in earlier chapters, but doing
it again may help to make some of the puzzling points a little clearer. We can
think of an electron at rest as a two-state system. Although we will be talking
in this section about “an electron,” what we ﬁnd out will be true for any spin
one-half particle. Suppose we choose for our base states | 1⟩and | 2⟩the states
in which the z-component of the electron spin is +ℏ/2 and −ℏ/2.
These states are, of course, the same ones we have called (+) and (−) in
earlier chapters. To keep the notation of this chapter consistent, though, we
call the “plus” spin state | 1⟩and the “minus” spin state | 2⟩—where “plus” and
“minus” refer to the angular momentum in the z-direction.
10-22","460"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","265","Any possible state ψ for the electron can be described as in Eq. (10.1) by
giving the amplitude C1 that the electron is in state | 1⟩, and the amplitude C2
that it is in state | 2⟩. To treat this problem, we will need to know the Hamiltonian
for this two-state system—that is, for an electron in a magnetic ﬁeld. We begin
with the special case of a magnetic ﬁeld in the z-direction.
Suppose that the vector B has only a z-component Bz. From the deﬁnition
of the two base states (that is, spins parallel and antiparallel to B) we know that
they are already stationary states with a deﬁnite energy in the magnetic ﬁeld.
State | 1⟩corresponds to an energy† equal to −µBz and state | 2⟩to +µBz. The
Hamiltonian must be very simple in this case since C1, the amplitude to be in
state | 1⟩, is not aﬀected by C2, and vice versa:
iℏdC1
dt = E1C1 = −µBzC1,
iℏdC2
dt = E2C2 = +µBzC2.
(10.17)
For this special case, the Hamiltonian is
H11 = −µBz,
H12 = 0,
H21 = 0,
H22 = +µBz.
(10.18)
So we know what the Hamiltonian is for the magnetic ﬁeld in the z-direction,
and we know the energies of the stationary states.
Now suppose the ﬁeld is not in the z-direction. What is the Hamiltonian?
How are the matrix elements changed if the ﬁeld is not in the z-direction? We are
going to make an assumption that there is a kind of superposition principle for
the terms of the Hamiltonian. More speciﬁcally, we want to assume that if two
magnetic ﬁelds are superposed, the terms in the Hamiltonian simply add—if we
know the Hij for a pure Bz and we know the Hij for a pure Bx, then the Hij for
both Bz and Bx together is simply the sum. This is certainly true if we consider
only ﬁelds in the z-direction—if we double Bz, then all the Hij are doubled. So
let’s assume that H is linear in the ﬁeld B. That’s all we need to be able to ﬁnd
the Hij for any magnetic ﬁeld.
† We are taking the rest energy m0c2 as our “zero” of energy and treating the magnetic
moment µ of the electron as a negative number, since it points opposite to the spin.
10-23","392"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","266","Suppose we have a constant ﬁeld B. We could have chosen our z-axis in its
direction, and we would have found two stationary states with the energies ∓µB.
Just choosing our axes in a diﬀerent direction won’t change the physics. Our
description of the stationary states will be diﬀerent, but their energies will still
be ∓µB—that is,
EI = −µ
q
B2x + B2y + B2z
and
(10.19)
EII = +µ
q
B2x + B2y + B2z.
The rest of the game is easy. We have here the formulas for the energies. We
want a Hamiltonian which is linear in Bx, By, and Bz, and which will give these
energies when used in our general formula of Eq. (10.3). The problem: ﬁnd the
Hamiltonian. First, notice that the energy splitting is symmetric, with an average
value of zero. Looking at Eq. (10.3), we can see directly that that requires
H22 = −H11.
(Note that this checks with what we already know when Bx and By are both
zero; in that case H11 = −µBz, and H22 = µBz.) Now if we equate the energies
of Eq. (10.3) with what we know from Eq. (10.19), we have
H11 −H22
2
2
+ |H12|2 = µ2(B2
x + B2
y + B2
z).
(10.20)
(We have also made use of the fact that H21 = H∗
12, so that H12H21 can also be
written as |H12|2.) Again for the special case of a ﬁeld in the z-direction, this
gives
µ2B2
z + |H12|2 = µ2B2
z.
Clearly, |H12| must be zero in this special case, which means that H12 cannot
have any terms in Bz. (Remember, we have said that all terms must be linear in
Bx, By, and Bz.)
So far, then, we have discovered that H11 and H22 have terms in Bz, while
H12 and H21 do not. We can make a simple guess that will satisfy Eq. (10.20) if
we say that
H11 = −µBz,
H22 = µBz,
(10.21)
10-24","331"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","267","and
|H12|2 = µ2(B2
x + B2
y).
And it turns out that that’s the only way it can be done!
“Wait”—you say—“H12 is not linear in B; Eq. (10.21) gives H12
=
µ
q
B2x + B2y.” Not necessarily. There is another possibility which is linear, namely,
H12 = µ(Bx + iBy).
There are, in fact, several such possibilities—most generally, we could write
H12 = µ(Bx ± iBy)eiδ,
where δ is some arbitrary phase. Which sign and phase should we use? It turns
out that you can choose either sign, and any phase you want, and the physical
results will always be the same. So the choice is a matter of convention. People
ahead of us have chosen to use the minus sign and to take eiδ = −1. We might
as well follow suit and write
H12 = −µ(Bx −iBy),
H21 = −µ(Bx + iBy).
(Incidentally, these conventions are related to, and consistent with, some of the
arbitrary choices we made in Chapter 6.)
The complete Hamiltonian for an electron in an arbitrary magnetic ﬁeld is,
then
H11 = −µBz,
H12 = −µ(Bx −iBy),
H21 = −µ(Bx + iBy),
H22 = +µBz.
(10.22)
And the equations for the amplitudes C1 and C2 are
iℏdC1
dt = −µ[BzC1 + (Bx −iBy)C2],
iℏdC2
dt = −µ[(Bx + iBy)C1 −BzC2].
(10.23)
So we have discovered the “equations of motion for the spin states” of an
electron in a magnetic ﬁeld. We guessed at them by making some physical
argument, but the real test of any Hamiltonian is that it should give predictions
10-25","262"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","268","in agreement with experiment. According to any tests that have been made,
these equations are right. In fact, although we made our arguments only for
constant ﬁelds, the Hamiltonian we have written is also right for magnetic ﬁelds
which vary with time. So we can now use Eq. (10.23) to look at all kinds of
interesting problems.
10-7 The spinning electron in a magnetic ﬁeld
Example number one: We start with a constant ﬁeld in the z-direction.
There are just the two stationary states with energies ∓µBz. Suppose we add
a small ﬁeld in the x-direction. Then the equations look like our old two-state
problem. We get the ﬂip-ﬂop business once more, and the energy levels are
split a little farther apart. Now let’s let the x-component of the ﬁeld vary with
time—say, as cos ωt. The equations are then the same as we had when we put
an oscillating electric ﬁeld on the ammonia molecule in Chapter 9. You can
work out the details in the same way. You will get the result that the oscillating
ﬁeld causes transitions from the +z-state to the −z-state—and vice versa—when
the horizontal ﬁeld oscillates near the resonant frequency ω0 = 2µBz/ℏ. This
gives the quantum mechanical theory of the magnetic resonance phenomena we
described in Chapter 35 of Volume II (see Appendix).
It is also possible to make a maser which uses a spin one-half system. A
Stern-Gerlach apparatus is used to produce a beam of particles polarized in, say,
the +z-direction, which are sent into a cavity in a constant magnetic ﬁeld. The
oscillating ﬁelds in the cavity can couple with the magnetic moment and induce
transitions which give energy to the cavity.
Now let’s look at the following question. Suppose we have a magnetic ﬁeld B
which points in the direction whose polar angle is θ and azimuthal angle is φ,
as in Fig. 10-10. Suppose, additionally, that there is an electron which has been
prepared with its spin pointing along this ﬁeld. What are the amplitudes C1
and C2 for such an electron? In other words, calling the state of the electron | ψ⟩,
we want to write
| ψ⟩= | 1⟩C1 + | 2⟩C2,
where C1 and C2 are
C1 = ⟨1 | ψ⟩,
C2 = ⟨2 | ψ⟩,
where by | 1⟩and | 2⟩we mean the same thing we used to call | +⟩and | −⟩
(referred to our chosen z-axis).
10-26","405"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","269","x
y
z
B
φ
θ
Fig. 10-10. The direction of B is deﬁned by the polar angle θ and the
azimuthal angle φ.
The answer to this question is also in our general equations for two-state
systems. First, we know that since the electron’s spin is parallel to B it is in
a stationary state with energy EI = −µB. Therefore, both C1 and C2 must
vary as e−iEIt/ℏ, as in (9.18); and their coeﬃcients a1 and a2 are given by (10.5),
namely,
a1
a2
=
H12
EI −H11
.
(10.24)
An additional condition is that a1 and a2 should be normalized so that |a1|2 +
|a2|2 = 1. We can take H11 and H12 from (10.22) using
Bz = B cos θ,
Bx = B sin θ cos φ,
By = B sin θ sin φ.
So we have
H11 = −µB cos θ,
H12 = −µB sin θ (cos φ −i sin φ).
(10.25)
The last factor in the second equation is, incidentally, e−iφ, so it is simpler to
write
H12 = −µB sin θ e−iφ.
(10.26)
10-27","181"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","270","Using these matrix elements in Eq. (10.24)—and canceling −µB from numer-
ator and denominator—we ﬁnd
a1
a2
= sin θ e−iφ
1 −cos θ .
(10.27)
With this ratio and the normalization condition, we can ﬁnd both a1 and a2.
That’s not hard, but we can make a short cut with a little trick. Notice that
1 −cos θ = 2 sin2 (θ/2), and that sin θ = 2 sin (θ/2) cos (θ/2). Then Eq. (10.27)
is equivalent to
a1
a2
=
cos θ
2 e−iφ
sin θ
2
.
(10.28)
So one possible answer is
a1 = cos θ
2 e−iφ,
a2 = sin θ
2,
(10.29)
since it ﬁts with (10.28) and also makes
|a1|2 + |a2|2 = 1.
As you know, multiplying both a1 and a2 by an arbitrary phase factor doesn’t
change anything. People generally prefer to make Eqs. (10.29) more symmetric
by multiplying both by eiφ/2. So the form usually used is
a1 = cos θ
2 e−iφ/2,
a2 = sin θ
2 e+iφ/2,
(10.30)
and this is the answer to our question. The numbers a1 and a2 are the amplitudes
to ﬁnd an electron with its spin up or down along the z-axis when we know that
its spin is along the axis at θ and φ. (The amplitudes C1 and C2 are just a1
and a2 times e−iEIt/ℏ.)
Now we notice an interesting thing. The strength B of the magnetic ﬁeld does
not appear anywhere in (10.30). The result is clearly the same in the limit that
B goes to zero. This means that we have answered in general the question of
how to represent a particle whose spin is along an arbitrary axis. The amplitudes
of (10.30) are the projection amplitudes for spin one-half particles corresponding
to the projection amplitudes we gave in Chapter 5 [Eqs. (5.38)] for spin-one
10-28","308"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","271","particles. We can now ﬁnd the amplitudes for ﬁltered beams of spin one-half
particles to go through any particular Stern-Gerlach ﬁlter.
Let | +z⟩represent a state with spin up along the z-axis, and | −z⟩represent
the spin down state. If | +z′⟩represents a state with spin up along a z′-axis
which makes the polar angles θ and φ with the z-axis, then in the notation of
Chapter 5, we have
⟨+z | +z′⟩= cos θ
2 e−iφ/2,
⟨−z | +z′⟩= sin θ
2 e+iφ/2.
(10.31)
These results are equivalent to what we found in Chapter 6, Eq. (6.36), by purely
geometrical arguments. (So if you decided to skip Chapter 6, you now have the
essential results anyway.)
As our ﬁnal example lets look again at one which we’ve already mentioned a
number of times. Suppose that we consider the following problem. We start with
an electron whose spin is in some given direction, then turn on a magnetic ﬁeld
in the z-direction for 25 minutes, and then turn it oﬀ. What is the ﬁnal state?
Again let’s represent the state by the linear combination | ψ⟩= | 1⟩C1 + | 2⟩C2.
For this problem, however, the states of deﬁnite energy are also our base states
| 1⟩and | 2⟩. So C1 and C2 only vary in phase. We know that
C1(t) = C1(0)e−iEIt/ℏ= C1(0)e+iµBt/ℏ,
and
C2(t) = C2(0)e−iEIIt/ℏ= C2(0)e−iµBt/ℏ.
Now initially we said the electron spin was set in a given direction. That means
that initially C1 and C2 are two numbers given by Eqs. (10.30).
After we
wait for a period of time T, the new C1 and C2 are the same two numbers
multiplied respectively by eiµBzT/ℏand e−iµBzT/ℏ. What state is that? That’s
easy. It’s exactly the same as if the angle φ had been changed by the subtraction
of 2µBzT/ℏand the angle θ had been left unchanged. That means that at the
end of the time T, the state | ψ⟩represents an electron lined up in a direction
which diﬀers from the original direction only by a rotation about the z-axis
through the angle ∆φ = 2µBzT/ℏ. Since this angle is proportional to T, we can
also say the direction of the spin precesses at the angular velocity 2µBz/ℏaround
the z-axis. This result we discussed several times previously in a less complete
and rigorous manner. Now we have obtained a complete and accurate quantum
mechanical description of the precession of atomic magnets.
10-29","406"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","272","B(t)
Spin
Rotates at ω(t)
Fig. 10-11. The spin direction of an electron in a varying magnetic
ﬁeld B(t) precesses at the frequency ω(t) about an axis parallel to B.
It is interesting that the mathematical ideas we have just gone over for the
spinning electron in a magnetic ﬁeld can be applied to any two-state system.
That means that by making a mathematical analogy to the spinning electron, any
problem about two-state systems can be solved by pure geometry. It works like
this. First you shift the zero of energy so that (H11 +H22) is equal to zero so that
H11 = −H22. Then any two-state problem is formally the same as the electron in
a magnetic ﬁeld. All you have to do is identify −µBz with H11 and −µ(Bx −iBy)
with H12. No matter what the physics is originally—an ammonia molecule, or
whatever—you can translate it into a corresponding electron problem. So if we
can solve the electron problem in general, we have solved all two-state problems.
And we have the general solution for the electron! Suppose you have some
state to start with that has spin “up” in some direction, and you have a magnetic
ﬁeld B that points in some other direction. You just rotate the spin direction
around the axis of B with the vector angular velocity ω(t) equal to a constant
times the vector B (namely ω = 2µB/ℏ). As B varies with time, you keep
moving the axis of the rotation to keep it parallel with B, and keep changing
the speed of rotation so that it is always proportional to the strength of B.
See Fig. 10-11. If you keep doing this, you will end up with a certain ﬁnal
orientation of the spin axis, and the amplitudes C1 and C2 are just given by
the projections—using (10.30)—into your coordinate frame. You see, it’s just
a geometric problem to keep track of where you end up after all the rotating.
10-30","329"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","273","Although it’s easy to see what’s involved, this geometric problem (of ﬁnding the
net result of a rotation with a varying angular velocity vector) is not easy to solve
explicitly in the general case. Anyway, we see, in principle, the general solution
to any two-state problem. In the next chapter we will look some more into
the mathematical techniques for handling the important case of a spin one-half
particle—and, therefore, for handling two-state systems in general.
10-31","77"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","274","11
More Two-State Systems
Review: Chapter 33, Vol. I, Polarization
11-1 The Pauli spin matrices
We continue our discussion of two-state systems. At the end of the last
chapter we were talking about a spin one-half particle in a magnetic ﬁeld. We
described the spin state by giving the amplitude C1 that the z-component of
spin angular momentum is +ℏ/2 and the amplitude C2 that it is −ℏ/2. In earlier
chapters we have called these base states | +⟩and | −⟩. We will now go back to
that notation, although we may occasionally ﬁnd it convenient to use | +⟩or | 1⟩,
and | −⟩or | 2⟩, interchangeably.
We saw in the last chapter that when a spin one-half particle with a magnetic
moment µ is in a magnetic ﬁeld B = (Bx, By, Bz), the amplitudes C+ (= C1)
and C−(= C2) are connected by the following diﬀerential equations:
iℏdC+
dt
= −µ[BzC+ + (Bx −iBy)C−],
iℏdC−
dt
= −µ[(Bx + iBy)C+ −BzC−].
(11.1)
In other words, the Hamiltonian matrix Hij is
H11 = −µBz,
H12 = −µ(Bx −iBy),
H21 = −µ(Bx + iBy),
H22 = +µBz.
(11.2)
And Eqs. (11.1) are, of course, the same as
iℏdCi
dt =
X
j
HijCj,
(11.3)
where i and j take on the values + and −(or 1 and 2).
11-1","220"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","275","The two-state system of the electron spin is so important that it is very useful
to have a neater way of writing things. We will now make a little mathematical
digression to show you how people usually write the equations of a two-state
system. It is done this way: First, note that each term in the Hamiltonian is
proportional to µ and to some component of B; we can then—purely formally—
write that
Hij = −µ[σx
ijBx + σy
ijBy + σz
ijBz].
(11.4)
There is no new physics here; this equation just means that the coeﬃcients σx
ij,
σy
ij, and σz
ij—there are 4 × 3 = 12 of them—can be ﬁgured out so that (11.4) is
identical with (11.2).
Let’s see what they have to be. We start with Bz. Since Bz appears only in
H11 and H22, everything will be O.K. if
σz
11 = 1,
σz
12 = 0,
σz
21 = 0,
σz
22 = −1.
We often write the matrix Hij as a little table like this:
Hij =
i↓
j→
H11
H12
H21
H22

.
For the Hamiltonian of a spin one-half particle in the magnetic ﬁeld Bz, this
is the same as
Hij =
i↓
j→
 
−µBz
−µ(Bx −iBy)
−µ(Bx + iBy)
+µBz
!
.
In the same way, we can write the coeﬃcients σz
ij as the matrix
σz
ij =
i↓
j→
1
0
0
−1

.
(11.5)
Working with the coeﬃcients of Bx, we get that the terms of σx have to be
σx
11 = 0,
σx
12 = 1,
σx
21 = 1,
σx
22 = 0.
11-2","274"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","276","Or, in shorthand,
σx
ij =

0
1
1
0

.
(11.6)
Finally, looking at By, we get
σy
11 = 0,
σy
12 = −i,
σy
21 = i,
σy
22 = 0;
or
σy
ij =
0
−i
i
0

.
(11.7)
With these three sigma matrices, Eqs. (11.2) and (11.4) are identical. To leave
room for the subscripts i and j, we have shown which σ goes with which component
of B by putting x, y, and z as superscripts. Usually, however, the i and j are
omitted—it’s easy to imagine they are there—and the x, y, z are written as
subscripts. Then Eq. (11.4) is written
H = −µ[σxBx + σyBy + σzBz].
(11.8)
Because the sigma matrices are so important—they are used all the time by the
professionals—we have gathered them together in Table 11-1. (Anyone who is
going to work in quantum physics really has to memorize them.) They are also
called the Pauli spin matrices after the physicist who invented them.
Table 11-1
The Pauli spin matrices
σz =

1
0
0
−1

σx =

0
1
1
0

σy =

0
−i
i
0

1 =

1
0
0
1

11-3","210"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","277","In the table we have included one more two-by-two matrix which is needed if
we want to be able to take care of a system which has two spin states of the same
energy, or if we want to choose a diﬀerent zero energy. For such situations we
must add E0C+ to the ﬁrst equation in (11.1) and E0C−to the second equation.
We can include this in the new notation if we deﬁne the unit matrix “1” as δij,
1 = δij =
1
0
0
1

,
(11.9)
and rewrite Eq. (11.8) as
H = E0δij −µ(σxBx + σyBy + σzBz).
(11.10)
Usually, it is understood that any constant like E0 is automatically to be multiplied
by the unit matrix; then one writes simply
H = E0 −µ(σxBx + σyBy + σzBz).
(11.11)
One reason the spin matrices are useful is that any two-by-two matrix at all
can be written in terms of them. Any matrix you can write has four numbers in
it, say,
M =

a
b
c
d

.
It can always be written as a linear combination of four matrices. For example,
M = a
1
0
0
0

+ b
0
1
0
0

+ c
0
0
1
0

+ d
0
0
0
1

.
There are many ways of doing it, but one special way is to say that M is a certain
amount of σx, plus a certain amount of σy, and so on, like this:
M = α1 + βσx + γσy + δσz,
where the “amounts” α, β, γ, and δ may, in general, be complex numbers.
Since any two-by-two matrix can be represented in terms of the unit matrix
and the sigma matrices, we have all that we ever need for any two-state system.
No matter what the two-state system—the ammonia molecule, the magenta
dye, anything—the Hamiltonian equation can be written in terms of the sigmas.
Although the sigmas seem to have a geometrical signiﬁcance in the physical
11-4","339"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","278","situation of an electron in a magnetic ﬁeld, they can also be thought of as just
useful matrices, which can be used for any two-state problem.
For instance, in one way of looking at things a proton and a neutron can
be thought of as the same particle in either of two states. We say the nucleon
(proton or neutron) is a two-state system—in this case, two states with respect
to its charge. When looked at that way, the | 1⟩state can represent the proton
and the | 2⟩state can represent the neutron. People say that the nucleon has
two “isotopic-spin” states.
Since we will be using the sigma matrices as the “arithmetic” of the quantum
mechanics of two-state systems, let’s review quickly the conventions of matrix
algebra. By the “sum” of any two or more matrices we mean just what was
obvious in Eq. (11.4). In general, if we “add” two matrices A and B, the “sum” C
means that each term Cij is given by
Cij = Aij + Bij.
Each term of C is the sum of the terms in the same slots of A and B.
In Section 5-6 we have already encountered the idea of a matrix “product.”
The same idea will be useful in dealing with the sigma matrices. In general, the
“product” of two matrices A and B (in that order) is deﬁned to be a matrix C
whose elements are
Cij =
X
k
AikBkj.
(11.12)
It is the sum of products of terms taken in pairs from the ith row of A and the
jth column of B. If the matrices are written out in tabular form as in Fig. 11-1,
there is a good “system” for getting the terms of the product matrix. Suppose
you are calculating C23. You run your left index ﬁnger along the second row of A
and your right index ﬁnger down the third column of B, multiplying each pair
and adding as you go. We have tried to indicate how to do it in the ﬁgure.
It is, of course, particularly simple for two-by-two matrices. For instance, if
we multiply σx times σx, we get
σ2
x = σx · σx =
0
1
1
0

·
0
1
1
0

=
1
0
0
1

,
which is just the unit matrix 1. Or, for another example, let’s work out σxσy:
σxσy =
0
1
1
0

·
0
−i
i
0

=
i
0
0
−i

.
11-5","421"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","279","










A11
A12
A13
A14
A21
A22
A23
A24
A31
A32
A33
A34
A41
A42
A43
A44











·











B11
B12
B13
B14
B21
B22
B23
B24
B31
B32
B33
B34
B41
B42
B43
B44











=











C11
C12
C13
C14
C21
C22
C23
C24
C31
C32
C33
C34
C41
C42
C43
C44











·
··
···
····
·
··
···
····
Cij =
X
k
AikBkj
Example: C23 = A21B13 + A22B23 + A23B33 + A24B43
Fig. 11-1. Multiplying two matrices.
11-6","145"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","280","Referring to Table 11-1, you see that the product is just i times the matrix σz.
(Remember that a number times a matrix just multiplies each term of the matrix.)
Since the products of the sigmas taken two at a time are important—as well as
rather amusing—we have listed them all in Table 11-2. You can work them out
as we have done for σ2
x and σxσy.
Table 11-2
Products of the spin matrices
σ2
x = 1
σ2
y = 1
σ2
z = 1
σxσy = −σyσx = iσz
σyσz = −σzσy = iσx
σzσx = −σxσz = iσy
There’s another very important and interesting point about these σ matrices.
We can imagine, if we wish, that the three matrices σx, σy, and σz are analogous
to the three components of a vector—it is sometimes called the “sigma vector”
and is written σ. It is really a “matrix vector” or a “vector matrix.” It is three
diﬀerent matrices—one matrix associated with each axis, x, y, and z. With it,
we can write the Hamiltonian of the system in a nice form which works in any
coordinate system:
H = −µσ · B.
(11.13)
Although we have written our three matrices in the representation in which
“up” and “down” are in the z-direction—so that σz has a particular simplicity—we
could ﬁgure out what the matrices would look like in some other representation.
Although it takes a lot of algebra, you can show that they change among them-
selves like the components of a vector. (We won’t, however, worry about proving
it right now. You can check it if you want.) You can use σ in diﬀerent coordinate
systems as though it is a vector.
You remember that the H is related to energy in quantum mechanics. It
is, in fact, just equal to the energy in the simple situation where there is only
one state. Even for two-state systems of the electron spin, when we write the
Hamiltonian as in Eq. (11.13), it looks very much like the classical formula for
11-7","344"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","281","the energy of a little magnet with magnetic moment µ in a magnetic ﬁeld B.
Classically, we would say
U = −µ · B,
(11.14)
where µ is the property of the object and B is an external ﬁeld. We can imagine
that Eq. (11.14) can be converted to (11.13) if we replace the classical energy by
the Hamiltonian and the classical µ by the matrix µσ. Then, after this purely
formal substitution, we interpret the result as a matrix equation. It is sometimes
said that to each quantity in classical physics there corresponds a matrix in
quantum mechanics. It is really more correct to say that the Hamiltonian matrix
corresponds to the energy, and any quantity that can be deﬁned via energy has a
corresponding matrix.
For example, the magnetic moment can be deﬁned via energy by saying
that the energy in an external ﬁeld B is −µ · B. This deﬁnes the magnetic
moment vector µ. Then we look at the formula for the Hamiltonian of a real
(quantum) object in a magnetic ﬁeld and try to identify whatever the matrices
are that correspond to the various quantities in the classical formula. That’s
the trick by which sometimes classical quantities have their quantum counter-
parts.
You may try, if you want, to understand how a classical vector is equal to
a matrix µσ, and maybe you will discover something—but don’t break your
head on it. That’s not the idea—they are not equal. Quantum mechanics is a
diﬀerent kind of a theory to represent the world. It just happens that there are
certain correspondences which are hardly more than mnemonic devices—things
to remember with. That is, you remember Eq. (11.14) when you learn classical
physics; then if you remember the correspondence µ →µσ, you have a handle
for remembering Eq. (11.13). Of course, nature knows the quantum mechanics,
and the classical mechanics is only an approximation; so there is no mystery in
the fact that in classical mechanics there is some shadow of quantum mechanical
laws—which are truly the ones underneath. To reconstruct the original object
from the shadow is not possible in any direct way, but the shadow does help
you to remember what the object looks like. Equation (11.13) is the truth, and
Eq. (11.14) is the shadow. Because we learn classical mechanics ﬁrst, we would
like to be able to get the quantum formula from it, but there is no sure-ﬁre
scheme for doing that. We must always go back to the real world and discover
the correct quantum mechanical equations. When they come out looking like
something in classical physics, we are in luck.
11-8","440"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","282","If the warnings above seem repetitious and appear to you to be belaboring
self-evident truths about the relation of classical physics to quantum physics,
please excuse the conditioned reﬂexes of a professor who has usually taught
quantum mechanics to students who hadn’t heard about Pauli spin matrices
until they were in graduate school. Then they always seemed to be hoping that,
somehow, quantum mechanics could be seen to follow as a logical consequence of
classical mechanics which they had learned thoroughly years before. (Perhaps they
wanted to avoid having to learn something new.) You have learned the classical
formula, Eq. (11.14), only a few months ago—and then with warnings that it was
inadequate—so maybe you will not be so unwilling to take the quantum formula,
Eq. (11.13), as the basic truth.
11-2 The spin matrices as operators
While we are on the subject of mathematical notation, we would like to
describe still another way of writing things—a way which is used very often
because it is so compact. It follows directly from the notation introduced in
Chapter 8. If we have a system in a state | ψ(t)⟩, which varies with time, we
can—as we did in Eq. (8.34)—write the amplitude that the system would be in
the state | i⟩at t + ∆t as
⟨i | ψ(t + ∆t)⟩=
X
j
⟨i | U(t + ∆t, t) | j⟩⟨j | ψ(t)⟩
The matrix element ⟨i | U(t + ∆t, t) | j⟩is the amplitude that the base state | j⟩
will be converted into the base state | i⟩in the time interval ∆t. We then deﬁned
Hij by writing
⟨i | U(t + ∆t, t) | j⟩= δij −i
ℏHij(t) ∆t,
and we showed that the amplitudes Ci(t) = ⟨i | ψ(t)⟩were related by the diﬀeren-
tial equations
iℏdCi
dt =
X
j
HijCj.
(11.15)
If we write out the amplitudes Ci explicitly, the same equation appears as
iℏd
dt ⟨i | ψ⟩=
X
j
Hij⟨j | ψ⟩.
(11.16)
11-9","331"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","283","Now the matrix elements Hij are also amplitudes which we can write as ⟨i | H | j⟩;
our diﬀerential equation looks like this:
iℏd
dt ⟨i | ψ⟩=
X
j
⟨i | H | j⟩⟨j | ψ⟩.
(11.17)
We see that −i/ℏ⟨i | H | j⟩dt is the amplitude that—under the physical conditions
described by H—a state | j⟩will, during the time dt, “generate” the state | i⟩.
(All of this is implicit in the discussion of Section 8-4.)
Now following the ideas of Section 8-2, we can drop out the common term ⟨i |
in Eq. (11.17)—since it is true for any state | i⟩—and write that equation simply
as
iℏd
dt | ψ⟩=
X
j
H | j⟩⟨j | ψ⟩.
(11.18)
Or, going one step further, we can also remove the j and write
iℏd
dt | ψ⟩= H | ψ⟩.
(11.19)
In Chapter 8 we pointed out that when things are written this way, the H in
H | j⟩or H | ψ⟩is called an operator. From now on we will put the little hat (ˆ)
over an operator to remind you that it is an operator and not just a number.
We will write ˆH | ψ⟩. Although the two equations (11.18) and (11.19) mean
exactly the same thing as Eq. (11.17) or Eq. (11.15), we can think about them
in a diﬀerent way. For instance, we would describe Eq. (11.18) in this way:
“The time derivative of the state vector | ψ⟩times iℏis equal to what you get
by operating with the Hamiltonian operator ˆH on each base state, multiplying
by the amplitude ⟨j | ψ⟩that ψ is in the state j, and summing over all j.” Or
Eq. (11.19) is described this way. “The time derivative (times iℏ) of a state | ψ⟩
is equal to what you get if you operate with the Hamiltonian ˆH on the state
vector | ψ⟩.” It’s just a shorthand way of saying what is in Eq. (11.17), but, as
you will see, it can be a great convenience.
If we wish, we can carry the “abstraction” idea one more step. Equation (11.19)
is true for any state | ψ⟩. Also the left-hand side, iℏd/dt, is also an operator—it’s
the operation “diﬀerentiate by t and multiply by iℏ.” Therefore, Eq. (11.19) can
also be thought of as an equation between operators—the operator equation
iℏd
dt = ˆH.
11-10","401"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","284","The Hamiltonian operator (within a constant) produces the same result as
does d/dt when acting on any state. Remember that this equation—as well
as Eq. (11.19)—is not a statement that the ˆH operator is just the identical
operation as iℏd/dt. The equations are the dynamical law of nature—the law of
motion—for a quantum system.
Just to get some practice with these ideas, we will show you another way we
could get to Eq. (11.18). You know that we can write any state | ψ⟩in terms of
its projections into some base set [see Eq. (8.8)],
| ψ⟩=
X
i
| i⟩⟨i | ψ⟩.
(11.20)
How does | ψ⟩change with time? Well, just take its derivative:
d
dt | ψ⟩= d
dt
X
i
| i⟩⟨i | ψ⟩.
(11.21)
Now the base states | i⟩do not change with time (at least we are always taking
them as deﬁnite ﬁxed states), but the amplitudes ⟨i | ψ⟩are numbers which may
vary. So Eq. (11.21) becomes
d
dt | ψ⟩=
X
i
| i⟩d
dt ⟨i | ψ⟩.
(11.22)
Since we know d⟨i | ψ⟩/dt from Eq. (11.16), we get
d
dt | ψ⟩= −i
ℏ
X
i
| i⟩
X
j
Hij⟨j | ψ⟩
= −i
ℏ
X
ij
| i⟩⟨i | H | j⟩⟨j | ψ⟩= −i
ℏ
X
j
H | j⟩⟨j | ψ⟩.
This is Eq. (11.18) all over again.
So we have many ways of looking at the Hamiltonian. We can think of
the set of coeﬃcients Hij as just a bunch of numbers, or we can think of the
“amplitudes” ⟨i | H | j⟩, or we can think of the “matrix” Hij, or we can think of
the “operator” ˆH. It all means the same thing.
Now let’s go back to our two-state systems. If we write the Hamiltonian in
terms of the sigma matrices (with suitable numerical coeﬃcients like Bx, etc.),
11-11","317"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","285","we can clearly also think of σx
ij as an amplitude ⟨i | σx | j⟩or, for short, as the
operator ˆσx. If we use the operator idea, we can write the equation of motion of
a state | ψ⟩in a magnetic ﬁeld as
iℏd
dt | ψ⟩= −µ(Bxˆσx + Byˆσy + Bzˆσz) | ψ⟩.
(11.23)
When we want to “use” such an equation we will normally have to express | ψ⟩
in terms of base vectors (just as we have to ﬁnd the components of space vectors
when we want speciﬁc numbers). So we will usually want to put Eq. (11.23) in
the somewhat expanded form:
iℏd
dt | ψ⟩= −µ
X
i
(Bxˆσx + Byˆσy + Bzˆσz) | i⟩⟨i | ψ⟩.
(11.24)
Now you will see why the operator idea is so neat. To use Eq. (11.24) we need
to know what happens when the ˆσ operators work on each of the base states.
Let’s ﬁnd out. Suppose we have ˆσz | +⟩; it is some vector | ?⟩, but what? Well,
let’s multiply it on the left by ⟨+ |; we have
⟨+ | ˆσz | +⟩= σz
11 = 1
(using Table 11-1). So we know that
⟨+ | ?⟩= 1.
(11.25)
Now let’s multiply ˆσz | +⟩on the left by ⟨−|. We get
⟨−| ˆσz | +⟩= σz
21 = 0;
so
⟨−| ?⟩= 0.
(11.26)
There is only one state vector that satisﬁes both (11.25) and (11.26); it is | +⟩.
We discover then that
ˆσz | +⟩= | +⟩.
(11.27)
By this kind of argument you can easily show that all of the properties of the
sigma matrices can be described in the operator notation by the set of rules given
in Table 11-3.
11-12","292"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","286","Table 11-3
Properties of the ˆσ-operator
ˆσz | +⟩= | +⟩
ˆσz | −⟩= −| −⟩
ˆσx | +⟩= | −⟩
ˆσx | −⟩= | +⟩
ˆσy | +⟩= i| −⟩
ˆσy | −⟩= −i| +⟩
If we have products of sigma matrices, they go over into products of operators.
When two operators appear together as a product, you carry out ﬁrst the operation
with the operator which is farthest to the right. For instance, by ˆσxˆσy | +⟩we
are to understand ˆσx(ˆσy | +⟩). From Table 11-3, we get ˆσy | +⟩= i | −⟩, so
ˆσxˆσy | +⟩= ˆσx(i | −⟩).
(11.28)
Now any number—like i—just moves through an operator (operators work only
on state vectors); so Eq. (11.28) is the same as
ˆσxˆσy | +⟩= iˆσx | −⟩= i | +⟩.
If you do the same thing for ˆσxˆσy | −⟩, you will ﬁnd that
ˆσxˆσy | −⟩= −i | −⟩.
Looking at Table 11-3, you see that ˆσxˆσy operating on | +⟩or | −⟩gives just what
you get if you operate with ˆσz and multiply by i. We can, therefore, say that the
operation ˆσxˆσy is identical with the operation iˆσz and write this statement as
an operator equation:
ˆσxˆσy = iˆσz.
(11.29)
Notice that this equation is identical with one of our matrix equations of Table 11-2.
So again we see the correspondence between the matrix and operator points of
view. Each of the equations in Table 11-2 can, therefore, also be considered as
equations about the sigma operators. You can check that they do indeed follow
from Table 11-3. It is best, when working with these things, not to keep track of
whether a quantity like σ or H is an operator or a matrix. All the equations are
the same either way, so Table 11-2 is for sigma operators, or for sigma matrices,
as you wish.
11-13","314"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","287","11-3 The solution of the two-state equations
We can now write our two-state equation in various forms, for example, either
as
iℏdCi
dt =
X
j
HijCj
or
iℏd | ψ⟩
dt
= ˆH | ψ⟩.
(11.30)
They both mean the same thing. For a spin one-half particle in a magnetic ﬁeld,
the Hamiltonian H is given by Eq. (11.8) or by Eq. (11.13).
If the ﬁeld is in the z-direction, then—as we have seen several times by now—
the solution is that the state | ψ⟩, whatever it is, precesses around the z-axis
(just as if you were to take the physical object and rotate it bodily around the
z-axis) at an angular velocity equal to twice the magnetic ﬁeld times µ/ℏ. The
same is true, of course, for a magnetic ﬁeld along any other direction, because
the physics is independent of the coordinate system. If we have a situation where
the magnetic ﬁeld varies from time to time in a complicated way, then we can
analyze the situation in the following way. Suppose you start with the spin in the
+z-direction and you have an x-magnetic ﬁeld. The spin starts to turn. Then if
the x-ﬁeld is turned oﬀ, the spin stops turning. Now if a z-ﬁeld is turned on, the
spin precesses about z, and so on. So depending on how the ﬁelds vary in time,
you can ﬁgure out what the ﬁnal state is—along what axis it will point. Then you
can refer that state back to the original | +⟩and | −⟩with respect to z by using
the projection formulas we had in Chapter 10 (or Chapter 6). If the state ends up
with its spin in the direction (θ, φ), it will have an up-amplitude cos (θ/2)e−iφ/2
and a down-amplitude sin (θ/2)e+iφ/2. That solves any problem. It is a word
description of the solution of the diﬀerential equations.
The solution just described is suﬃciently general to take care of any two-state
system. Let’s take our example of the ammonia molecule—including the eﬀects
of an electric ﬁeld. If we describe the system in terms of the states | I⟩and | II⟩,
the equations (9.38) and (9.39) look like this:
iℏdCI
dt = +ACI + µECII,
iℏdCII
dt
= −ACII + µECI.
(11.31)
11-14","378"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","288","You say, “No, I remember there was an E0 in there.” Well, we have shifted the
origin of energy to make the E0 zero. (You can always do that by changing both
amplitudes by the same factor—eiE0T/ℏ—and get rid of any constant energy.)
Now if corresponding equations always have the same solutions, then we really
don’t have to do it twice. If we look at these equations and look at Eq. (11.1),
then we can make the following identiﬁcation. Let’s call | I⟩the state | +⟩and
| II⟩the state | −⟩. That does not mean that we are lining-up the ammonia
in space, or that | +⟩and | −⟩has anything to do with the z-axis. It is purely
artiﬁcial. We have an artiﬁcial space that we might call the “ammonia molecule
representative space,” or something—a three-dimensional “diagram” in which
being “up” corresponds to having the molecule in the state | I⟩and being “down”
along this false z-axis represents having a molecule in the state | II⟩. Then, the
equations will be identiﬁed as follows. First of all, you see that the Hamiltonian
can be written in terms of the sigma matrices as
H = +Aσz + µEσx.
(11.32)
Or, putting it another way, µBz in Eq. (11.1) corresponds to −A in Eq. (11.32),
and µBx corresponds to −µE. In our “model” space, then, we have a constant
B ﬁeld along the z-direction. If we have an electric ﬁeld E which is changing with
time, then we have a B ﬁeld along the x-direction which varies in proportion.
So the behavior of an electron in a magnetic ﬁeld with a constant component in
the z-direction and an oscillating component in the x-direction is mathematically
analogous and corresponds exactly to the behavior of an ammonia molecule in
an oscillating electric ﬁeld. Unfortunately, we do not have the time to go any
further into the details of this correspondence, or to work out any of the technical
details. We only wished to make the point that all systems of two states can be
made analogous to a spin one-half object precessing in a magnetic ﬁeld.
11-4 The polarization states of the photon
There are a number of other two-state systems which are interesting to
study, and the ﬁrst new one we would like to talk about is the photon. To
describe a photon we must ﬁrst give its vector momentum. For a free photon, the
frequency is determined by the momentum, so we don’t have to say also what the
frequency is. After that, though, we still have a property called the polarization.
Imagine that there is a photon coming at you with a deﬁnite monochromatic
11-15","444"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","289","frequency (which will be kept the same throughout all this discussion so that
we don’t have a variety of momentum states). Then there are two directions of
polarization. In the classical theory, light can be described as having an electric
ﬁeld which oscillates horizontally or an electric ﬁeld which oscillates vertically
(for instance); these two kinds of light are called x-polarized and y-polarized light.
The light can also be polarized in some other direction, which can be made up
from the superposition of a ﬁeld in the x-direction and one in the y-direction.
Or if you take the x- and the y-components out of phase by 90◦, you get an
electric ﬁeld that rotates—the light is elliptically polarized. (This is just a quick
reminder of the classical theory of polarized light that we studied in Chapter 33,
Vol. I.)
Now, however, suppose we have a single photon—just one. There is no electric
ﬁeld that we can discuss in the same way. All we have is one photon. But a
photon has to have the analog of the classical phenomena of polarization. There
must be at least two diﬀerent kinds of photons. At ﬁrst, you might think there
should be an inﬁnite variety—after all, the electric vector can point in all sorts of
directions. We can, however, describe the polarization of a photon as a two-state
system. A photon can be in the state | x⟩or in the state | y⟩. By | x⟩we mean the
polarization state of each one of the photons in a beam of light which classically
is x-polarized light. On the other hand, by | y⟩we mean the polarization state of
each of the photons in a y-polarized beam. And we can take | x⟩and | y⟩as our
base states of a photon of given momentum pointing at you—in what we will
call the z-direction. So there are two base states | x⟩and | y⟩, and they are all
that are needed to describe any photon at all.
For example, if we have a piece of polaroid set with its axis to pass light
polarized in what we call the x-direction, and we send in a photon which we
know is in the state | y⟩, it will be absorbed by the polaroid. If we send in a
photon which we know is in the state | x⟩, it will come right through as | x⟩. If
we take a piece of calcite which takes a beam of polarized light and splits it
into an | x⟩beam and a | y⟩beam, that piece of calcite is the complete analog
of a Stern-Gerlach apparatus which splits a beam of silver atoms into the two
states | +⟩and | −⟩. So everything we did before with particles and Stern-Gerlach
apparatuses, we can do again with light and pieces of calcite. And what about
light ﬁltered through a piece of polaroid set at an angle θ? Is that another state?
Yes, indeed, it is another state. Let’s call the axis of the polaroid x′ to distinguish
it from the axes of our base states. See Fig. 11-2. A photon that comes out will be
in the state | x′⟩. However, any state can be represented as a linear combination
11-16","541"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","290","x
y
x′
y ′
θ
Fig. 11-2. Coordinates at right angles to the momentum vector of
the photon.
of base states, and the formula for the combination is, here,
| x′⟩= cos θ | x⟩+ sin θ | y⟩.
(11.33)
That is, if a photon comes through a piece of polaroid set at the angle θ (with
respect to x), it can still be resolved into | x⟩and | y⟩beams—by a piece of calcite,
for example. Or you can, if you wish, just analyze it into x- and y-components
in your imagination. Either way, you will ﬁnd the amplitude cos θ to be in the
| x⟩state and the amplitude sin θ to be in the | y⟩state.
Now we ask this question: Suppose a photon is polarized in the x′-direction
by a piece of polaroid set at the angle θ and arrives at a polaroid at the angle
zero—as in Fig. 11-3; what will happen?
With what probability will it get
through? The answer is the following. After it gets through the ﬁrst polaroid, it
is deﬁnitely in the state | x′⟩. The second polaroid will let the photon through
if it is in the state | x⟩(but absorb it if it is the state | y⟩). So we are asking
with what probability does the photon appear to be in the state | x⟩? We get
that probability from the absolute square of amplitude ⟨x | x′⟩that a photon in
the state | x′⟩is also in the state | x⟩. What is ⟨x | x′⟩? Just multiply Eq. (11.33)
by ⟨x | to get
⟨x | x′⟩= cos θ ⟨x | x⟩+ sin θ ⟨x | y⟩.
Now ⟨x | y⟩= 0, from the physics—as they must be if | x⟩and | y⟩are base
states—and ⟨x | x⟩= 1. So we get
⟨x | x′⟩= cos θ,
11-17","311"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","291","and the probability is cos2 θ. For example, if the ﬁrst polaroid is set at 30◦, a
photon will get through 3/4 of the time, and 1/4 of the time it will heat the
polaroid by being absorbed therein.
x
y
z
x
y
θ
LIGHT
AXIS OF POLARIZER
STATE | x′⟩
Fig. 11-3. Two sheets of polaroid with angle θ between planes of
polarization.
Now let us see what happens classically in the same situation. We would
have a beam of light with an electric ﬁeld which is varying in some way or
another—say “unpolarized.” After it gets through the ﬁrst polaroid, the electric
ﬁeld is oscillating in the x′-direction with a size E; we would draw the ﬁeld as an
oscillating vector with a peak value E0 in a diagram like Fig. 11-4. Now when
the light arrives at the second polaroid, only the x-component, E0 cos θ, of the
electric ﬁeld gets through. The intensity is proportional to the square of the ﬁeld
and, therefore, to E2
0 cos2 θ. So the energy coming through is cos2 θ weaker than
the energy which was entering the last polaroid.
The classical picture and the quantum picture give similar results. If you were
to throw 10 billion photons at the second polaroid, and the average probability of
each one going through is, say, 3/4, you would expect 3/4 of 10 billion would get
through. Likewise, the energy that they would carry would be 3/4 of the energy
that you attempted to put through. The classical theory says nothing about the
statistics of the thing—it simply says that the energy that comes through will
11-18","275"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","292","x
y
θ
E0
E0 cos θ
Fig. 11-4. The classical picture of the electric vector E.
be precisely 3/4 of the energy which you were sending in. That is, of course,
impossible if there is only one photon. There is no such thing as 3/4 of a photon.
It is either all there, or it isn’t there at all. Quantum mechanics tells us it is all
there 3/4 of the time. The relation of the two theories is clear.
What about the other kinds of polarization? For example, right-hand circular
polarization? In the classical theory, right-hand circular polarization has equal
components in x and y which are 90◦out of phase. In the quantum theory,
a right-hand circularly polarized (RHC) photon has equal amplitudes to be
polarized | x⟩or | y⟩, and the amplitudes are 90◦out of phase. Calling a RHC
photon a state | R⟩and a LHC photon a state | L⟩, we can write (see Vol. I,
Section 33-1)
| R⟩=
1
√
2 (| x⟩+ i | y⟩),
| L⟩=
1
√
2 (| x⟩−i | y⟩).
(11.34)
—the 1/
√
2 is put in to get normalized states. With these states you can calculate
any ﬁltering or interference eﬀects you want, using the laws of quantum theory. If
you want, you can also choose | R⟩and | L⟩as base states and represent everything
in terms of them. You only need to show ﬁrst that ⟨R | L⟩= 0—which you can
do by taking the conjugate form of the ﬁrst equation above [see Eq. (8.13)] and
multiplying it by the other. You can resolve light into x- and y-polarizations, or
into x′- and y′-polarizations, or into right and left polarizations as a basis.
11-19","287"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","293","Just as an example, let’s try to turn our formulas around. Can we represent
the state | x⟩as a linear combination of right and left? Yes, here it is:
| x⟩=
1
√
2 (| R⟩+ | L⟩),
| y⟩= −i
√
2 (| R⟩−| L⟩).
(11.35)
Proof: Add and subtract the two equations in (11.34). It is easy to go from
one base to the other.
One curious point has to be made, though. If a photon is right circularly
polarized, it shouldn’t have anything to do with the x- and y-axes. If we were
to look at the same thing from a coordinate system turned at some angle about
the direction of ﬂight, the light would still be right circularly polarized—and
similarly for left. The right and left circularly polarized light are the same for
any such rotation; the deﬁnition is independent of any choice of the x-direction
(except that the photon direction is given). Isn’t that nice—it doesn’t take any
axes to deﬁne it. Much better than x and y. On the other hand, isn’t it rather a
miracle that when you add the right and left together you can ﬁnd out which
direction x was? If “right” and “left” do not depend on x in any way, how is
it that we can put them back together again and get x? We can answer that
question in part by writing out the state | R′⟩, which represents a photon RHC
polarized in the frame x′, y′. In that frame, you would write
| R′⟩=
1
√
2 (| x′⟩+ i | y′⟩).
How does such a state look in the frame x, y? Just substitute | x′⟩from Eq. (11.33)
and the corresponding | y′⟩—we didn’t write it down, but it is (−sin θ) | x⟩+
(cos θ) | y⟩. Then
| R′⟩=
1
√
2 [cos θ | x⟩+ sin θ | y⟩−i sin θ | x⟩+ i cos θ | y⟩]
=
1
√
2 [(cos θ −i sin θ) | x⟩+ i(cos θ −i sin θ) | y⟩]
=
1
√
2 (| x⟩+ i | y⟩)(cos θ −i sin θ).
11-20","358"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","294","The ﬁrst term is just | R⟩, and the second is e−iθ; our result is that
| R′⟩= e−iθ | R⟩.
(11.36)
The states | R′⟩and | R⟩are the same except for the phase factor e−iθ. If you
work out the same thing for | L′⟩, you get that†
| L′⟩= e+iθ | L⟩.
(11.37)
Now you see what happens. If we add | R⟩and | L⟩, we get something diﬀerent
from what we get when we add | R′⟩and | L′⟩. For instance, an x-polarized
photon is [Eq. (11.35)] the sum of | R⟩and | L⟩, but a y-polarized photon is
the sum with the phase of one shifted 90◦backward and the other 90◦forward.
That is just what we would get from the sum of | R′⟩and | L′⟩for the special
angle θ = 90◦, and that’s right. An x-polarization in the prime frame is the same
as a y-polarization in the original frame. So it is not exactly true that a circularly
polarized photon looks the same for any set of axes. Its phase (the phase relation
of the right and left circularly polarized states) keeps track of the x-direction.
11-5 The neutral K-meson‡
We will now describe a two-state system in the world of the strange particles—
a system for which quantum mechanics gives a most remarkable prediction. To
describe it completely would involve us in a lot of stuﬀabout strange particles,
so we will, unfortunately, have to cut some corners. We can only give an outline
of how a certain discovery was made—to show you the kind of reasoning that
was involved. It begins with the discovery by Gell-Mann and Nishijima of the
concept of strangeness and of a new law of conservation of strangeness. It was
when Gell-Mann and Pais were analyzing the consequences of these new ideas
† It’s similar to what we found (in Chapter 6) for a spin one-half particle when we rotated
the coordinates about the z-axis—then we got the phase factors e±iφ/2. It is, in fact, exactly
what we wrote down in Section 5-7 for the | +⟩and | −⟩states of a spin-one particle—which is
no coincidence. The photon is a spin-one particle which has, however, no “zero” state.
‡ We now feel that the material of this section is longer and harder than is appropriate at
this point in our development. We suggest that you skip it and continue with Section 11-6.
If you are ambitious and have time you may wish to come back to it later. We leave it here,
because it is a beautiful example—taken from recent work in high-energy physics—of what can
be done with our formulation of the quantum mechanics of two-state systems.
11-21","450"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","295","that they came across the prediction of a most remarkable phenomenon we are
going to describe. First, though, we have to tell you a little about “strangeness.”
We must begin with what are called the strong interactions of nuclear particles.
These are the interactions which are responsible for the strong nuclear forces—as
distinct, for instance, from the relatively weaker electromagnetic interactions.
The interactions are “strong” in the sense that if two particles get close enough
to interact at all, they interact in a big way and produce other particles very
easily. The nuclear particles have also what is called a “weak interaction” by
which certain things can happen, such as beta decay, but always very slowly on
a nuclear time scale—the weak interactions are many, many orders of magnitude
weaker than the strong interactions and even much weaker than electromagnetic
interactions.
When the strong interactions were being studied with the big accelerators,
people were surprised to ﬁnd that certain things that “should” happen—that
were expected to happen—did not occur. For instance, in some interactions a
particle of a certain type did not appear when it was expected. Gell-Mann and
Nishijima noticed that many of these peculiar happenings could be explained
at once by inventing a new conservation law: the conservation of strangeness.
They proposed that there was a new kind of attribute associated with each
particle—which they called its “strangeness” number—and that in any strong
interaction the “quantity of strangeness” is conserved.
Suppose, for instance, that a high-energy negative K-meson—with, say, an
energy of many Bev—collides with a proton. Out of the interaction may come
many other particles: π-mesons, K-mesons, lambda particles, sigma particles—any
of the mesons or baryons listed in Table 2-2 of Vol. I. It is observed, however, that
only certain combinations appear, and never others. Now certain conservation
laws were already known to apply. First, energy and momentum are always
conserved. The total energy and momentum after an event must be the same as
before the event. Second, there is the conservation of electric charge which says
that the total charge of the outgoing particles must be equal to the total charge
carried by the original particles. In our example of a K-meson and a proton
coming together, the following reactions do occur:
K−+ p →p + K−+ π+ + π−+ π0
or
(11.38)
K−+ p →Σ−+ π+.
11-22","392"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","296","We would never get:
K−+ p →p + K−+ π+
or
K−+ p →Λ0 + π+,
(11.39)
because of the conservation of charge. It was also known that the number of
baryons is conserved. The number of baryons out must be equal to the number
of baryons in. For this law, an antiparticle of a baryon is counted as minus one
baryon. This means that we can—and do—see
K−+ p →Λ0 + π0
or
(11.40)
K−+ p →p + K−+ p + p
(where p is the antiproton, which carries a negative charge). But we never see
K−+ p →K−+ π+ + π0
or
(11.41)
K−+ p →p + K−+ n
(even when there is plenty of energy), because baryons would not be conserved.
These laws, however, do not explain the strange fact that the following
reactions—which do not immediately appear to be especially diﬀerent from some
of those in (11.38) or (11.40)—are also never observed:
K−+ p →p + K−+ K0
or
K−+ p →p + π−
(11.42)
or
K−+ p →Λ0 + K0.
The explanation is the conservation of strangeness. With each particle goes a
number—its strangeness S—and there is a law that in any strong interaction, the
total strangeness out must equal the total strangeness that went in. The proton
and antiproton (p, p), the neutron and antineutron (n, n), and the π-mesons
(π+, π0, π−) all have the strangeness number zero; the K+ and K0 mesons have
strangeness +1; the K−and K0 (the anti-K0),† the Λ0 and the Σ-particles (+,
† Read as: “K-naught-bar,” or “K-zero-bar.”
11-23","261"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","297","Table 11-4
The strangeness numbers of the strongly interacting particles
S
−2
−1
0
+1
Baryons
Σ+
p
Ξ0
Λ0, Σ0
n
Ξ−
Σ−
Mesons
π+
K+
K0
π0
K0
K−
π−
Note: The π−is the antiparticle of the π+ (or vice versa).
0, −) have strangeness −1. There is also a particle with strangeness −2—the
Ξ-particle (capital “ksi”)—and perhaps others as yet unknown. We have made a
list of these strangenesses in Table 11-4.
Let’s see how the strangeness conservation works in some of the reactions
we have written down. If we start with a K−and a proton, we have a total
strangeness of (−1 + 0) = −1. The conservation of strangeness says that the
strangeness of products after the reaction must also add up to −1. You see that
that is so for the reactions of (11.38) and (11.40). But in the reactions of (11.42)
the strangeness of the right-hand side is zero in each case. Such reactions do not
conserve strangeness, and do not occur. Why? Nobody knows. Nobody knows
any more than what we have just told you about this. Nature just works that way.
Now let’s look at the following reaction: a π−hits a proton. You might, for
instance, get a Λ0 particle plus a neutral K-particle—two neutral particles. Now
which neutral K do you get? Since the Λ-particle has a strangeness −1 and the
π and p+ have a strangeness zero, and since this is a fast production reaction,
the strangeness must not change. The K-particle must have strangeness +1—it
must therefore be the K0. The reaction is
π−+ p →Λ0 + K0,
with
S = 0 + 0 = −1 + +1
(conserved).
11-24","282"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","298","If the K0 were there instead of the K0, the strangeness on the right would be −2—
which nature does not permit, since the strangeness on the left side is zero. On
the other hand, a K0 can be produced in other reactions, such as
n + n →n + p + K0 + K+,
S = 0 + 0 = 0 + 0 + −1 + +1
or
K−+ p →n + K0,
S = −1 + 0 = 0 + −1.
You may be thinking, “That’s all a lot of stuﬀ, because how do you know
whether it is a K0 or a K0? They look exactly the same. They are antiparticles
of each other, so they have exactly the same mass, and both have zero electric
charge. How do you distinguish them?” By the reactions they produce. For
example, a K0 can interact with matter to produce a Λ-particle, like this:
K0 + p →Λ0 + π+,
but a K0 cannot. There is no way a K0 can produce a Λ-particle when it interacts
with ordinary matter (protons and neutrons).† So the experimental distinction
between the K0 and the K0 would be that one of them will and one of them will
not produce Λ’s.
One of the predictions of the strangeness theory is then this—if, in an
experiment with high-energy pions, a Λ-particle is produced with a neutral K-
meson, then that neutral K-meson going into other pieces of matter will never
produce a Λ. The experiment might run something like this. You send a beam
of π−-mesons into a large hydrogen bubble chamber. A π−track disappears, but
somewhere else a pair of tracks appear (a proton and a π−) indicating that a
Λ-particle has disintegrated‡—see Fig. 11-5. Then you know that there is a K0
somewhere which you cannot see.
† Except, of course, if it also produces two K+’s or other particles with a total strangeness
of +2. We can think here of reactions in which there is insuﬃcient energy to produce these
additional strange particles.
‡ The free Λ-particle decays slowly via a weak interaction (so strangeness need not be
conserved). The decay products are either a p and a π−, or an n and a π0. The lifetime
is 2.2 × 10−10 sec.
11-25","382"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","299","(a)
LIQUID HYDROGEN
π−
Λ0
p
π−
K0
π−
π+
NUCLEAR
INTERACTION
Λ0-decay
K0-decay
(b)
LIQUID HYDROGEN
K0
Λ0
π−
p
π+
NUCLEAR
INTERACTION
Fig. 11-5. High-energy events as seen in a hydrogen bubble chamber.
(a) A π−meson interacts with a hydrogen nucleus (proton) producing
a Λ0 particle and a K0 meson. Both particles decay in the chamber.
(b) A K0 meson interacts with a proton producing a π+ meson and a
Λ0 particle which then decays. (The neutral particles leave no tracks.
Their inferred trajectories are indicated here by light dashed lines.)
You can, however, ﬁgure out where it is going by using the conservation of
momentum and energy. [It could reveal itself later by disintegrating into two
charged particles, as shown in Fig. 11-5(a).] As the K0 goes ﬂying along, it may
interact with one of the hydrogen nuclei (protons), producing perhaps some other
particles. The prediction of the strangeness theory is that it will never produce
11-26","160"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","300","a Λ-particle in a simple reaction like, say,
K0 + p →Λ0 + π+,
although a K0 can do just that. That is, in a bubble chamber a K0 might produce
the event sketched in Fig. 11-5(b)—in which the Λ0 is seen because it decays—but
a K0 will not. That’s the ﬁrst part of our story. That’s the conservation of
strangeness.
The conservation of strangeness is, however, not perfect. There are very
slow disintegrations of the strange particles—decays taking a long† time like
10−10 second in which the strangeness is not conserved. These are called the
“weak” decays. For example, the K0 disintegrates into a pair of π-mesons (+
and −) with a lifetime of 10−10 second. That was, in fact, the way K-particles
were ﬁrst seen. Notice that the decay reaction
K0 →π+ + π−
does not conserve strangeness, so it cannot go “fast” by the strong interaction; it
can only go through the weak decay process.
Now the K0 also disintegrates in the same way—into a π+ and a π−—and
also with the same lifetime
K0 →π−+ π+.
Again we have a weak decay because it does not conserve strangeness. There is a
principle that for any reaction there is the corresponding reaction with “matter”
replaced by “antimatter” and vice versa. Since the K0 is the antiparticle of the K0,
it should decay into the antiparticles of the π+ and π−, but the antiparticle of
a π+ is the π−. (Or, if you prefer, vice versa. It turns out that for the π-mesons
it doesn’t matter which one you call “matter.”) So as a consequence of the weak
decays, the K0 and K0 can go into the same ﬁnal products. When “seen” through
their decays—as in a bubble chamber—they look like the same particle. Only
their strong interactions are diﬀerent.
At last we are ready to describe the work of Gell-Mann and Pais. They ﬁrst
noticed that since the K0 and the K0 can both turn into states of two π-mesons
there must be some amplitude that a K0 can turn into a K0, and also that a K0
can turn into a K0. Writing the reactions as one does in chemistry, we would
have
K0 ⇌π−+ π+ ⇌K0.
(11.43)
† A typical time for strong interactions is more like 10−23 sec.
11-27","385"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","301","These reactions imply that there is some amplitude per unit time, say −i/ℏ
times ⟨K0 | W | K0⟩, that a K0 will turn into a K0 through the weak interaction
responsible for the decay into two π-mesons. And there is the corresponding
amplitude ⟨K0 | W | K0⟩for the reverse process. Because matter and antimatter
behave in exactly the same way, these two amplitudes are numerically equal;
we’ll call them both A:
⟨K0 | W | K0⟩= ⟨K0 | W | K0⟩= A.
(11.44)
Now—said Gell-Mann and Pais—here is an interesting situation. What people
have been calling two distinct states of the world—the K0 and the K0—should
really be considered as one two-state system, because there is an amplitude to
go from one state to the other. For a complete treatment, one would, of course,
have to deal with more than two states, because there are also the states of 2π’s,
and so on; but since they were mainly interested in the relation of K0 and K0,
they did not have to complicate things and could make the approximation of a
two-state system. The other states were taken into account to the extent that
their eﬀects appeared implicitly in the amplitudes of Eq. (11.44).
Accordingly, Gell-Mann and Pais analyzed the neutral particle as a two-state
system. They began by choosing as their two base states the states | K0⟩and | K0⟩.
(From here on, the story goes very much as it did for the ammonia molecule.)
Any state | ψ⟩of the neutral K-particle could then be described by giving the
amplitudes that it was in either base state. We’ll call these amplitudes
C+ = ⟨K0 | ψ⟩,
C−= ⟨K0 | ψ⟩.
(11.45)
The next step was to write the Hamiltonian equations for this two-state
system. If there were no coupling between the K0 and the K0, the equations
would be simply
iℏdC+
dt
= E0C+,
iℏdC−
dt
= E0C−.
(11.46)
But since there is the amplitude ⟨K0 | W | K0⟩for the K0 to turn into a K0 there
should be the additional term
⟨K0 | W | K0⟩C−= AC−
11-28","352"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","302","added to the right-hand side of the ﬁrst equation. And similarly, the term AC+
should be inserted in the equation for the rate of change of C−.
But that’s not all. When the two-pion eﬀect is taken into account there is an
additional amplitude for the K0 to turn into itself through the process
K0 →π−+ π+ →K0.
The additional amplitude, which we would write ⟨K0 | W | K0⟩, is just equal to
the amplitude ⟨K0 | W | K0⟩, since the amplitudes to go to and from a pair of
π-mesons are identical for the K0 and the K0. If you wish, the argument can be
written out in detail like this. First write†
⟨K0 | W | K0⟩= ⟨K0 | W | 2π⟩⟨2π | W | K0⟩
and
⟨K0 | W | K0⟩= ⟨K0 | W | 2π⟩⟨2π | W | K0⟩.
Because of the symmetry of matter and antimatter
⟨2π | W | K0⟩= ⟨2π | W | K0⟩,
and also
⟨K0 | W | 2π⟩= ⟨K0 | W | 2π⟩.
It then follows that ⟨K0 | W | K0⟩= ⟨K0 | W | K0⟩, and also that ⟨K0 | W | K0⟩=
⟨K0 | W | K0⟩, as we said earlier. Anyway, there are the two additional amplitudes
⟨K0 | W | K0⟩and ⟨K0 | W | K0⟩, both equal to A, which should be included in
the Hamiltonian equations.
The ﬁrst gives a term AC+ on the right-hand
side of the equation for dC+/dt, and the second gives a new term AC−in the
equation for dC−/dt. Reasoning this way, Gell-Mann and Pais concluded that
the Hamiltonian equations for the K0 K0 system should be
iℏdC+
dt
= E0C+ + AC−+ AC+,
iℏdC−
dt
= E0C−+ AC+ + AC−.
(11.47)
† We are making a simpliﬁcation here. The 2π-system can have many states corresponding
to various momenta of the π-mesons, and we should make the right-hand side of this equation
into a sum over the various base states of the π’s. The complete treatment still leads to the
same conclusions.
11-29","346"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","303","We must now correct something we have said in earlier chapters: that two
amplitudes like ⟨K0 | W | K0⟩and ⟨K0 | W | K0⟩which are the reverse of each
other, are always complex conjugates. That was true when we were talking
about particles that did not decay. But if particles can decay—and can, therefore,
become “lost”—the two amplitudes are not necessarily complex conjugates. So
the equality of (11.44) does not mean that the amplitudes are real numbers; they
are in fact complex numbers. The coeﬃcient A is, therefore, complex; and we
can’t just incorporate it into the energy E0.
Having played often with electron spins and such, our heroes knew that the
Hamiltonian equations of (11.47) meant that there was another pair of base states
which could also be used to represent the K-particle system and which would
have especially simple behaviors. They said, “Let’s take the sum and diﬀerence
of these two equations. Also, let’s measure all our energies from E0, and use
units for energy and time that make ℏ= 1.” (That’s what modern theoretical
physicists always do. It doesn’t change the physics but makes the equations take
on a simple form.) Their result:
i d
dt (C+ + C−) = 2A(C+ + C−),
i d
dt (C+ −C−) = 0.
(11.48)
It is apparent that the combinations of amplitudes (C+ + C−) and (C+ −C−)
act independently from each other (corresponding, of course, to the stationary
states we have been studying earlier). So they concluded that it would be more
convenient to use a diﬀerent representation for the K-particle. They deﬁned the
two states
| K1⟩=
1
√
2 (| K0⟩+ | K0⟩),
| K2⟩=
1
√
2 (| K0⟩−| K0⟩).
(11.49)
They said that instead of thinking of the K0 and K0 mesons, we can equally
well think in terms of the two “particles” (that is, “states”) K1 and K2. (These
correspond, of course, to the states we have usually called | I⟩and | II⟩. We are
not using our old notation because we want now to follow the notation of the
original authors—and the one you will see in physics seminars.)
Now Gell-Mann and Pais didn’t do all this just to get diﬀerent names for the
particles—there is also some strange new physics in it. Suppose that C1 and C2
are the amplitudes that some state | ψ⟩will be either a K1 or a K2 meson:
C1 = ⟨K1 | ψ⟩,
C2 = ⟨K2 | ψ⟩.
11-30","413"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","304","From the equations of (11.49),
C1 =
1
√
2 (C+ + C−),
C2 =
1
√
2 (C+ −C−).
(11.50)
Then the Eqs. (11.48) become
i dC1
dt = 2AC1,
i dC2
dt = 0.
(11.51)
The solutions are
C1(t) = C1(0)e−i2At,
C2(t) = C2(0),
(11.52)
where, of course, C1(0) and C2(0) are the amplitudes at t = 0.
These equations say that if a neutral K-particle starts out in the state | K1⟩
at t = 0 [then C1(0) = 1 and C2(0) = 0], the amplitudes at the time t are
C1(t) = e−i2At,
C2(t) = 0.
Remembering that A is a complex number, it is convenient to take 2A = α−iβ.
(Since the imaginary part of 2A turns out to be negative, we write it as minus iβ.)
With this substitution, C1(t) reads
C1(t) = C1(0)e−βte−iαt.
(11.53)
The probability of ﬁnding a K1 particle at t is the absolute square of this amplitude,
which is e−2βt. And, from Eqs. (11.52), the probability of ﬁnding the K2 state at
any time is zero. That means that if you make a K-particle in the state | K1⟩, the
probability of ﬁnding it in the same state decreases exponentially with time—but
you will never ﬁnd it in state | K2⟩. Where does it go? It disintegrates into two
π-mesons with the mean life τ = 1/2β which is, experimentally, 10−10 sec. We
made provisions for that when we said that A was complex.
On the other hand, Eq. (11.52) says that if we make a K-particle completely
in the K2 state, it stays that way forever. Well, that’s not really true. It is
observed experimentally to disintegrate into three π-mesons, but 600 times slower
than the two-pion decay we have described. So there are some other small terms
we have left out in our approximation. But so long as we are considering only
the two-pion decay, the K2 lasts “forever.”
Now to ﬁnish the story of Gell-Mann and Pais. They went on to consider what
happens when a K-particle is produced with a Λ0 particle in a strong interaction.
11-31","351"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","305","Since it must then have a strangeness of +1, it must be produced in the K0 state.
So at t = 0 it is neither a K1 nor a K2 but a mixture. The initial conditions are
C+(0) = 1,
C−(0) = 0.
But that means—from Eq. (11.50)—that
C1(0) =
1
√
2,
C2(0) =
1
√
2,
and—from Eqs. (11.52) and (11.53)—that
C1(t) =
1
√
2 e−βte−iαt,
C2(t) =
1
√
2.
(11.54)
Now remember that K0 and K0 are each linear combinations of K1 and K2. In
Eqs. (11.54) the amplitudes have been chosen so that at t = 0 the K0 parts
cancel each other out by interference, leaving only a K0 state. But the | K1⟩state
changes with time, and the | K2⟩state does not. After t = 0 the interference of
C1 and C2 will give ﬁnite amplitudes for both K0 and K0.
What does all this mean? Let’s go back and think of the experiment we
sketched in Fig. 11-5. A π−meson has produced a Λ0 particle and a K0 meson
which is tooting along through the hydrogen in the chamber. As it goes along,
there is some small but uniform chance that it will collide with a hydrogen nucleus.
At ﬁrst, we thought that strangeness conservation would prevent the K-particle
from making a Λ0 in such an interaction. Now, however, we see that that is
not right. For although our K-particle starts out as a K0—which cannot make
a Λ0—it does not stay this way. After a while, there is some amplitude that it
will have ﬂipped to the K0 state. We can, therefore, sometimes expect to see
a Λ0 produced along the K-particle track. The chance of this happening is given
by the amplitude C−, which we can [by using Eq. (11.50)) backwards] relate to
C1 and C2. The relation is
C−=
1
√
2 (C1 −C2) = 1
2(e−βte−iαt −1).
(11.55)
As our K-particle goes along, the probability that it will “act like” a K0 is equal
to |C−|2, which is
|C−|2 = 1
4(1 + e−2βt −2e−βt cos αt).
(11.56)
A complicated and strange result!
11-32","357"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","306","This, then, is the remarkable prediction of Gell-Mann and Pais: when a K0
is produced, the chance that it will turn into a K0—as it can demonstrate by
being able to produce a Λ0—varies with time according to Eq. (11.56). This
prediction came from using only sheer logic and the basic principles of the
quantum mechanics—with no knowledge at all of the inner workings of the K-
particle. Since nobody knows anything about the inner machinery, that is as far
as Gell-Mann and Pais could go. They could not give any theoretical values for
α and β. And nobody has been able to do so to this date. They were able to
give a value of β obtained from the experimentally observed rate of decay into
two π’s (2β = 1010 sec−1), but they could say nothing about α.
We have plotted the function of Eq. (11.56) for two values of α in Fig. 11-6.
You can see that the form depends very much on the ratio of α to β. There is
no K0 probability at ﬁrst; then it builds up. If α is large, the probability would
have large oscillations. If α is small, there will be little or no oscillation—the
probability will just rise smoothly to 1/4.
Now, typically, the K-particle will be travelling at a constant speed near the
speed of light. The curves of Fig. 11-6 then also represent the probability along
the track of observing a K0—with typical distances of several centimeters. You
can see why this prediction is so remarkably peculiar. You produce a single
particle and instead of just disintegrating, it does something else. Sometimes
it disintegrates, and other times it turns into a diﬀerent kind of a particle. Its
characteristic probability of producing an eﬀect varies in a strange way as it goes
along. There is nothing else quite like it in nature. And this most remarkable
prediction was made solely by arguments about the interference of amplitudes.
If there is any place where we have a chance to test the main principles of
quantum mechanics in the purest way—does the superposition of amplitudes
work or doesn’t it?—this is it. In spite of the fact that this eﬀect has been
predicted now for several years, there is no experimental determination that is
very clear. There are some rough results which indicate that the α is not zero,
and that the eﬀect really occurs—they indicate that α is between 2β and 4β.
That’s all there is, experimentally. It would be very beautiful to check out the
curve exactly to see if the principle of superposition really still works in such a
mysterious world as that of the strange particles—with unknown reasons for the
decays, and unknown reasons for the strangeness.
The analysis we have just described is very characteristic of the way quantum
mechanics is being used today in the search for an understanding of the strange
particles. All the complicated theories that you may hear about are no more
11-33","499"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","307","0
0.25
0.50
0.75
1.0
0
0.5
1.0
1.5
2.0
t (10−10 sec)
(a)
|C−|2
α = 4πβ
2β = 1010 sec−1
0
0.25
0.50
0.75
0
2
4
6
8
t (10−10 sec)
(b)
|C−|2
α = πβ
2β = 1010 sec−1
Fig. 11-6. The function of Eq. (11.56): (a) for α = 4πβ, (b) for
α = πβ (with 2β = 1010 sec−1).
and no less than this kind of elementary hocus-pocus using the principles of
superposition and other principles of quantum mechanics of that level. Some
people claim that they have theories by which it is possible to calculate the β
and α, or at least the α given the β, but these theories are completely useless.
For instance, the theory that predicts the value of α, given the β, tells us that
11-34","137"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","308","the value of α should be inﬁnite. The set of equations with which they originally
start involves two π-mesons and then goes from the two π’s back to a K0, and so
on. When it’s all worked out, it does indeed produce a pair of equations like the
ones we have here; but because there are an inﬁnite number of states of two π’s,
depending on their momenta, integrating over all the possibilities gives an α
which is inﬁnite. But nature’s α is not inﬁnite. So the dynamical theories are
wrong. It is really quite remarkable that the phenomena which can be predicted
at all in the world of the strange particles come from the principles of quantum
mechanics at the level at which you are learning them now.
11-6 Generalization to N-state systems
We have ﬁnished with all the two-state systems we wanted to talk about. In
the following chapters we will go on to study systems with more states. The
extension to N-state systems of the ideas we have worked out for two states is
pretty straightforward. It goes like this.
If a system has N distinct states, we can represent any state | ψ(t)⟩as a linear
combination of any set of base states | i⟩, where i = 1, 2, 3, . . . , N;
| ψ(t)⟩=
X
all i
| i⟩Ci(t).
(11.57)
The coeﬃcients Ci(t) are the amplitudes ⟨i | ψ(t)⟩. The behavior of the ampli-
tudes Ci with time is governed by the equations
iℏdCi(t)
dt
=
X
j
HijCj,
(11.58)
where the energy matrix Hij describes the physics of the problem. It looks the
same as for two states. Only now, both i and j must range over all N base
states, and the energy matrix Hij—or, if you prefer, the Hamiltonian—is an N
by N matrix with N 2 numbers. As before, H∗
ij = Hji—so long as particles are
conserved—and the diagonal elements Hii are real numbers.
We have found a general solution for the C’s of a two-state system when the
energy matrix is constant (doesn’t depend on t). It is also not diﬃcult to solve
Eq. (11.58) for an N-state system when H is not time dependent. Again, we
begin by looking for a possible solution in which the amplitudes all have the
11-35","385"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","309","same time dependence. We try
Ci = aie−(i/ℏ)Et.
(11.59)
When these Ci’s are substituted into (11.58), the derivatives dCi(t)/dt become
just (−i/ℏ)ECi. Canceling the common exponential factor from all terms, we get
Eai =
X
j
Hijaj.
(11.60)
This is a set of N linear algebraic equations for the N unknowns a1, a2, . . . , aN,
and there is a solution only if you are lucky—only if the determinant of the
coeﬃcients of all the a’s is zero. But it’s not necessary to be that sophisticated;
you can just start to solve the equations any way you want, and you will ﬁnd
that they can be solved only for certain values of E. (Remember that E is the
only adjustable thing we have in the equations.)
If you want to be formal, however, you can write Eq. (11.60) as
X
j
(Hij −δijE)aj = 0.
(11.61)
Then you can use the rule—if you know it—that these equations will have a
solution only for those values of E for which
Det (Hij −δijE) = 0.
(11.62)
Each term of the determinant is just Hij, except that E is subtracted from every
diagonal element. That is, (11.62) means just
Det





H11 −E
H12
H13
. . .
H21
H22 −E
H23
. . .
H31
H32
H33 −E
. . .
. . .
. . .
. . .
. . .




= 0.
(11.63)
This is, of course, just a special way of writing an algebraic equation for E which
is the sum of a bunch of products of all the terms taken a certain way. These
products will give all the powers of E up to EN.
11-36","290"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","310","So we have an Nth order polynomial equal to zero, and there are, in general,
N roots. (We must remember, however, that some of them may be multiple
roots—meaning that two or more roots are equal.) Let’s call the N roots
EI, EII, EIII, . . . , En, . . . , EN.
(11.64)
(We will use n to represent the nth Roman numeral, so that n takes on the
values I, II, . . . , N.) It may be that some of these energies are equal—say
EII = EIII—but we will still choose to call them by diﬀerent names.
The equations (11.60)—or (11.61)—have one solution for each value of E. If
you put any one of the E’s—say En—into (11.60) and solve for the ai, you get a
set which belongs to the energy En. We will call this set ai(n).
Using these ai(n) in Eq. (11.59), we have the amplitudes Ci(n) that the
deﬁnite energy states are in the base state | i⟩. Letting | n⟩stand for the state
vector of the deﬁnite energy state at t = 0, we can write
Ci(n) = ⟨i | n⟩e−(i/ℏ)Ent,
with
⟨i | n⟩= ai(n).
(11.65)
The complete deﬁnite energy state | ψn(t)⟩can then be written as
| ψn(t)⟩=
X
i
| i⟩ai(n)e−(i/ℏ)Ent,
or
| ψn(t)⟩= | n⟩e−(i/ℏ)Ent.
(11.66)
The state vectors | n⟩describe the conﬁguration of the deﬁnite energy states, but
have the time dependence factored out. Then they are constant vectors which
can be used as a new base set if we wish.
Each of the states | n⟩has the property—as you can easily show—that when
operated on by the Hamiltonian operator ˆH it gives just En times the same state:
ˆH | n⟩= En | n⟩.
(11.67)
The energy En is, then, a number which is a characteristic of the Hamiltonian
operator ˆH.
As we have seen, a Hamiltonian will, in general, have several
characteristic energies. In the mathematician’s world these would be called
11-37","329"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","311","the “characteristic values” of the matrix Hij. Physicists usually call them the
“eigenvalues” of ˆH. (“Eigen” is the German word for “characteristic” or “proper.”)
With each eigenvalue of ˆH—in other words, for each energy—there is the state of
deﬁnite energy, which we have called the “stationary state.” Physicists usually call
the states | n⟩“the eigenstates of ˆH.” Each eigenstate corresponds to a particular
eigenvalue En.
Now, generally, the states | n⟩—of which there are N—can also be used as a
base set. For this to be true, all of the states must be orthogonal, meaning that
for any two of them, say | n⟩and | m⟩,
⟨n | m⟩= 0.
(11.68)
This will be true automatically if all the energies are diﬀerent. Also, we can
multiply all the ai(n) by a suitable factor so that all the states are normalized—by
which we mean that
⟨n | n⟩= 1
(11.69)
for all n.
When it happens that Eq. (11.63) accidentally has two (or more) roots with
the same energy, there are some minor complications. First, there are still two
diﬀerent sets of ai’s which go with the two equal energies, but the states they
give may not be orthogonal. Suppose you go through the normal procedure and
ﬁnd two stationary states with equal energies—let’s call them | µ⟩and | ν⟩. Then
it will not necessarily be so that they are orthogonal—if you are unlucky,
⟨µ | ν⟩̸= 0.
It is, however, always true that you can cook up two new states, which we will
call | µ′⟩and | ν′⟩, that have the same energies and are also orthogonal, so that
⟨µ′ | ν′⟩= 0.
(11.70)
You can do this by making | µ′⟩and | ν′⟩a suitable linear combination of | µ⟩
and | ν⟩, with the coeﬃcients chosen to make it come out so that Eq. (11.70) is
true. It is always convenient to do this. We will generally assume that this has
been done so that we can always assume that our proper energy states | n⟩are
all orthogonal.
We would like, for fun, to prove that when two of the stationary states
have diﬀerent energies they are indeed orthogonal. For the state | n⟩with the
11-38","367"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","312","energy En, we have that
ˆH | n⟩= En | n⟩.
(11.71)
This operator equation really means that there is an equation between numbers.
Filling the missing parts, it means the same as
X
j
⟨i | ˆH | j⟩⟨j | n⟩= En⟨i | n⟩.
(11.72)
If we take the complex conjugate of this equation, we get
X
j
⟨i | ˆH | j⟩∗⟨j | n⟩∗= E∗
n⟨i | n⟩∗.
(11.73)
Remember now that the complex conjugate of an amplitude is the reverse ampli-
tude, so (11.73) can be rewritten as
X
j
⟨n | j⟩⟨j | ˆH | i⟩= E∗
n⟨n | i⟩.
(11.74)
Since this equation is valid for any i, its “short form” is
⟨n | ˆH = E∗
n⟨n |,
(11.75)
which is called the adjoint to Eq. (11.71).
Now we can easily prove that En is a real number. We multiply Eq. (11.71)
by ⟨n | to get
⟨n | ˆH | n⟩= En,
(11.76)
since ⟨n | n⟩= 1. Then we multiply Eq. (11.75) on the right by | n⟩to get
⟨n | ˆH | n⟩= E∗
n.
(11.77)
Comparing (11.76) with (11.77) it is clear that
En = E∗
n,
(11.78)
which means that En is real. We can erase the star on En in Eq. (11.75).
Finally we are ready to show that the diﬀerent energy states are orthogonal.
Let | n⟩and | m⟩be any two of the deﬁnite energy base states. Using Eq. (11.75)
for the state m, and multiplying it by | n⟩, we get that
⟨m | ˆH | n⟩= Em⟨m | n⟩.
11-39","265"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","313","But if we multiply (11.71) by ⟨m |, we get
⟨m | ˆH | n⟩= En⟨m | n⟩.
Since the left sides of these two equations are equal, the right sides are, also:
Em⟨m | n⟩= En⟨m | n⟩.
(11.79)
If Em = En the equation does not tell us anything. But if the energies of the
two states | m⟩and | n⟩are diﬀerent (Em ̸= En), Eq. (11.79) says that ⟨m | n⟩
must be zero, as we wanted to prove. The two states are necessarily orthogonal
so long as En and Em are numerically diﬀerent.
11-40","98"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","314","12
The Hyperfine Splitting in Hydrogen
12-1 Base states for a system with two spin one-half particles
In this chapter we take up the “hyperﬁne splitting” of hydrogen, because
it is a physically interesting example of what we can already do with quantum
mechanics. It’s an example with more than two states, and it will be illustrative
of the methods of quantum mechanics as applied to slightly more complicated
problems. It is enough more complicated that once you see how this one is
handled you can get immediately the generalization to all kinds of problems.
As you know, the hydrogen atom consists of an electron sitting in the neigh-
borhood of the proton, where it can exist in any one of a number of discrete
energy states in each one of which the pattern of motion of the electron is diﬀerent.
The ﬁrst excited state, for example, lies 3/4 of a Rydberg, or about 10 electron
volts, above the ground state. But even the so-called ground state of hydrogen
is not really a single, deﬁnite-energy state, because of the spins of the electron
and the proton. These spins are responsible for the “hyperﬁne structure” in the
energy levels, which splits all the energy levels into several nearly equal levels.
The electron can have its spin either “up” or “down” and, the proton can
also have its spin either “up” or “down.” There are, therefore, four possible spin
states for every dynamical condition of the atom. That is, when people say “the
ground state” of hydrogen, they really mean the “four ground states,” and not
just the very lowest state. The four spin states do not all have exactly the same
energy; there are slight shifts from the energies we would expect with no spins.
The shifts are, however, much, much smaller than the 10 electron volts or so from
the ground state to the next state above. As a consequence, each dynamical state
has its energy split into a set of very close energy levels—the so-called hyperﬁne
splitting.
The energy diﬀerences among the four spin states is what we want to calculate
in this chapter. The hyperﬁne splitting is due to the interaction of the magnetic
12-1","367"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","315","moments of the electron and proton, which gives a slightly diﬀerent magnetic
energy for each spin state. These energy shifts are only about ten-millionths of
an electron volt—really very small compared with 10 electron volts! It is because
of this large gap that we can think about the ground state of hydrogen as a
“four-state” system, without worrying about the fact that there are really many
more states at higher energies. We are going to limit ourselves here to a study of
the hyperﬁne structure of the ground state of the hydrogen atom.
For our purposes we are not interested in any of the details about the positions
of the electron and proton because that has all been worked out by the atom so
to speak—it has worked itself out by getting into the ground state. We need
know only that we have an electron and proton in the neighborhood of each
other with some deﬁnite spatial relationship. In addition, they can have various
diﬀerent relative orientations of their spins. It is only the eﬀect of the spins that
we want to look into.
The ﬁrst question we have to answer is: What are the base states for the
system? Now the question has been put incorrectly. There is no such thing as
“the” base states, because, of course, the set of base states you may choose is
not unique. New sets can always be made out of linear combinations of the old.
There are always many choices for the base states, and among them, any choice
is equally legitimate. So the question is not what is the base set, but what could
a base set be? We can choose any one we wish for our own convenience. It is
usually best to start with a base set which is physically the clearest. It may not
be the solution to any problem, or may not have any direct importance, but it
will generally make it easier to understand what is going on.
We choose the following four base states:
State 1: The electron and proton are both spin “up.”
State 2: The electron is “up” and the proton is “down.”
State 3: The electron is “down” and the proton is “up.”
State 4: The electron and proton are both “down.”
We need a handy notation for these four states, so we’ll represent them this way:
State 1: | + +⟩; electron up, proton up.
State 2: | + −⟩; electron up, proton down.
State 3: | −+⟩; electron down, proton up.
State 4: | −−⟩; electron down, proton down.
(12.1)
12-2","431"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","316","PROTON
ELECTRON
| + +⟩, | 1⟩
| + −⟩, | 2⟩
| −+⟩, | 3⟩
| −−⟩, | 4⟩
Fig. 12-1. A set of base states for the ground state of the hydrogen
atom.
You will have to remember that the ﬁrst plus or minus sign refers to the electron
and the second, to the proton. For handy reference, we’ve also summarized the
notation in Fig. 12-1. Sometimes it will also be convenient to call these states
| 1⟩, | 2⟩, | 3⟩, and | 4⟩.
You may say, “But the particles interact, and maybe these aren’t the right base
states. It sounds as though you are considering the two particles independently.”
Yes, indeed! The interaction raises the problem: what is the Hamiltonian for the
system, but the interaction is not involved in the question of how to describe the
system. What we choose for the base states has nothing to do with what happens
next. It may be that the atom cannot ever stay in one of these base states, even
if it is started that way. That’s another question. That’s the question: How do
the amplitudes change with time in a particular (ﬁxed) base? In choosing the
base states, we are just choosing the “unit vectors” for our description.
While we’re on the subject, let’s look at the general problem of ﬁnding a set
of base states when there is more than one particle. You know the base states
for a single particle. An electron, for example, is completely described in real
life—not in our simpliﬁed cases, but in real life—by giving the amplitudes to be
in each of the following states:
| electron “up” with momentum p⟩
or
| electron “down” with momentum p⟩.
12-3","289"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","317","There are really two inﬁnite sets of states, one state for each value of p. That
is to say that an electron state | ψ⟩is completely described if you know all the
amplitudes
⟨+, p | ψ⟩
and
⟨−, p | ψ⟩,
where the + and −represent the components of angular momentum along some
axis—usually the z-axis—and p is the vector momentum. There must, therefore,
be two amplitudes for every possible momentum (a multi-inﬁnite set of base
states). That is all there is to describing a single particle.
When there is more than one particle, the base states can be written in a
similar way. For instance, if there were an electron and a proton in a more
complicated situation than we are considering, the base states could be of the
following kind:
| an electron with spin “up,” moving with momentum p1 and
a proton with spin “down,” moving with momentum p2⟩.
And so on for other spin combinations. If there are more than two particles—same
idea. So you see that to write down the possible base states is really very easy.
The only problem is, what is the Hamiltonian?
For our study of the ground state of hydrogen we don’t need to use the full sets
of base states for the various momenta. We are specifying particular momentum
states for the proton and electron when we say “the ground state.” The details
of the conﬁguration—the amplitudes for all the momentum base states—can be
calculated, but that is another problem. Now we are concerned only with the
eﬀects of the spin, so we can take only the four base states of (12.1). Our next
problem is: What is the Hamiltonian for this set of states?
12-2 The Hamiltonian for the ground state of hydrogen
We’ll tell you in a moment what it is. But ﬁrst, we should remind you of one
thing: any state can always be written as a linear combination of the base states.
For any state | ψ⟩we can write
| ψ⟩= | + +⟩⟨+ + | ψ⟩+ | + −⟩⟨+ −| ψ⟩+ | −+⟩⟨−+ | ψ⟩
+ | −−⟩⟨−−| ψ⟩.
(12.2)
12-4","357"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","318","Remember that the complete brackets are just complex numbers, so we can also
write them in the usual fashion as Ci, where i = 1, 2, 3, or 4, and write Eq. (12.2)
as
| ψ⟩= | + +⟩C1 + | + −⟩C2 + | −+⟩C3 + | −−⟩C4.
(12.3)
By giving the four amplitudes Ci we completely describe the spin state | ψ⟩. If
these four amplitudes change with time, as they will, the rate of change in time
is given by the operator ˆH. The problem is to ﬁnd the ˆH.
There is no general rule for writing down the Hamiltonian of an atomic
system, and ﬁnding the right formula is much more of an art than ﬁnding a set
of base states. We were able to tell you a general rule for writing a set of base
states for any problem of a proton and an electron, but to describe the general
Hamiltonian of such a combination is too hard at this level. Instead, we will lead
you to a Hamiltonian by some heuristic argument—and you will have to accept
it as the correct one because the results will agree with the test of experimental
observation.
You will remember that in the last chapter we were able to describe the
Hamiltonian of a single, spin one-half particle by using the sigma matrices—
or the exactly equivalent sigma operators.
The properties of the operators
are summarized in Table 12-1. These operators—which are just a convenient,
shorthand way of keeping track of the matrix elements of the type ⟨+ | σz | +⟩—
were useful for describing the behavior of a single particle of spin one-half. The
question is: Can we ﬁnd an analogous device to describe a system with two
spins? The answer is yes, very simply, as follows. We invent a thing which we
will call “sigma electron,” which we represent by the vector operator σe, and
which has the x-, y-, and z-components, σe
x, σe
y, σe
z. We now make the convention
that when one of these things operates on any one of our four base states of the
Table 12-1
σz | +⟩= + | +⟩
σz | −⟩= −| −⟩
σx | +⟩= + | −⟩
σx | −⟩= + | +⟩
σy | +⟩= +i | −⟩
σy | −⟩= −i | +⟩
12-5","393"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","319","hydrogen atom, it acts only on the electron spin, and in exactly the same way as
if the electron were all by itself. Example: What is σe
y | −+⟩? Since σy on an
electron “down” is −i times the corresponding state with the electron “up”,
σe
y | −+⟩= −i | + +⟩.
(When σe
y acts on the combined state it ﬂips over the electron, but does nothing
to the proton and multiplies the result by −i.) Operating on the other states, σe
y
would give
σe
y | + +⟩= i | −+⟩,
σe
y | + −⟩= i | −−⟩,
σe
y | −−⟩= −i | + −⟩.
Just remember that the operators σe work only on the ﬁrst spin symbol—that
is, on the electron spin.
Next we deﬁne the corresponding operator “sigma proton” for the proton
spin. Its three components σp
x, σp
y, σp
z act in the same way as σe, only on the
proton spin. For example, if we have σp
x acting on each of the four base states,
we get—always using Table 12-1—
σp
x | + +⟩= | + −⟩,
σp
x | + −⟩= | + +⟩,
σp
x | −+⟩= | −−⟩,
σp
x | −−⟩= | −+⟩.
As you can see, it’s not very hard.
Now in the most general case we could have more complex things. For instance,
we could have products of the two operators like σe
yσp
z. When we have such a
product we do ﬁrst what the operator on the right says, and then do what the
other one says.† For example, we would have that
σe
xσp
z | + −⟩= σe
x(σp
z | + −⟩) = σe
x(−| + −⟩) = −σe
x | + −⟩= −| −−⟩.
† For these particular operators, you will notice it turns out that the sequence of the
operators doesn’t matter.
12-6","320"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","320","Note that these operators don’t do anything on pure numbers—we have used
this fact when we wrote σe
x(−1) = (−1)σe
x. We say that the operators “commute”
with pure numbers, or that a number “can be moved through” the operator. You
can practice by showing that the product σe
xσp
z gives the following results for the
four states:
σe
xσp
z | + +⟩= + | −+⟩,
σe
xσp
z | + −⟩= −| −−⟩,
σe
xσp
z | −+⟩= + | + +⟩,
σe
xσp
z | −−⟩= −| + −⟩.
If we take all the possible operators, using each kind of operator only once,
there are sixteen possibilities. Yes, sixteen—provided we include also the “unit
operator” ˆ1. First, there are the three: σe
x, σe
y, σe
z. Then the three σp
x, σp
y,
σp
z—that makes six. In addition, there are the nine possible products of the
form σe
xσp
y which makes a total of 15. And there’s the unit operator which just
leaves any state unchanged. Sixteen in all.
Now note that for a four-state system, the Hamiltonian matrix has to be
a four-by-four matrix of coeﬃcients—it will have sixteen entries. It is easily
demonstrated that any four-by-four matrix—and, therefore, the Hamiltonian
matrix in particular—can be written as a linear combination of the sixteen
double-spin matrices corresponding to the set of operators we have just made
up. Therefore, for the interaction between a proton and an electron that involves
only their spins, we can expect that the Hamiltonian operator can be written as
a linear combination of the same 16 operators. The only question is, how?
Well, ﬁrst, we know that the interaction doesn’t depend on our choice of axes
for a coordinate system. If there is no external disturbance—like a magnetic
ﬁeld—to determine a unique direction in space, the Hamiltonian can’t depend
on our choice of the direction of the x-, y-, and z-axes. That means that the
Hamiltonian can’t have a term like σe
x all by itself. It would be ridiculous, because
then somebody with a diﬀerent coordinate system would get diﬀerent results.
The only possibilities are a term with the unit matrix, say a constant a
(times ˆ1), and some combination of the sigmas that doesn’t depend on the
coordinates—some “invariant” combination. The only scalar invariant combina-
tion of two vectors is the dot product, which for our σ’s is
σe · σp = σe
xσp
x + σe
yσp
y + σe
zσp
z.
(12.4)
12-7","419"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","321","This operator is invariant with respect to any rotation of the coordinate system.
So the only possibility for a Hamiltonian with the proper symmetry in space is a
constant times the unit matrix plus a constant times this dot product, say,
ˆH = E0 + A σe · σp.
(12.5)
That’s our Hamiltonian. It’s the only thing that it can be, by the symmetry of
space, so long as there is no external ﬁeld. The constant term doesn’t tell us
much; it just depends on the level we choose to measure energies from. We may
just as well take E0 = 0. The second term tells us all we need to know to ﬁnd
the level splitting of the hydrogen.
If you want to, you can think of the Hamiltonian in a diﬀerent way. If there
are two magnets near each other with magnetic moments µe and µp, the mutual
energy will depend on µe · µp—among other things. And, you remember, we
found that the classical thing we call µe appears in quantum mechanics as µeσe.
Similarly, what appears classically as µp will usually turn out in quantum
mechanics to be µpσp (where µp is the magnetic moment of the proton, which is
about 1000 times smaller than µe, and has the opposite sign). So Eq. (12.5) says
that the interaction energy is like the interaction between two magnets—only
not quite, because the interaction of the two magnets depends on the radial
distance between them. But Eq. (12.5) could be—and, in fact, is—some kind
of an average interaction. The electron is moving all around inside the atom,
and our Hamiltonian gives only the average interaction energy. All it says is
that for a prescribed arrangement in space for the electron and proton there
is an energy proportional to the cosine of the angle between the two magnetic
moments, speaking classically. Such a classical qualitative picture may help you
to understand where it comes from, but the important thing is that Eq. (12.5) is
the correct quantum mechanical formula.
The order of magnitude of the classical interaction between two magnets
would be the product of the two magnetic moments divided by the cube of the
distance between them. The distance between the electron and the proton in the
hydrogen atom is, speaking roughly, one half an atomic radius, or 0.5 angstrom.
It is, therefore, possible to make a crude estimate that the constant A should
be about equal to the product of the two magnetic moments µe and µp divided
by the cube of 1/2 angstrom. Such an estimate gives a number in the right ball
park. It turns out that A can be calculated accurately once you understand the
complete quantum theory of the hydrogen atom—which we so far do not. It has,
12-8","465"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","322","in fact, been calculated to an accuracy of about 30 parts in one million. So, unlike
the ﬂip-ﬂop constant A of the ammonia molecule, which couldn’t be calculated
at all well by a theory, our constant A for the hydrogen can be calculated from a
more detailed theory. But never mind, we will for our present purposes think of
the A as a number which could be determined by experiment, and analyze the
physics of the situation.
Taking the Hamiltonian of Eq. (12.5), we can use it with the equation
iℏ˙Ci =
X
j
HijCj
(12.6)
to ﬁnd out what the spin interactions do to the energy levels. To do that, we
need to work out the sixteen matrix elements Hij = ⟨i | H | j⟩corresponding to
each pair of the four base states in (12.1).
We begin by working out what ˆH | j⟩is for each of the four base states. For
example,
ˆH | + +⟩= Aσe · σp | + +⟩= A{σe
xσp
x + σe
yσp
y + σe
zσp
z} | + +⟩.
(12.7)
Using the method we described a little earlier—it’s easy if you have memorized
Table 12-1—we ﬁnd what each pair of σ’s does on | + +⟩. The answer is
σe
xσp
x | + +⟩= + | −−⟩,
σe
yσp
y | + +⟩= −| −−⟩,
σe
zσp
z | + +⟩= + | + +⟩.
(12.8)
So (12.7) becomes
ˆH | + +⟩= A{| −−⟩−| −−⟩+ | + +⟩} = A | + +⟩.
(12.9)
Since our four base states are all orthogonal, that gives us immediately that
⟨+ + | H | + +⟩= A⟨+ + | + +⟩= A,
⟨+ −| H | + +⟩= A⟨+ −| + +⟩= 0,
⟨−+ | H | + +⟩= A⟨−+ | + +⟩= 0,
⟨−−| H | + +⟩= A⟨−−| + +⟩= 0.
(12.10)
12-9","315"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","323","Remembering that ⟨j | H | i⟩= ⟨i | H | j⟩∗, we can already write down the diﬀer-
ential equation for the amplitudes C1:
iℏ˙C1 = H11C1 + H12C2 + H13C3 + H14C4
or
iℏ˙C1 = AC1.
(12.11)
That’s all! We get only the one term.
Table 12-2
Spin operators for the hydrogen atom
σe
xσp
x | + +⟩= + | −−⟩
σe
xσp
x | + −⟩= + | −+⟩
σe
xσp
x | −+⟩= + | + −⟩
σe
xσp
x | −−⟩= + | + +⟩
σe
yσp
y | + +⟩= −| −−⟩
σe
yσp
y | + −⟩= + | −+⟩
σe
yσp
y | −+⟩= + | + −⟩
σe
yσp
y | −−⟩= −| + +⟩
σe
zσp
z | + +⟩= + | + +⟩
σe
zσp
z | + −⟩= −| + −⟩
σe
zσp
z | −+⟩= −| −+⟩
σe
zσp
z | −−⟩= + | −−⟩
Now to get the rest of the Hamiltonian equations we have to crank through
the same procedure for ˆH operating on the other states. First, we will let you
practice by checking out all of the sigma products we have written down in
Table 12-2. Then we can use them to get:
ˆH | + −⟩= A{2 | −+⟩−| + −⟩},
ˆH | −+⟩= A{2 | + −⟩−| −+⟩},
ˆH | −−⟩= A | −−⟩.
(12.12)
12-10","236"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","324","Then, multiplying each one in turn on the left by all the other state vectors, we
get the following Hamiltonian matrix, Hij:
Hij =
i↓
j→





A
0
0
0
0
−A
2A
0
0
2A
−A
0
0
0
0
A




.
(12.13)
It means, of course, nothing more than that our diﬀerential equations for the
four amplitudes Ci are
iℏ˙C1 = AC1,
iℏ˙C2 = −AC2 + 2AC3,
iℏ˙C3 = 2AC2 −AC3,
iℏ˙C4 = AC4.
(12.14)
Before solving these equations we can’t resist telling you about a clever rule
due to Dirac—it will make you feel that you are really advanced—although we
don’t need it for our work. We have—from the equations (12.9) and (12.12)—that
σe · σp | + +⟩= | + +⟩,
σe · σp | + −⟩= 2 | −+⟩−| + −⟩,
σe · σp | −+⟩= 2 | + −⟩−| −+⟩,
σe · σp | −−⟩= | −−⟩.
(12.15)
Look, said Dirac, I can also write the ﬁrst and last equations as
σe · σp | + +⟩= 2 | + +⟩−| + +⟩,
σe · σp | −−⟩= 2 | −−⟩−| −−⟩;
then they are all quite similar.
Now I invent a new operator, which I will
12-11","213"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","325","call Pspin exch and which I deﬁne to have the following properties:†
Pspin exch | + +⟩= | + +⟩,
Pspin exch | + −⟩= | −+⟩,
Pspin exch | −+⟩= | + −⟩,
Pspin exch | −−⟩= | −−⟩.
All the operator does is interchange the spin directions of the two particles. Then
I can write the whole set of equations in (12.15) as a simple operator equation:
σe · σp = 2Pspin exch −1.
(12.16)
That’s the formula of Dirac. His “spin exchange operator” gives a handy rule
for ﬁguring out σe · σp. (You see, you can do everything now. The gates are
open.)
12-3 The energy levels
Now we are ready to work out the energy levels of the ground state of hydrogen
by solving the Hamiltonian equations (12.14). We want to ﬁnd the energies of
the stationary states. This means that we want to ﬁnd those special states | ψ⟩
for which each amplitude Ci = ⟨i | ψ⟩in the set belonging to | ψ⟩has the same
time dependence—namely, e−iωt. Then the state will have the energy E = ℏω.
So we want a set for which
Ci = aie−(i/ℏ)Et,
(12.17)
where the four coeﬃcients ai are independent of time. To see whether we can get
such amplitudes, we substitute (12.17) into Eq. (12.14) and see what happens.
Each iℏdC/dt in Eq. (12.14) turns into EC, and—after cancelling out the common
exponential factor—each C becomes an a; we get
Ea1 = Aa1,
Ea2 = −Aa2 + 2Aa3,
Ea3 = 2Aa2 −Aa3,
Ea4 = Aa4,
(12.18)
† This operator is now called the “Pauli spin exchange operator.”
12-12","272"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","326","which we have to solve for a1, a2, a3, and a4. Isn’t it nice that the ﬁrst equation
is independent of the rest—that means we can see one solution right away. If we
choose E = A,
a1 = 1,
a2 = a3 = a4 = 0,
gives a solution. (Of course, taking all the a’s equal to zero also gives a solution,
but that’s no state at all!) Let’s call our ﬁrst solution the state | I⟩:†
| I⟩= | 1⟩= | + +⟩.
(12.19)
Its energy is
EI = A.
With that clue you can immediately see another solution from the last equation
in (12.18):
a1 = a2 = a3 = 0,
a4 = 1,
E = A.
We’ll call that solution state | II⟩:
| II⟩= | 4⟩= | −−⟩,
(12.20)
EII = A.
Now it gets a little harder; the two equations left in (12.18) are mixed up.
But we’ve done it all before. Adding the two, we get
E(a2 + a3) = A(a2 + a3).
(12.21)
Subtracting, we have
E(a2 −a3) = −3A(a2 −a3).
(12.22)
By inspection—and remembering ammonia—we see that there are two solutions:
a2 = a3,
E = A
and
(12.23)
a2 = −a3,
E = −3A.
† The state is really | I⟩e−(i/ℏ)EI t; but, as usual we will identify the states by the constant
vectors which are equal to the complete vectors at t = 0.
12-13","236"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","327","They are mixtures of | 2⟩and | 3⟩. Calling these states | III⟩and | IV ⟩, and
putting in a factor 1/
√
2 to make the states properly normalized, we have
| III⟩=
1
√
2 (| 2⟩+ | 3⟩) =
1
√
2 (| + −⟩+ | −+⟩),
EIII = A
(12.24)
and
| IV ⟩=
1
√
2 (| 2⟩−| 3⟩) =
1
√
2 (| + −⟩−| −+⟩),
EIV = −3A.
(12.25)
We have found four stationary states and their energies. Notice, incidentally,
that our four states are orthogonal, so they also can be used for base states if
desired. Our problem is completely solved.
Three of the states have the energy A, and the last has the energy −3A. The
average is zero—which means that when we took E0 = 0 in Eq. (12.5), we were
choosing to measure all the energies from the average energy. We can draw the
energy-level diagram for the ground state of hydrogen as shown in Fig. 12-2.
I, II, III
IV
|∆E| = ¯hω
E0 + A
E0
E0 −3A
Fig. 12-2. Energy-level diagram for the ground state of atomic hydrogen.
Now the diﬀerence in energy between state | IV ⟩and any one of the others
is 4A. An atom which happens to have gotten into state | I⟩could fall from there
to state | IV ⟩and emit light. Not optical light, because the energy is so tiny—it
would emit a microwave quantum. Or, if we shine microwaves on hydrogen
gas, we will ﬁnd an absorption of energy as the atoms in state | IV ⟩pick up
12-14","269"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","328","energy and go into one of the upper states—but only at the frequency ω = 4A/ℏ.
This frequency has been measured experimentally; the best result, obtained very
recently,† is
f = ω/2π = (1,420,405,751.800 ± 0.028) cycles per second.
(12.26)
The error is only two parts in 100 billion! Probably no basic physical quantity
is measured better than that—it’s one of the most remarkably accurate mea-
surements in physics. The theorists were very happy that they could compute
the energy to an accuracy of 3 parts in 105, but in the meantime it has been
measured to 2 parts in 1011—a million times more accurate than the theory. So
the experimenters are way ahead of the theorists. In the theory of the ground
state of the hydrogen atom you are as good as anybody. You, too, can just
take your value of A from experiment—that’s what everybody has to do in the
end.
You have probably heard before about the “21-centimeter line” of hydrogen.
That’s the wavelength of the 1420 megacycle spectral line between the hyperﬁne
states.
Radiation of this wavelength is emitted or absorbed by the atomic
hydrogen gas in the galaxies. So with radio telescopes tuned in to 21-cm waves
(or 1420 megacycles approximately) we can observe the velocities and the location
of concentrations of atomic hydrogen gas. By measuring the intensity, we can
estimate the amount of hydrogen. By measuring the frequency shift due to the
Doppler eﬀect, we can ﬁnd out about the motion of the gas in the galaxy. That
is one of the big programs of radio astronomy. So now we are talking about
something that’s very real—it is not an artiﬁcial problem.
12-4 The Zeeman splitting
Although we have ﬁnished the problem of ﬁnding the energy levels of the
hydrogen ground state, we would like to study this interesting system some more.
In order to say anything more about it—for instance, in order to calculate the rate
at which the hydrogen atom absorbs or emits radio waves at 21 centimeters—we
have to know what happens when the atom is disturbed. We have to do as we
did for the ammonia molecule—after we found the energy levels we went on and
studied what happened when the molecule was in an electric ﬁeld. We were
then able to ﬁgure out the eﬀects from the electric ﬁeld in a radio wave. For the
† Crampton, Kleppner, and Ramsey; Physical Review Letters, Vol. 11, page 338 (1963).
12-15","411"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","329","hydrogen atom, the electric ﬁeld does nothing to the levels, except to move them
all by some constant amount proportional to the square of the ﬁeld—which is not
of any interest because that won’t change the energy diﬀerences. It is now the
magnetic ﬁeld which is important. So the next step is to write the Hamiltonian
for a more complicated situation in which the atom sits in an external magnetic
ﬁeld.
What, then, is the Hamiltonian? We’ll just tell you the answer, because we
can’t give you any “proof” except to say that this is the way the atom works.
The Hamiltonian is
ˆH = A(σe · σp) −µeσe · B −µpσp · B.
(12.27)
It now consists of three parts. The ﬁrst term Aσe · σp represents the magnetic
interaction between the electron and the proton—it is the same one that would
be there if there were no magnetic ﬁeld. This is the term we have already
had; and the inﬂuence of the magnetic ﬁeld on the constant A is negligible.
The eﬀect of the external magnetic ﬁeld shows up in the last two terms. The
second term, −µeσe · B, is the energy the electron would have in the magnetic
ﬁeld if it were there alone.† In the same way, the last term −µpσp · B, would
have been the energy of a proton alone.
Classically, the energy of the two
of them together would be the sum of the two, and that works also quantum
mechanically. In a magnetic ﬁeld, the energy of interaction due to the magnetic
ﬁeld is just the sum of the energy of interaction of the electron with the external
ﬁeld, and of the proton with the ﬁeld—both expressed in terms of the sigma
operators. In quantum mechanics these terms are not really the energies, but
thinking of the classical formulas for the energy is a way of remembering the
rules for writing down the Hamiltonian. Anyway, the correct Hamiltonian is
Eq. (12.27).
Now we have to go back to the beginning and do the problem all over again.
Much of the work is, however, done—we need only to add the eﬀects of the new
terms. Let’s take a constant magnetic ﬁeld B in the z-direction. Then we have
to add to our Hamiltonian operator ˆH the two new pieces—which we can call ˆH′:
ˆH′ = −(µeσe
z + µpσp
z)B.
† Remember that classically U = −µ · B, so the energy is lowest when the moment is along
the ﬁeld. For positive particles, the magnetic moment is parallel to the spin and for negative
particles it is opposite. So in Eq. (12.27), µp is a positive number, but µe is a negative number.
12-16","453"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","330","Using Table 12-1, we get right away that
ˆH′ | + +⟩= −(µe + µp)B | + +⟩,
ˆH′ | + −⟩= −(µe −µp)B | + −⟩,
ˆH′ | −+⟩= −(−µe + µp)B | −+⟩,
ˆH′ | −−⟩= (µe + µp)B | −−⟩.
(12.28)
How very convenient! The ˆH′ operating on each state just gives a number times
that state. The matrix ⟨i | H′ | j⟩has, therefore, only diagonal elements—we can
just add the coeﬃcients in (12.28) to the corresponding diagonal terms of (12.13),
and the Hamiltonian equations of (12.14) become
iℏdC1/dt = {A −(µe + µp)B}C1,
iℏdC2/dt = −{A + (µe −µp)B}C2 + 2AC3,
iℏdC3/dt = 2AC2 −{A −(µe −µp)B}C3,
iℏdC4/dt = {A + (µe + µp)B}C4.
(12.29)
The form of the equations is not diﬀerent—only the coeﬃcients. So long
as B doesn’t vary with time, we can continue as we did before. Substituting
Ci = aie−(i/ℏ)Et, we get—as a modiﬁcation of (12.18)—
Ea1 = {A −(µe + µp)B}a1,
Ea2 = −{A + (µe −µp)B}a2 + 2Aa3,
Ea3 = 2Aa2 −{A −(µe −µp)B}a3,
Ea4 = {A + (µe + µp)B}a4.
(12.30)
Fortunately, the ﬁrst and fourth equations are still independent of the rest, so
the same technique works again.
One solution is the state | I⟩for which a1 = 1, a2 = a3 = a4 = 0, or
| I⟩= | 1⟩= | + +⟩,
with
(12.31)
EI = A −(µe + µp)B.
12-17","236"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","331","Another is
| II⟩= | 4⟩= | −−⟩,
with
(12.32)
EII = A + (µe + µp)B.
A little more work is involved for the remaining two equations, because the
coeﬃcients of a2 and a3 are no longer equal. But they are just like the pair we
had for the ammonia molecule. Looking back at Eq. (9.20), we can make the
following analogy (remembering that the labels 1 and 2 there correspond to 2
and 3 here):
H11 →−A −(µe −µp)B,
H12 →2A,
H21 →2A,
H22 →−A + (µe −µp)B.
(12.33)
The energies are then given by (9.25), which was
E = H11 + H22
2
±
r
(H11 −H22)2
4
+ H12H21.
(12.34)
Making the substitutions from (12.33), the energy formula becomes
E = −A ±
q
(µe −µp)2B2 + 4A2.
Although in Chapter 9 we used to call these energies EI and EII, and we are in
this problem calling them EIII and EIV,
EIII = A{−1 + 2
q
1 + (µe −µp)2B2/4A2},
EIV = −A{1 + 2
q
1 + (µe −µp)2B2/4A2}.
(12.35)
So we have found the energies of the four stationary states of a hydrogen
atom in a constant magnetic ﬁeld. Let’s check our results by letting B go to zero
and seeing whether we get the same energies we had in the preceding section.
You see that we do. For B = 0, the energies EI, EII, and EIII go to +A, and
EIV goes to −3A. Even our labeling of the states agrees with what we called
12-18","256"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","332","them before. When we turn on the magnetic ﬁeld though, all of the energies
change in a diﬀerent way. Let’s see how they go.
First, we have to remember that for the electron, µe is negative, and about
1000 times larger than µp—which is positive. So µe + µp and µe −µp are both
negative numbers, and nearly equal. Let’s call them −µ and −µ′:
µ = −(µe + µp),
µ′ = −(µe −µp).
(12.36)
(Both µ and µ′ are positive numbers, nearly equal to magnitude of µe—which is
about one Bohr magneton.) Then our four energies are
EI = A + µB,
EII = A −µB,
EIII = A{−1 + 2
p
1 + µ′2B2/4A2},
EIV = −A{1 + 2
p
1 + µ′2B2/4A2}.
(12.37)
The energy EI starts at A and increases linearly with B—with the slope µ. The
energy EII also starts at A but decreases linearly with increasing B—its slope
is −µ. These two levels vary with B as shown in Fig. 12-3. We show also in
the ﬁgure the energies EIII and EIV. They have a diﬀerent B-dependence. For
small B, they depend quadratically on B, so they start out with horizontal slopes.
Then they begin to curve, and for large B they approach straight lines with
slopes ±µ′, which are nearly the same as the slopes of EI and EII.
The shift of the energy levels of an atom due to a magnetic ﬁeld is called the
Zeeman eﬀect. We say that the curves in Fig. 12-3 show the Zeeman splitting of
the ground state of hydrogen. When there is no magnetic ﬁeld, we get just one
spectral line from the hyperﬁne structure of hydrogen. The transitions between
state | IV ⟩and any one of the others occurs with the absorption or emission of a
photon whose frequency 1420 megacycles is 1/h times the energy diﬀerence 4A.
When the atom is in a magnetic ﬁeld B, however, there are many more lines.
There can be transitions between any two of the four states. So if we have atoms
in all four states, energy can be absorbed—or emitted—in any one of the six
transitions shown by the vertical arrows in Fig. 12-4. Many of these transitions
can be observed by the Rabi molecular beam technique we described in Volume II,
Section 35-3 (see Appendix).
What makes the transitions go? The transitions will occur if you apply a
small disturbing magnetic ﬁeld that varies with time (in addition to the steady
12-19","416"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","333","µB/A
1
2
3
4
E
A
−5
−4
−3
−2
−1
0
1
2
3
4
EI = A + µB
EIII
EII = A −µB
EIV
−A + µ′B
−A −µ′B
Fig. 12-3. The energy levels of the ground state of hydrogen in a
magnetic ﬁeld B.
12-20","50"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","334","B
E
I
II
III
IV
Fig. 12-4. Transitions between the levels of ground state energy levels
of hydrogen in some particular magnetic ﬁeld B.
12-21","26"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","335","strong ﬁeld B). It’s just as we saw for a varying electric ﬁeld on the ammonia
molecule. Only here, it is the magnetic ﬁeld which couples with the magnetic
moments and does the trick. But the theory follows through in the same way
that we worked it out for the ammonia. The theory is the simplest if you take a
perturbing magnetic ﬁeld that rotates in the xy-plane—although any horizontal
oscillating ﬁeld will do. When you put in this perturbing ﬁeld as an additional
term in the Hamiltonian, you get solutions in which the amplitudes vary with
time—as we found for the ammonia molecule. So you can calculate easily and
accurately the probability of a transition from one state to another. And you
ﬁnd that it all agrees with experiment.
12-5 The states in a magnetic ﬁeld
We would like now to discuss the shapes of the curves in Fig. 12-3. In the ﬁrst
place, the energies for large ﬁelds are easy to understand, and rather interesting.
For B large enough (namely for µB/A ≫1) we can neglect the 1 in the formulas
of (12.37). The four energies become
EI = A + µB,
EII = A −µB,
EIII = −A + µ′B,
EIV = −A −µ′B.
(12.38)
These are the equations of the four straight lines in Fig. 12-3. We can understand
these energies physically in the following way. The nature of the stationary states
in a zero ﬁeld is determined completely by the interaction of the two magnetic
moments. The mixtures of the base states | + −⟩and | −+⟩in the stationary
states | III⟩and | IV ⟩are due to this interaction. In large external ﬁelds, however,
the proton and electron will be inﬂuenced hardly at all by the ﬁeld of the other;
each will act as if it were alone in the external ﬁeld. Then—as we have seen many
times—the electron spin will be either parallel to or opposite to the external
magnetic ﬁeld.
Suppose the electron spin is “up”—that is, along the ﬁeld; its energy will
be −µeB. The proton can still be either way. If the proton spin is also “up,” its
energy is −µpB. The sum of the two is −(µe + µp)B = µB. That is just what
we ﬁnd for EI—which is ﬁne, because we are describing the state | + +⟩= | I⟩.
There is still the small additional term A (now µB ≫A) which represents the
interaction energy of the proton and electron when their spins are parallel. (We
originally took A as positive because the theory we spoke of says it should be,
12-22","435"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","336","and experimentally it is indeed so.) On the other hand, the proton can have
its spin down. Then its energy in the external ﬁeld goes to +µpB, so it and
the electron have the energy −(µe −µp)B = µ′B. And the interaction energy
becomes −A. The sum is just the energy EIII, in (12.38). So the state | III⟩
must for large ﬁelds become the state | + −⟩.
Suppose now the electron spin is “down.” Its energy in the external ﬁeld is µeB.
If the proton is also “down,” the two together have the energy (µe +µp)B = −µB,
plus the interaction energy A—since their spins are parallel. That makes just
the energy EII in (12.38) and corresponds to the state | −−⟩= | II⟩—which
is nice. Finally if the electron is “down” and the proton is “up,” we get the
energy (µe−µp)B−A (minus A for the interaction because the spins are opposite)
which is just EIV. And the state corresponds to | −+⟩.
“But, wait a moment!”, you are probably saying, “The states | III⟩and | IV ⟩
are not the states | + −⟩and | −+⟩; they are mixtures of the two.” Well, only
slightly. They are indeed mixtures for B = 0, but we have not yet ﬁgured out
what they are for large B. When we used the analogies of (12.33) in our formulas
of Chapter 9 to get the energies of the stationary states, we could also have taken
the amplitudes that go with them. They come from Eq. (9.24), which is
a2
a3
= E −H22
H21
.
The ratio a2/a3 is, of course, just C2/C3. Plugging in the analogous quantities
from (12.33), we get
C2
C3
= E + A −(µe −µp)B
2A
or
C2
C3
= E + A + µ′B
2A
,
(12.39)
where for E we are to use the appropriate energy—either EIII or EIV. For
instance, for state | III⟩we have
C2
C3

III
≈µ′B
A .
(12.40)
So for large B the state | III⟩has C2 ≫C3; the state becomes almost completely
the state | 2⟩= | + −⟩. Similarly, if we put EIV into (12.39) we get (C2/C3)IV ≪
12-23","365"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","337","1; for high ﬁelds state | IV ⟩becomes just the state | 3⟩= | −+⟩. You see that
the coeﬃcients in the linear combinations of our base states which make up the
stationary states depend on B. The state we call | III⟩is a 50-50 mixture of
| + −⟩and | −+⟩at very low ﬁelds, but shifts completely over to | + −⟩at high
ﬁelds. Similarly, the state | IV ⟩, which at low ﬁelds is also a 50-50 mixture (with
opposite signs) of | + −⟩and | −+⟩, goes over into the state | −+⟩when the
spins are uncoupled by a strong external ﬁeld.
B
E
+A
0
−3A
j
m
I
1
+1
III
1
0
II
1
−1
IV
0
0
Fig. 12-5. The states of the hydrogen atom for small magnetic ﬁelds.
We would also like to call your attention particularly to what happens at
very low magnetic ﬁelds. There is one energy—at −3A—which does not change
when you turn on a small magnetic ﬁeld. And there is another energy—at +A—
which splits into three diﬀerent energy levels when you turn on a small magnetic
ﬁeld. For weak ﬁelds the energies vary with B as shown in Fig. 12-5. Suppose
that we have somehow selected a bunch of hydrogen atoms which all have the
energy −3A. If we put them through a Stern-Gerlach experiment—with ﬁelds
that are not too strong—we would ﬁnd that they just go straight through. (Since
their energy doesn’t depend on B, there is—according to the principle of virtual
12-24","257"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","338","work—no force on them in a magnetic ﬁeld gradient.) Suppose, on the other
hand, we were to select a bunch of atoms with the energy +A, and put them
through a Stern-Gerlach apparatus, say an S apparatus. (Again the ﬁelds in
the apparatus should not be so great that they disrupt the insides of the atom,
by which we mean a ﬁeld small enough that the energies vary linearly with B.)
We would ﬁnd three beams. The states | I⟩and | II⟩get opposite forces—their
energies vary linearly with B with the slopes ±µ so the forces are like those on a
dipole with µz = ∓µ; but the state | III⟩goes straight through. So we are right
back in Chapter 5. A hydrogen atom with the energy +A is a spin-one particle.
This energy state is a “particle” for which j = 1, and it can be described—with
respect to some set of axes in space—in terms of the base states | +S⟩, | 0S⟩,
and | −S⟩we used in Chapter 5. On the other hand, when a hydrogen atom has
the energy −3A, it is a spin-zero particle. (Remember, what we are saying is
only strictly true for inﬁnitesimal magnetic ﬁelds.) So we can group the states of
hydrogen in zero magnetic ﬁeld this way:
| I⟩= | + +⟩
| III⟩= | + −⟩+ | −+⟩
√
2
| II⟩= | −−⟩











spin 1











| +S⟩
| 0S⟩
| −S⟩
(12.41)
| IV ⟩= | + −⟩−| −+⟩
√
2
spin 0.
(12.42)
We have said in Chapter 35 of Volume II (Appendix) that for any particle its
component of angular momentum along any axis can have only certain values
always ℏapart. The z-component of angular momentum Jz can be jℏ, (j −1)ℏ,
(j −2)ℏ, . . . , (−j)ℏ, where j is the spin of the particle (which can be an integer
or half-integer). Although we neglected to say so at the time, people usually
write
Jz = mℏ,
(12.43)
where m stands for one of the numbers j, j −1, j −2, . . . , −j. You will, therefore,
see people in books label the four ground states of hydrogen by the so-called
quantum numbers j and m [often called the “total angular momentum quantum
number” (j) and “magnetic quantum number” (m)]. Then, instead of our state
symbols | I⟩, | II⟩, and so on, they will write a state as | j, m⟩. So they would
12-25","435"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","339","write our little table of states for zero ﬁeld in (12.41) and (12.42) as shown in
Table 12-3. It’s not new physics, it’s all just a matter of notation.
Table 12-3
Zero ﬁeld states of the hydrogen atom
State | j, m⟩
j
m
Our notation
| 1, +1⟩
1
+1
| I⟩= | +S⟩
| 1, 0⟩
1
0
| III⟩= | 0S⟩
| 1, −1⟩
1
−1
| II⟩= | −S⟩
| 0, 0⟩
0
0
| IV⟩
12-6 The projection matrix for spin one†
We would like now to use our knowledge of the hydrogen atom to do something
special. We discussed in Chapter 5 that a particle of spin one which was in one
of the base states (+, 0, or −) with respect to a Stern-Gerlach apparatus of a
particular orientation—say an S apparatus—would have a certain amplitude to
be in each of the three states with respect to a T apparatus with a diﬀerent
orientation in space. There are nine such amplitudes ⟨jT | iS⟩which make up
the projection matrix. In Section 5-7 we gave without proof the terms of this
matrix for various orientations of T with respect to S. Now we will show you
one way they can be derived.
In the hydrogen atom we have found a spin-one system which is made up of
two spin one-half particles. We have already worked out in Chapter 6 how to
transform the spin one-half amplitudes. We can use this information to calculate
the transformation for spin one. This is the way it works: We have a system—a
hydrogen atom with the energy +A—which has spin one. Suppose we run it
through a Stern-Gerlach ﬁlter S, so that we know it is in one of the base states
with respect to S, say | +S⟩. What is the amplitude that it will be in one of the
base states, say | +T⟩, with respect to the T apparatus? If we call the coordinate
system of the S apparatus the x, y, z system, the | +S⟩state is what we have
been calling the state | + +⟩. But suppose another guy took his z-axis along the
axis of T. He will be referring his states to what we will call the x′, y′, z′ frame.
† Those who chose to jump over Chapter 6 should skip this section also.
12-26","396"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","340","His “up” and “down” states for the electron and proton would be diﬀerent from
ours. His “plus-plus” state—which we can write | +′ +′⟩, referring to the “prime”
frame—is the | +T⟩state of the spin-one particle. What we want is ⟨+T | +S⟩
which is just another way of writing the amplitude ⟨+′ +′ | + +⟩.
We can ﬁnd the amplitude ⟨+′ +′ | + +⟩in the following way. In our frame
the electron in the | + +⟩state has its spin “up”. That means that it has some
amplitude ⟨+′ | +⟩e of being “up” in his frame, and some amplitude ⟨−′ | +⟩e of
being “down” in that frame. Similarly, the proton in the | + +⟩state has spin
“up” in our frame and the amplitudes ⟨+′ | +⟩p and ⟨−′ | +⟩p of having spin “up”
or spin “down” in the “prime” frame. Since we are talking about two distinct
particles, the amplitude that both particles will be “up” together in his frame is
the product of the two amplitudes,
⟨+′ +′ | + +⟩= ⟨+′ | +⟩e⟨+′ | +⟩p.
(12.44)
We have put the subscripts e and p on the amplitudes ⟨+′ | +⟩to make it clear
what we were doing. But they are both just the transformation amplitudes for
a spin one-half particle, so they are really identical numbers. They are, in fact,
just the amplitude we have called ⟨+T | +S⟩in Chapter 6, and which we listed
in the tables at the end of that chapter.
Now, however, we are about to get into trouble with notation. We have
to be able to distinguish the amplitude ⟨+T | +S⟩for a spin one-half particle
from what we have also called ⟨+T | +S⟩for a spin-one particle—yet they are
completely diﬀerent! We hope it won’t be too confusing, but for the moment at
least, we will have to use some diﬀerent symbols for the spin one-half amplitudes.
To help you keep things straight, we summarize the new notation in Table 12-4.
We will continue to use the notation | +S⟩, | 0S⟩, and | −S⟩for the states of a
spin-one particle.
Table 12-4
Spin one-half amplitudes
This chapter
Chapter 6
a = ⟨+′ | +⟩
⟨+T | +S⟩
b = ⟨−′ | +⟩
⟨−T | +S⟩
c = ⟨+′ | −⟩
⟨+T | −S⟩
d = ⟨−′ | −⟩
⟨−T | −S⟩
12-27","398"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","341","With our new notation, Eq. (12.44) becomes simply
⟨+′ +′ | + +⟩= a2,
and this is just the spin-one amplitude ⟨+T | +S⟩. Now, let’s suppose, for instance,
that the other guy’s coordinate frame—that is, the T, or “primed,” apparatus—is
just rotated with respect to our z-axis by the angle φ; then from Table 6-2,
a = ⟨+′ | +⟩= eiφ/2.
So from (12.44) we have that the spin-one amplitude is
⟨+T | +S⟩= ⟨+′ +′ | + +⟩= (eiφ/2)2 = eiφ.
(12.45)
You can see how it goes.
Now we will work through the general case for all the states. If the proton
and electron are both “up” in our frame—the S-frame—the amplitudes that
it will be in any one of the four possible states in the other guy’s frame—the
T-frame—are
⟨+′ +′ | + +⟩= ⟨+′ | +⟩e⟨+′ | +⟩p = a2,
⟨+′ −′ | + +⟩= ⟨+′ | +⟩e⟨−′ | +⟩p = ab,
⟨−′ +′ | + +⟩= ⟨−′ | +⟩e⟨+′ | +⟩p = ba,
⟨−′ −′ | + +⟩= ⟨−′ | +⟩e⟨−′ | +⟩p = b2.
(12.46)
We can, then, write the state | + +⟩as the following linear combination:
| + +⟩= a2 | +′ +′⟩+ ab{| +′ −′⟩+ | −′ +′⟩} + b2 | −′ −′⟩.
(12.47)
Now we notice that | +′ +′⟩is the state | +T⟩, that {| +′ −′⟩+ | −′ +′⟩} is just
√
2 times the state | 0T⟩—see (12.41)—and that | −′ −′⟩= | −T⟩. In other words,
Eq. (12.47) can be rewritten as
| +S⟩= a2 | +T⟩+
√
2 ab | 0T⟩+ b2 | −T⟩.
(12.48)
In a similar way you can easily show that
| −S⟩= c2 | +T⟩+
√
2 cd | 0T⟩+ d2 | −T⟩.
(12.49)
12-28","295"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","342","For | 0S⟩it’s a little more complicated, because
| 0S⟩=
1
√
2 {| + −⟩+ | −+⟩}.
But we can express each of the states | + −⟩and | −+⟩in terms of the “prime”
states and take the sum. That is,
| + −⟩= ac | +′ +′⟩+ ad | +′ −′⟩+ bc | −′ +′⟩+ bd | −′ −′⟩
(12.50)
and
| −+⟩= ac | +′ +′⟩+ bc | +′ −′⟩+ ad | −′ +′⟩+ bd | −′ −′⟩.
(12.51)
Taking 1/
√
2 times the sum, we get
| 0S⟩=
2
√
2 ac | +′ +′⟩+ ad + bc
√
2
{| +′ −′⟩+ | −′ +′⟩} + 2
√
2 bd | −′ −′⟩.
It follows that
| 0S⟩=
√
2 ac | +T⟩+ (ad + bc) | 0T⟩+
√
2 bd | −T⟩.
(12.52)
We have now all of the amplitudes we wanted.
The coeﬃcients of Eqs.
(12.48), (12.49), and (12.52) are the matrix elements ⟨jT | iS⟩. Let’s pull them
all together:
⟨jT | iS⟩=
jT ↓
iS→




a2
√
2 ac
c2
√
2 ab
ad + bc
√
2 cd
b2
√
2 bd
d2



.
(12.53)
We have expressed the spin-one transformation in terms of the spin one-half
amplitudes a, b, c, and d.
For instance, if the T-frame is rotated with respect to S by the angle α about
the y-axis—as in Fig. 5-6—the amplitudes in Table 12-4 are just the matrix
elements of Ry(α) in Table 6-2.
a = cos α
2 ,
b = −sin α
2 ,
c = sin α
2 ,
d = cos α
2 .
(12.54)
12-29","282"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","343","Using these in (12.53), we get the formulas of (5.38), which we gave there without
proof.
What ever happened to the state | IV ⟩?! Well, it is a spin-zero system, so it
has only one state—it is the same in all coordinate systems. We can check that
everything works out by taking the diﬀerence of Eq. (12.50) and (12.51); we get
that
| + −⟩−| −+⟩= (ad −bc){| +′ −′⟩−| −′ +′⟩}.
But (ad −bc) is the determinant of the spin one-half matrix, and so is equal to 1.
We get that
| IV ′⟩= | IV ⟩
for any relative orientation of the two coordinate frames.
12-30","109"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","344","13
Propagation in a Crystal Lattice
13-1 States for an electron in a one-dimensional lattice
You would, at ﬁrst sight, think that a low-energy electron would have great
diﬃculty passing through a solid crystal. The atoms are packed together with
their centers only a few angstroms apart, and the eﬀective diameter of the atom
for electron scattering is roughly an angstrom or so. That is, the atoms are
large, relative to their spacing, so that you would expect the mean free path
between collisions to be of the order of a few angstroms—which is practically
nothing. You would expect the electron to bump into one atom or another almost
immediately. Nevertheless, it is a ubiquitous phenomenon of nature that if the
lattice is perfect, the electrons are able to travel through the crystal smoothly
and easily—almost as if they were in a vacuum. This strange fact is what lets
metals conduct electricity so easily; it has also permitted the development of
many practical devices. It is, for instance, what makes it possible for a transistor
to imitate the radio tube. In a radio tube electrons move freely through a vacuum,
while in the transistor they move freely through a crystal lattice. The machinery
behind the behavior of a transistor will be described in this chapter; the next
one will describe the application of these principles in various practical devices.
The conduction of electrons in a crystal is one example of a very common
phenomenon. Not only can electrons travel through crystals, but other “things”
like atomic excitations can also travel in a similar manner. So the phenomenon
which we want to discuss appears in many ways in the study of the physics of
the solid state.
You will remember that we have discussed many examples of two-state systems.
Let’s now think of an electron which can be in either one of two positions, in
each of which it is in the same kind of environment. Let’s also suppose that there
is a certain amplitude to go from one position to the other, and, of course, the
same amplitude to go back, just as we have discussed for the hydrogen molecular
ion in Section 10-1. The laws of quantum mechanics then give the following
13-1","374"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","345","results. There are two possible states of deﬁnite energy for the electron. Each
state can be described by the amplitude for the electron to be in each of the two
basic positions. In either of the deﬁnite-energy states, the magnitudes of these
two amplitudes are constant in time, and the phases vary in time with the same
frequency. On the other hand, if we start the electron in one position, it will
later have moved to the other, and still later will swing back again to the ﬁrst
position. The amplitude is analogous to the motions of two coupled pendulums.
Now consider a perfect crystal lattice in which we imagine that an electron
can be situated in a kind of “pit” at one particular atom and with some particular
energy. Suppose also that the electron has some amplitude to move into a diﬀerent
pit at one of the nearby atoms. It is something like the two-state system—but
with an additional complication. When the electron arrives at the neighboring
atom, it can afterward move on to still another position as well as return to its
starting point. Now we have a situation analogous not to two coupled pendulums,
but to an inﬁnite number of pendulums all coupled together. It is something like
what you see in one of those machines—made with a long row of bars mounted on
a torsion wire—that is used in ﬁrst-year physics to demonstrate wave propagation.
If you have a harmonic oscillator which is coupled to another harmonic
oscillator, and that one to another, and so on . . . , and if you start an irregularity
in one place, the irregularity will propagate as a wave along the line. The same
situation exists if you place an electron at one atom of a long chain of atoms.
Usually, the simplest way of analyzing the mechanical problem is not to think
in terms of what happens if a pulse is started at a deﬁnite place, but rather in
terms of steady-wave solutions. There exist certain patterns of displacements
which propagate through the crystal as a wave of a single, ﬁxed frequency. Now
the same thing happens with the electron—and for the same reason, because it’s
described in quantum mechanics by similar equations.
You must appreciate one thing, however; the amplitude for the electron to be
at a place is an amplitude, not a probability. If the electron were simply leaking
from one place to another, like water going through a hole, the behavior would be
completely diﬀerent. For example, if we had two tanks of water connected by a
tube to permit some leakage from one to the other, then the levels would approach
each other exponentially. But for the electron, what happens is amplitude leakage
and not just a plain probability leakage. And it’s a characteristic of the imaginary
term—the i in the diﬀerential equations of quantum mechanics—which changes
the exponential solution to an oscillatory solution. What happens then is quite
diﬀerent from the leakage between interconnected tanks.
13-2","503"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","346","(a)
b
Atom
. . .
n−3 n−2 n−1
n
n+1 n+2 n+3
. . .
(b)
Electron
| n−1⟩
(c)
| n⟩
(d)
| n+1⟩
Fig. 13-1. The base states of an electron in a one-dimensional crystal.
We want now to analyze quantitatively the quantum mechanical situation.
Imagine a one-dimensional system made of a long line of atoms as shown in
Fig. 13-1(a). (A crystal is, of course, three-dimensional but the physics is very
much the same; once you understand the one-dimensional case you will be able
to understand what happens in three dimensions.) Next, we want to see what
happens if we put a single electron on this line of atoms. Of course, in a real
crystal there are already millions of electrons. But most of them (nearly all for an
insulating crystal) take up positions in some pattern of motion each around its
own atom—and everything is quite stationary. However, we now want to think
about what happens if we put an extra electron in. We will not consider what
the other ones are doing because we suppose that to change their motion involves
a lot of excitation energy. We are going to add an electron as if to produce one
slightly bound negative ion. In watching what the one extra electron does we are
making an approximation which disregards the mechanics of the inside workings
of the atoms.
Of course the electron could then move to another atom, transferring the
negative ion to another place. We will suppose that just as in the case of an
electron jumping between two protons, the electron can jump from one atom to
the neighbor on either side with a certain amplitude.
Now how do we describe such a system? What will be reasonable base states?
If you remember what we did when we had only two possible positions, you can
13-3","312"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","347","guess how it will go. Suppose that in our line of atoms the spacings are all equal;
and that we number the atoms in sequence, as shown in Fig. 13-1(a). One of
the base states is that the electron is at atom number 6, another base state is
that the electron is at atom number 7, or at atom number 8, and so on. We
can describe the nth base state by saying that the electron is at atom number n.
Let’s say that this is the base state | n⟩. Figure 13-1 shows what we mean by the
three base states
| n −1⟩, | n⟩, and | n + 1⟩.
Using these base states, any state | φ⟩of the electron in our one-dimensional
crystal can be described by giving all the amplitudes ⟨n | φ⟩that the state | φ⟩is
in one of the base states—which means the amplitude that it is located at one
particular atom. Then we can write the state | φ⟩as a superposition of the base
states
| φ⟩=
X
n
| n⟩⟨n | φ⟩.
(13.1)
Next, we are going to suppose that when the electron is at one atom, there
is a certain amplitude that it will leak to the atom on either side. And we’ll
take the simplest case for which it can only leak to the nearest neighbors—to
get to the next-nearest neighbor, it has to go in two steps. We’ll take that the
amplitudes for the electron jump from one atom to the next is iA/ℏ(per unit
time).
For the moment we would like to write the amplitude ⟨n | φ⟩to be on the
nth atom as Cn. Then Eq. (13.1) will be written
| φ⟩=
X
n
| n⟩Cn.
(13.2)
If we knew each of the amplitudes Cn at a given moment, we could take their
absolute squares and get the probability that you would ﬁnd the electron if you
looked at atom n at that time.
What will the situation be at some later time? By analogy with the two-state
systems we have studied, we would propose that the Hamiltonian equations for
this system should be made up of equations like this:
iℏdCn(t)
dt
= E0Cn(t) −ACn+1(t) −ACn−1(t).
(13.3)
The ﬁrst coeﬃcient on the right, E0, is, physically, the energy the electron
would have if it couldn’t leak away from one of the atoms. (It doesn’t matter
13-4","399"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","348","what we call E0; as we have seen many times, it represents really nothing but
our choice of the zero of energy.) The next term represents the amplitude per
unit time that the electron is leaking into the nth pit from the (n + 1)st pit; and
the last term is the amplitude for leakage from the (n −1)st pit. As usual, we’ll
assume that A is a constant (independent of t).
For a full description of the behavior of any state | φ⟩, we would have one
equation like (13.3) for every one of the amplitudes Cn.
Since we want to
consider a crystal with a very large number of atoms, we’ll assume that there
are an indeﬁnitely large number of states—that the atoms go on forever in both
directions. (To do the ﬁnite case, we will have to pay special attention to what
happens at the ends.) If the number N of our base states is indeﬁnitely large,
then also our full Hamiltonian equations are inﬁnite in number! We’ll write down
just a sample:
...
...
iℏdCn−1
dt
= E0Cn−1 −ACn−2 −ACn,
iℏdCn
dt
= E0Cn −ACn−1 −ACn+1,
iℏdCn+1
dt
= E0Cn+1 −ACn −ACn+2,
...
...
(13.4)
13-2 States of deﬁnite energy
We could study many things about an electron in a lattice, but ﬁrst let’s try
to ﬁnd the states of deﬁnite energy. As we have seen in earlier chapters this
means that we have to ﬁnd a situation in which the amplitudes all change at the
same frequency if they change with time at all. We look for solutions of the form
Cn = ane−iEt/ℏ.
(13.5)
The complex numbers an tell us about the non-time-varying part of the amplitude
to ﬁnd the electron at the nth atom. If we put this trial solution into the equations
of (13.4) to test them out, we get the result
Ean = E0an −Aan+1 −Aan−1.
(13.6)
13-5","317"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","349","We have an inﬁnite number of such equations for the inﬁnite number of un-
knowns an—which is rather petrifying.
All we have to do is take the determinant . . . but wait! Determinants are
ﬁne when there are 2, 3, or 4 equations. But if there are a large number—or an
inﬁnite number—of equations, the determinants are not very convenient. We’d
better just try to solve the equations directly. First, let’s label the atoms by their
positions; we’ll say that the atom n is at xn and the atom (n + 1) is at xn+1. if
the atomic spacing is b—as in Fig. 13-1—we will have that xn+1 = xn + b. By
choosing our origin at atom zero, we can even have it that xn = nb. We can
rewrite Eq. (13.5) as
Cn = a(xn)e−iEt/ℏ,
(13.7)
and Eq. (13.6) would become
Ea(xn) = E0a(xn) −Aa(xn+1) −Aa(xn−1).
(13.8)
Or, using the fact that xn+1 = xn + b, we could also write
Ea(xn) = E0a(xn) −Aa(xn + b) −Aa(xn −b).
(13.9)
This equation is somewhat similar to a diﬀerential equation. It tells us that a
quantity, a(x), at one point, (xn), is related to the same physical quantity at
some neighboring points, (xn ± b). (A diﬀerential equation relates the value of a
function at a point to the values at inﬁnitesimally nearby points.) Perhaps the
methods we usually use for solving diﬀerential equations will also work here; let’s
try.
Linear diﬀerential equations with constant coeﬃcients can always be solved
in terms of exponential functions. We can try the same thing here; let’s take as
a trial solution
a(xn) = eikxn.
(13.10)
Then Eq. (13.9) becomes
Eeikxn = E0eikxn −Aeik(xn+b) −Aeik(xn−b).
(13.11)
We can now divide out the common factor eikxn; we get
E = E0 −Aeikb −Ae−ikb.
(13.12)
13-6","302"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","350","The last two terms are just equal to (2A cos kb), so
E = E0 −2A cos kb.
(13.13)
We have found that for any choice at all for the constant k there is a solution
whose energy is given by this equation. There are various possible energies
depending on k, and each k corresponds to a diﬀerent solution. There are an
inﬁnite number of solutions—which is not surprising, since we started out with
an inﬁnite number of base states.
Let’s see what these solutions mean.
For each k, the a’s are given by
Eq. (13.10). The amplitudes Cn are then given by
Cn = eikxne−(i/ℏ)Et,
(13.14)
where you should remember that the energy E also depends on k as given in
Eq. (13.13). The space dependence of the amplitudes is eikxn. The amplitudes
oscillate as we go along from one atom to the next.
We mean that, in space, the amplitude goes as a complex oscillation—the
magnitude is the same at every atom, but the phase at a given time advances by
the amount (ikb) from one atom to the next. We can visualize what is going on
by plotting a vertical line to show just the real part at each atom as we have done
in Fig. 13-2. The envelope of these vertical lines (as shown by the broken-line
curve) is, of course, a cosine curve. The imaginary part of Cn is also an oscillating
function, but is shifted 90◦in phase so that the absolute square (which is the
sum of the squares of the real and imaginary parts) is the same for all the C’s.
Thus if we pick a k, we get a stationary state of a particular energy E. And
for any such state, the electron is equally likely to be found at every atom—there
x
Re Cn
xn
Fig. 13-2. Variation of the real part of Cn with xn.
13-7","316"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","351","is no preference for one atom or the other. Only the phase is diﬀerent for diﬀerent
atoms. Also, as time goes on the phases vary. From Eq. (13.14) the real and
imaginary parts propagate along the crystal as waves—namely as the real or
imaginary parts of
ei[kxn−(E/ℏ)t].
(13.15)
The wave can travel toward positive or negative x depending on the sign we have
picked for k.
Notice that we have been assuming that the number k that we put in our
trial solution, Eq. (13.10), was a real number. We can see now why that must
be so if we have an inﬁnite line of atoms. Suppose that k were an imaginary
number, say ik′. Then the amplitudes an would go as ek′xn, which means that
the amplitude would get larger and larger as we go toward large x’s—or toward
large negative x’s if k′ is a negative number. This kind of solution would be
O.K. if we were dealing with line of atoms that ended, but cannot be a physical
solution for an inﬁnite chain of atoms. It would give inﬁnite amplitudes—and,
therefore, inﬁnite probabilities—which can’t represent a real situation. Later on
we will see an example in which an imaginary k does make sense.
The relation between the energy E and the wave number k as given in
Eq. (13.13) is plotted in Fig. 13-3. As you can see from the ﬁgure, the energy
can go from (E0 −2A) at k = 0 to (E0 + 2A) at k = ±π/b. The graph is plotted
for positive A; if A were negative, the curve would simply be inverted, but the
range would be the same. The signiﬁcant result is that any energy is possible
within a certain range or “band” of energies, but no others. According to our
assumptions, if an electron in a crystal is in a stationary state, it can have no
energy other than values in this band.
According to Eq. (13.13), the smallest k’s correspond to low-energy states—
E ≈(E0 −2A). As k increases in magnitude (toward either positive or negative
values) the energy at ﬁrst increases, but then reaches a maximum at k = ±π/b,
as shown in Fig. 13-3. For k’s larger than π/b, the energy would start to decrease
again. But we do not really need to consider such values of k, because they do not
give new states—they just repeat states we already have for smaller k. We can
see that in the following way. Consider the lowest energy state for which k = 0.
The coeﬃcient a(xn) is the same for all xn. Now we would get the same energy
for k = 2π/b. But then, using Eq. (13.10), we have that
a(xn) = ei(2π/b)xn.
13-8","457"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","352","k
E
−π/b
0
π/b
E0 + 2A
E0
E0 −2A
Fig. 13-3. The energy of the stationary states as a function of the
parameter k.
However, taking x0 to be at the origin, we can set xn = nb; then a(xn) becomes
a(xn) = ei2πn = 1.
The state described by these a(xn) is physically the same state we got for k = 0.
It does not represent a diﬀerent solution.
As another example, suppose that k were −π/4b. The real part of a(xn)
would vary as shown by curve 1 in Fig. 13-4. If k were 7π/4b, the real part
of a(xn) would vary as shown by curve 2 in the ﬁgure. (The complete cosine
curves don’t mean anything, of course; all that matters is their values at the
points xn. The curves are just to help you see how things are going.) You see
that both values of k give the same amplitudes at all of the xn’s.
x
Re a(xn)
1
2
Fig. 13-4. Two values of k which represent the same physical situation;
curve 1 is for k = −π/4b, curve 2 is for k = 7π/4b.
13-9","193"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","353","The upshot is that we have all the possible solutions of our problem if we
take only k’s in a certain limited range. We’ll pick the range between −π/b
and +π/b—the one shown in Fig. 13-3. In this range, the energy of the stationary
states increases uniformly with an increase in the magnitude of k.
One side remark about something you can play with. Suppose that the
electron cannot only jump to the nearest neighbor with amplitude iA/ℏ, but
also has the possibility to jump in one direct leap to the next nearest neighbor
with some other amplitude iB/ℏ. You will ﬁnd that the solution can again be
written in the form an = eikxn—this type of solution is universal. You will
also ﬁnd that the stationary states with wave number k have an energy equal
to (E0 −2A cos kb −2B cos 2kb). This shows that the shape of the curve of E
against k is not universal, but depends upon the particular assumptions of the
problem. It is not always a cosine wave—it’s not even necessarily symmetrical
about some horizontal line. It is true, however, that the curve always repeats
itself outside of the interval from −π/b to π/b, so you never need to worry about
other values of k.
Let’s look a little more closely at what happens for small k—that is, when
the variations of the amplitudes from one xn to the next are quite slow. Suppose
we choose our zero of energy by deﬁning E0 = 2A; then the minimum of the
curve in Fig. 13-3 is at the zero of energy. For small enough k, we can write that
cos kb ≈1 −k2b2/2,
and the energy of Eq. (13.13) becomes
E = Ak2b2.
(13.16)
We have that the energy of the state is proportional to the square of the wave
number which describes the spatial variations of the amplitudes Cn.
13-3 Time-dependent states
In this section we would like to discuss the behavior of states in the one-
dimensional lattice in more detail. If the amplitude for an electron to be at xn
is Cn, the probability of ﬁnding it there is |Cn|2. For the stationary states
described by Eq. (13.14), this probability is the same for all xn and does not
change with time. How can we represent a situation which we would describe
13-10","392"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","354","roughly by saying an electron of a certain energy is localized in a certain region—
so that it is more likely to be found at one place than at some other place? We
can do that by making a superposition of several solutions like Eq. (13.14) with
slightly diﬀerent values of k—and, therefore, slightly diﬀerent energies. Then
at t = 0, at least, the amplitude Cn will vary with position because of the
interference between the various terms, just as one gets beats when there is a
mixture of waves of diﬀerent wavelengths (as we discussed in Chapter 48, Vol. I).
So we can make up a “wave packet” with a predominant wave number k0, but
with various other wave numbers near k0.†
In our superposition of stationary states, the amplitudes with diﬀerent k’s will
represent states of slightly diﬀerent energies, and, therefore, of slightly diﬀerent
frequencies; the interference pattern of the total Cn will, therefore, also vary
with time—there will be a pattern of “beats.” As we have seen in Chapter 48 of
Volume I, the peaks of the beats [the place where |C(xn)|2 is large] will move
along in x as time goes on; they move with the speed we have called the “group
velocity.” We found that this group velocity was related to the variation of k with
frequency by
vgroup = dω
dk ;
(13.17)
the same derivation would apply equally well here. An electron state which is
a “clump”—namely one for which the Cn vary in space like the wave packet of
Fig. 13-5—will move along our one-dimensional “crystal” with the speed v equal
to dω/dk, where ω = E/ℏ. Using (13.16) for E, we get that
v = 2Ab2
ℏ
k.
(13.18)
In other words, the electrons move along with a speed proportional to the
typical k. Equation (13.16) then says that the energy of such an electron is
proportional to the square of its velocity—it acts like a classical particle. So long
as we look at things on a scale gross enough that we don’t see the ﬁne structure,
our quantum mechanical picture begins to give results like classical physics. In
fact, if we solve Eq. (13.18) for k and substitute into (13.16), we can write
E = 1
2meﬀv2,
(13.19)
† Provided we do not try to make the packet too narrow.
13-11","391"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","355","x
Re C(xn)
v
Fig. 13-5. The real part of C(xn) as a function of x for a superposition
of several states of similar energy. (The spacing b is very small on the
scale of x shown.)
where meﬀis a constant. The extra “energy of motion” of the electron in a packet
depends on the velocity just as for a classical particle. The constant meﬀ—called
the “eﬀective mass”—is given by
meﬀ=
ℏ2
2Ab2 .
(13.20)
Also notice that we can write
meﬀv = ℏk.
(13.21)
If we choose to call meﬀv the “momentum,” it is related to the wave number k
in the way we have described earlier for a free particle.
Don’t forget that meﬀhas nothing to do with the real mass of an electron. It
may be quite diﬀerent—although in real crystals it often happens to turn out to
be the same general order of magnitude, about 2 to 20 times the free-space mass
of the electron.
We have now explained a remarkable mystery—how an electron in a crystal
(like an extra electron put into germanium) can ride right through the crystal
and ﬂow perfectly freely even though it has to hit all the atoms. It does so by
having its amplitudes going pip-pip-pip from one atom to the next, working its
way through the crystal. That is how a solid can conduct electricity.
13-4 An electron in a three-dimensional lattice
Let’s look for a moment at how we could apply the same ideas to see what
happens to an electron in three dimensions. The results turn out to be very
13-12","264"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","356","similar. Suppose we have a rectangular lattice of atoms with lattice spacings
of a, b, c in the three directions. (If you want a cubic lattice, take the three
spacings all equal.) Also suppose that the amplitude to leap in the x-direction to
a neighbor is (iAx/ℏ), to leap in the y-direction is (iAy/ℏ), and to leap in the
z-direction is (iAz/ℏ). Now how should we describe the base states? As in the
one-dimensional case, one base state is that the electron is at the atom whose
locations are x, y, z, where (x, y, z) is one of the lattice points. Choosing our
origin at one atom, these points are all at
x = nxa,
y = nyb,
and
z = nzc,
where nx, ny, nz are any three integers. Instead of using subscripts to indicate
such points, we will now just use x, y, and z, understanding that they take on
only their values at the lattice points. Thus the base state is represented by the
symbol | electron at x, y, z⟩, and the amplitude for an electron in some state | ψ⟩
to be in this base state is C(x, y, z) = ⟨electron at x, y, z | ψ⟩.
As before, the amplitudes C(x, y, z) may vary with time. With our assump-
tions, the Hamiltonian equations should be like this:
iℏdC(x, y, z)
dt
= E0C(x, y, z) −AxC(x + a, y, z) −AxC(x −a, y, z)
−AyC(x, y + b, z) −AyC(x, y −b, z)
−AzC(x, y, z + c) −AzC(x, y, z −c).
(13.22)
It looks rather long, but you can see where each term comes from.
Again we can try to ﬁnd a stationary state in which all the C’s vary with
time in the same way. Again the solution is an exponential:
C(x, y, z) = e−iEt/ℏei(kxx+kyy+kzz).
(13.23)
If you substitute this into (13.22) you see that it works, provided that the energy E
is related to kx, ky, and kz in the following way:
E = E0 −2Ax cos kxa −2Ay cos kyb −2Az cos kzc.
(13.24)
The energy now depends on the three wave numbers kx, ky, kz, which, incidentally,
are the components of a three-dimensional vector k.
In fact, we can write
Eq. (13.23) in vector notation as
C(x, y, z) = e−iEt/ℏeik·r.
(13.25)
13-13","388"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","357","The amplitude varies as a complex plane wave in three dimensions, moving in
the direction of k, and with the wave number k = (kx + ky + kz)1/2.
The energy associated with these stationary states depends on the three
components of k in the complicated way given in Eq. (13.24). The nature, of
the variation of E with k depends on relative signs and magnitudes of Ax, Ay,
and Az. If these three numbers are all positive, and if we are interested in small
values of k, the dependence is relatively simple.
Expanding the cosines as we did before to get Eq. (13.16), we can now get
that
E = Emin + Axa2k2
x + Ayb2k2
y + Azc2k2
z.
(13.26)
For a simple cubic lattice with lattice spacing a we expect that Ax and Ay
and Az would be equal—say all are just A—and we would have just
E = Emin + Aa2(k2
x + k2
y + k2
z),
or
E = Emin + Aa2k2.
(13.27)
This is just like Eq. (13.16). Following the arguments used there, we would
conclude that an electron packet in three dimensions (made up by superposing
many states with nearly equal energies) also moves like a classical particle with
some eﬀective mass.
In a crystal with a lower symmetry than cubic (or even in a cubic crystal
in which the state of the electron at each atom is not symmetrical) the three
coeﬃcients Ax, Ay, and Az are diﬀerent. Then the “eﬀective mass” of an electron
localized in a small region depends on its direction of motion. It could, for
instance, have a diﬀerent inertia for motion in the x-direction than for motion
in the y-direction. (The details of such a situation are sometimes described in
terms of an “eﬀective mass tensor.”)
13-5 Other states in a lattice
According to Eq. (13.24) the electron states we have been talking about can
have energies only in a certain “band” of energies which covers the energy range
from the minimum energy
E0 −2(Ax + Ay + Az)
13-14","344"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","358","to the maximum energy
E0 + 2(Ax + Ay + Az).
Other energies are possible, but they belong to a diﬀerent class of electron states.
For the states we have described, we imagined base states in which an electron is
placed on an atom of the crystal in some particular state, say the lowest energy
state.
If you have an atom in empty space, and add an electron to make an ion,
the ion can be formed in many ways. The electron can go on in such a way as
to make the state of lowest energy, or it can go on to make one or another of
many possible “excited states” of the ion each with a deﬁnite energy above the
lowest energy. The same thing can happen in a crystal. Let’s suppose that the
energy E0 we picked above corresponds to base states which are ions of the lowest
possible energy. We could also imagine a new set of base states in which the
electron sits near the nth atom in a diﬀerent way—in one of the excited states
of the ion—so that the energy E0 is now quite a bit higher. As before there is
some amplitude A (diﬀerent from before) that the electron will jump from its
excited state at one atom to the same excited state at a neighboring atom. The
whole analysis goes as before; we ﬁnd a band of possible energies centered at a
higher energy. There can, in general, be many such bands each corresponding to
a diﬀerent level of excitation.
There are also other possibilities. There may be some amplitude that the
electron jumps from an excited condition at one atom to an unexcited condition at
the next atom. (This is called an interaction between bands.) The mathematical
theory gets more and more complicated as you take into account more and more
bands and add more and more coeﬃcients for leakage between the possible states.
No new ideas are involved, however; the equations are set up much as we have
done in our simple example.
We should remark also that there is not much more to be said about the various
coeﬃcients, such as the amplitude A, which appear in the theory. Generally they
are very hard to calculate, so in practical cases very little is known theoretically
about these parameters and for any particular real situation we can only take
values determined experimentally.
There are other situations where the physics and mathematics are almost
exactly like what we have found for an electron moving in a crystal, but in which
the “object” that moves is quite diﬀerent. For instance, suppose that our original
crystal—or rather linear lattice—was a line of neutral atoms, each with a loosely
13-15","457"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","359","bound outer electron. Then imagine that we were to remove one electron. Which
atom has lost its electron? Let Cn now represent the amplitude that the electron
is missing from the atom at xn. There will, in general, be some amplitude iA/ℏ
that the electron at a neighboring atom—say the (n −1)st atom—will jump to
the nth leaving the (n −1)st atom without its electron. This is the same as
saying that there is an amplitude A for the “missing electron” to jump from the
nth atom to the (n −1)st atom. You can see that the equations will be exactly
the same—of course, the value of A need not be the same as we had before.
Again we will get the same formulas for the energy levels, for the “waves” of
probability which move through the crystal with the group velocity of Eq. (13.18),
for the eﬀective mass, and so on. Only now the waves describe the behavior of the
missing electron—or “hole” as it is called. So a “hole” acts just like a particle with
a certain mass meﬀ. You can see that this particle will appear to have a positive
charge. We’ll have some more to say about such holes in the next chapter.
As another example, we can think of a line of identical neutral atoms one of
which has been put into an excited state—that is, with more than its normal
ground state energy.
Let Cn be the amplitude that the nth atom has the
excitation. It can interact with a neighboring atom by handing over to it the
extra energy and returning to the ground state. Call the amplitude for this
process iA/ℏ. You can see that it’s the same mathematics all over again. Now
the object which moves is called an exciton. It behaves like a neutral “particle”
moving through the crystal, carrying the excitation energy. Such motion may
be involved in certain biological processes such as vision, or photosynthesis. It
has been guessed that the absorption of light in the retina produces an “exciton”
which moves through some periodic structure (such as the layers in the rods we
described in Chapter 36, Vol. I; see Fig. 36-5) to be accumulated at some special
station where the energy is used to induce a chemical reaction.
13-6 Scattering from imperfections in the lattice
We want now to consider the case of a single electron in a crystal which is not
perfect. Our earlier analysis says that perfect crystals have perfect conductivity—
that electrons can go slipping through the crystal, as in a vacuum, without friction.
One of the most important things that can stop an electron from going on forever
is an imperfection or irregularity in the crystal. As an example, suppose that
somewhere in the crystal there is a missing atom; or suppose that someone put
one wrong atom at one of the atomic sites so that things there are diﬀerent
13-16","489"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","360","than at the other atomic sites. Say the energy E0 or the amplitude A could be
diﬀerent. How would we describe what happens then?
To be speciﬁc, we will return to the one-dimensional case and we will assume
that atom number “zero” is an “impurity” atom and has a diﬀerent value of E0
than any of the other atoms. Let’s call this energy (E0 + F). What happens?
When an electron arrives at atom “zero” there is some probability that the
electron is scattered backwards. If a wave packet is moving along and it reaches
a place where things are a little bit diﬀerent, some of it will continue onward and
some of it will bounce back. It’s quite diﬃcult to analyze such a situation using
a wave packet, because everything varies in time. It is much easier to work with
steady-state solutions. So we will work with stationary states, which we will ﬁnd
can be made up of continuous waves which have transmitted and reﬂected parts.
In three dimensions we would call the reﬂected part the scattered wave, since it
would spread out in various directions.
We start out with a set of equations which are just like the ones in Eq. (13.6)
except that the equation for n = 0 is diﬀerent from all the rest. The ﬁve equations
for n = −2, −1, 0, +1, and +2 look like this:
...
...
Ea−2 = E0a−2 −Aa−1 −Aa−3,
Ea−1 = E0a−1 −Aa0 −Aa−2,
Ea0 = (E0 + F)a0 −Aa1 −Aa−1,
Ea1 = E0a1 −Aa2 −Aa0,
Ea2 = E0a2 −Aa3 −Aa1,
...
...
(13.28)
There are, of course, all the other equations for |n| is greater than 2. They will
look just like Eq. (13.6).
For the general case, we really ought to use a diﬀerent A for the amplitude
that the electron jumps to or from atom “zero,” but the main features of what
goes on will come out of a simpliﬁed example in which all the A’s are equal.
Equation (13.10) would still work as a solution for all of the equations except
the one for atom “zero”—it isn’t right for that one equation. We need a diﬀerent
solution which we can cook up in the following way. Equation (13.10) represents
13-17","375"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","361","a wave going in the positive x-direction. A wave going in the negative x-direction
would have been an equally good solution. It would be written
a(xn) = e−ikxn.
The most general solution we could have taken for Eq. (13.6) would be a combi-
nation of a forward and a backward wave, namely
an = αeikxn + βe−ikxn.
(13.29)
This solution represents a complex wave of amplitude α moving in the +x-
direction and a wave of amplitude β moving in the −x-direction.
Now take a look at the set of equations for our new problem—the ones
in (13.28) together with those for all the other atoms. The equations involving an’s
with n ≤−1 are all satisﬁed by Eq. (13.29), with the condition that k is related
to E and the lattice spacing b by
E = E0 −2A cos kb.
(13.30)
The physical meaning is an “incident” wave of amplitude α approaching atom
“zero” (the “scatterer”) from the left, and a “scattered” or “reﬂected” wave of
amplitude β going back toward the left. We do not loose any generality if we set
the amplitude α of the incident wave equal to 1. Then the amplitude β is, in
general, a complex number.
We can say all the same things about the solutions of an for n ≥1. The
coeﬃcients could be diﬀerent, so we would have for them
an = γeikxn + δe−ikxn,
for
n ≥1.
(13.31)
Here, γ is the amplitude of a wave going to the right and δ a wave coming from
the right. We want to consider the physical situation in which a wave is originally
started only from the left, and there is only a “transmitted” wave that comes out
beyond the scatterer—or impurity atom. We will try for a solution in which δ = 0.
We can, certainly, satisfy all of the equations for the an except for the middle
three in Eq. (13.28) by the following trial solutions.
an (for n < 0) = eikxn + βe−ikxn,
an (for n > 0) = γeikxn.
(13.32)
The situation we are talking about is illustrated in Fig. 13-6.
13-18","354"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","362","n →−4
−3
−2
−1
0
1
2
3
4
1
β
γ
INCIDENT WAVE
SCATTERED WAVE
TRANSMITTED WAVE
Fig. 13-6. Waves in a one-dimensional lattice with one “impurity”
atom at n = 0.
By using the formulas in Eq. (13.32) for a−1 and a+1, the three middle
equations of Eq. (13.28) will allow us to solve for a0 and also for the two coeﬃcients
β and γ. So we have found a complete solution. Setting xn = nb, we have to
solve the three equations
(E −E0){eik(−b) + βe−ik(−b)} = −A{a0 + eik(−2b) + βe−ik(−2b)},
(E −E0 −F)a0 = −A{γeikb + eik(−b) + βe−ik(−b)},
(E −E0)γeikb = −A{γeik(2b) + a0}.
(13.33)
Remember that E is given in terms of k by Eq. (13.30). If you substitute this
value for E into the equations, and remember that cos x = 1
2(eix + e−ix), you
get from the ﬁrst equation that
a0 = 1 + β,
(13.34)
and from the third equation that
a0 = γ.
(13.35)
These are consistent only if
γ = 1 + β.
(13.36)
This equation says that the transmitted wave (γ) is just the original incident
wave (1) with an added wave (β) equal to the reﬂected wave. This is not always
true, but happens to be so for a scattering at one atom only. If there were a
clump of impurity atoms, the amount added to the forward wave would not
necessarily be the same as the reﬂected wave.
13-19","246"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","363","We can get the amplitude β of the reﬂected wave from the middle equation
of Eq. (13.33); we ﬁnd that
β =
−F
F −2iA sin kb.
(13.37)
We have the complete solution for the lattice with one unusual atom.
You may be wondering how the transmitted wave can be “more” than the
incident wave as it appears in Eq. (13.34). Remember, though, that β and γ are
complex numbers and that the number of particles (or rather, the probability
of ﬁnding a particle) in a wave is proportional to the absolute square of the
amplitude. In fact, there will be “conservation of electrons” only if
|β|2 + |γ|2 = 1.
(13.38)
You can show that this is true for our solution.
13-7 Trapping by a lattice imperfection
There is another interesting situation that can arise if F is a negative number.
If the energy of the electron is lower at the impurity atom (at n = 0) than it
is anywhere else, then the electron can get caught on this atom. That is, if
(E0 + F) is below the bottom of the band at (E0 −2A), then the electron can
get “trapped” in a state with E < E0 −2A. Such a solution cannot come out of
what we have done so far. We can get this solution, however, if we permit the
trial solution we took in Eq. (13.10) to have an imaginary number for k. Let’s
set k = ±iκ. Again, we can have diﬀerent solutions for n < 0 and for n > 0. A
possible solution for n < 0 might be
an (for n < 0) = ce+κxn.
(13.39)
We have to take a plus sign in the exponent; otherwise the amplitude would get
indeﬁnitely large for large negative values of n. Similarly, a possible solution
for n > 0 would be
an (for n > 0) = c′e−κxn.
(13.40)
If we put these trial solutions into Eq. (13.28) all but the middle three are
satisﬁed provided that
E = E0 −A(eκb + e−κb).
(13.41)
13-20","343"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","364","Since the sum of the two exponential terms is always greater than 2, this energy
is below the regular band, and is what we are looking for. The remaining three
equations in Eq. (13.28) are satisﬁed if a0 = c = c′ and if κ is chosen so that
A(eκb −e−κb) = −F.
(13.42)
Combining this equation with Eq. (13.41) we can ﬁnd the energy of the trapped
electron; we get
E = E0 −
p
4A2 + F 2.
(13.43)
The trapped electron has a unique energy—located somewhat below the conduc-
tion band.
Notice that the amplitudes we have in Eq. (13.39) and (13.40) do not say that
the trapped electron sits right on the impurity atom. The probability of ﬁnding
the electron at nearby atoms is given by the square of these amplitudes. For one
particular choice of the parameters it might vary as shown in the bar graph of
Fig. 13-7. The probability is greatest for ﬁnding the electron on the impurity atom.
For nearby atoms the probability drops oﬀexponentially with the distance from
the impurity atom. This is another example of “barrier penetration.” From the
point-of-view of classical physics the electron doesn’t have enough energy to get
away from the energy “hole” at the trapping center. But quantum mechanically
it can leak out a little way.
n →−4
−3
−2
−1
0
1
2
3
4
x
PROBABILITY
Impurity Atom
c2e+2κx
c2e−2κx
Fig. 13-7. The relative probabilities of ﬁnding a trapped electron at
atomic sites near the trapping impurity atom.
13-8 Scattering amplitudes and bound states
Finally, our example can be used to illustrate a point which is very useful
these days in the physics of high-energy particles. It has to do with a relationship
13-21","290"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","365","between scattering amplitudes and bound states. Suppose we have discovered—
through experiment and theoretical analysis—the way that pions scatter from
protons. Then a new particle is discovered and someone wonders whether maybe
it is just a combination of a pion and a proton held together in some bound state
(in an analogy to the way an electron is bound to a proton to make a hydrogen
atom). By a bound state we mean a combination which has a lower energy than
the two free-particles.
There is a general theory which says that a bound state will exist at that energy
at which the scattering amplitude becomes inﬁnite if extrapolated algebraically
(the mathematical term is “analytically continued”) to energy regions outside of
the permitted band.
The physical reason for this is as follows. A bound state is a situation in
which there are only waves tied on to a point and there’s no wave coming in to
get it started, it just exists there by itself. The relative proportion between the
so-called “scattered” or created wave and the wave being “sent in” is inﬁnite.
We can test this idea in our example. Let’s write our expression Eq. (13.37) for
the scattered amplitude directly in terms of the energy E of the particle being
scattered (instead of in terms of k). Since Equation (13.30) can be rewritten as
2A sin kb =
p
4A2 −(E −E0)2,
the scattered amplitude is
β =
−F
F −i
p
4A2 −(E −E0)2 .
(13.44)
From our derivation, this equation should be used only for real states—those with
energies in the energy band, E = E0 ± 2A. But suppose we forget that fact and
extend the formula into the “unphysical” energy regions where |E −E0| > 2A.
For these unphysical regions we can write†
p
4A2 −(E −E0)2 = i
p
(E −E0)2 −4A2.
Then the “scattering amplitude,” whatever it may mean, is
β =
−F
F +
p
(E −E0)2 −4A2 .
(13.45)
† The sign of the root to be chosen here is a technical point related to the allowed signs of κ
in Eqs. (13.39) and (13.40). We won’t go into it here.
13-22","362"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","366","Now we ask: Is there any energy E for which β becomes inﬁnite (i.e., for which
the expression for β has a “pole”)? Yes, so long as F is negative, the denominator
of Eq. (13.45) will be zero when
(E −E0)2 −4A2 = F 2,
or when
E = E0 ±
p
4A2 + F 2.
The minus sign gives just the energy we found in Eq. (13.43) for the trapped
energy.
What about the plus sign? This gives an energy above the allowed energy
band. And indeed there is another bound state there which we missed when we
solved the equations of Eq. (13.28). We leave it as a puzzle for you to ﬁnd the
energy and amplitudes an for this bound state.
The relation between scattering and bound states provides one of the most
useful clues in the current search for an understanding of the experimental
observations about the new strange particles.
13-23","155"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","367","14
Semiconductors
Reference: C. Kittel, Introduction to Solid State Physics, John Wiley and
Sons, Inc., New York, 2nd ed., 1956. Chapters 13, 14, and 18.
14-1 Electrons and holes in semiconductors
One of the remarkable and dramatic developments in recent years has been the
application of solid state science to technical developments in electrical devices
such as transistors. The study of semiconductors led to the discovery of their
useful properties and to a large number of practical applications. The ﬁeld is
changing so rapidly that what we tell you today may be incorrect next year. It
will certainly be incomplete. And it is perfectly clear that with the continuing
study of these materials many new and more wonderful things will be possible as
time goes on. You will not need to understand this chapter for what comes later
in this volume, but you may ﬁnd it interesting to see that at least something of
what you are learning has some relation to the practical world.
There are large numbers of semiconductors known, but we’ll concentrate on
those which now have the greatest technical application. They are also the ones
that are best understood, and in understanding them we will obtain a degree
of understanding of many of the others. The semiconductor substances in most
common use today are silicon and germanium. These elements crystallize in the
diamond lattice, a kind of cubic structure in which the atoms have tetrahedral
bonding with their four nearest neighbors.
They are insulators at very low
temperatures—near absolute zero—although they do conduct electricity somewhat
at room temperature. They are not metals; they are called semiconductors.
If we somehow put an extra electron into a crystal of silicon or germanium
which is at a low temperature, we will have just the situation we described in the
last chapter. The electron will be able to wander around in the crystal jumping
from one atomic site to the next. Actually, we have looked only at the behavior of
14-1","331"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","368","electrons in a rectangular lattice, and the equations would be somewhat diﬀerent
for the real lattice of silicon or germanium. All of the essential points are, however,
illustrated by the results for the rectangular lattice.
As we saw in Chapter 13, these electrons can have energies only in a certain
energy band—called the conduction band. Within this band the energy is related
to the wave-number k of the probability amplitude C (see Eq. (13.24)) by
E = E0 −2Ax cos kxa −2Ay cos kyb −2Az cos kzc.
(14.1)
The A’s are the amplitudes for jumping in the x-, y-, and z-directions, and a, b,
and c are the lattice spacings in these directions.
For energies near the bottom of the band, we can approximate Eq. (14.1) by
E = Emin + Axa2k2
x + Ayb2k2
y + Azc2k2
z
(14.2)
(see Section 13-4).
If we think of electron motion in some particular direction, so that the
components of k are always in the same ratio, the energy is a quadratic function
of the wave number—and as we have seen of the momentum of the electron. We
can write
E = Emin + αk2,
(14.3)
where α is some constant, and we can make a graph of E versus k as in Fig. 14-1.
We’ll call such a graph an “energy diagram.” An electron in a particular state of
energy and momentum can be indicated by a point such as S in the ﬁgure.
k
E
Emin
S
Fig. 14-1. The energy diagram for an electron in an insulating crystal.
14-2","260"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","369","As we also mentioned in Chapter 13, we can have a similar situation if we
remove an electron from a neutral insulator. Then, an electron can jump over
from a nearby atom and ﬁll the “hole,” but leaving another “hole” at the atom
it started from. We can describe this behavior by writing an amplitude to ﬁnd
the hole at any particular atom, and by saying that the hole can jump from
one atom to the next. (Clearly, the amplitudes A that the hole jumps from
atom a to atom b is just the same as the amplitude that an electron on atom b
jumps into the hole at atom a.) The mathematics is just the same for the hole
as it was for the extra electron, and we get again that the energy of the hole is
related to its wave number by an equation just like Eq. (14.1) or (14.2), except,
of course, with diﬀerent numerical values for the amplitudes Ax, Ay, and Az.
The hole has an energy related to the wave number of its probability amplitudes.
Its energy lies in a restricted band, and near the bottom of the band its energy
varies quadratically with the wave number—or momentum—just as in Fig. 14-1.
Following the arguments of Section 13-3, we would ﬁnd that the hole also behaves
like a classical particle with a certain eﬀective mass—except that in noncubic
crystals the mass depends on the direction of motion. So the hole behaves like a
positive particle moving through the crystal. The charge of the hole-particle is
positive, because it is located at the site of a missing electron; and when it moves
in one direction there are actually electrons moving in the opposite direction.
If we put several electrons into a neutral crystal, they will move around much
like the atoms of a low-pressure gas. If there are not too many, their interactions
will not be very important. If we then put an electric ﬁeld across the crystal, the
electrons will start to move and an electric current will ﬂow. Eventually they
would all be drawn to one edge of the crystal, and, if there is a metal electrode
there, they would be collected, leaving the crystal neutral.
Similarly we could put many holes into a crystal. They would roam around
at random unless there is an electric ﬁeld. With a ﬁeld they would ﬂow toward
the negative terminal, and would be “collected”—what actually happens is that
they are neutralized by electrons from the metal terminal.
One can also have both holes and electrons together. If there are not too
many, they will all go their way independently. With an electric ﬁeld, they will all
contribute to the current. For obvious reasons, electrons are called the negative
carriers and the holes are called the positive carriers.
We have so far considered that electrons are put into the crystal from the
outside, or are removed to make a hole. It is also possible to “create” an electron-
hole pair by taking a bound electron away from one neutral atom and putting it
14-3","516"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","370","some distance away in the same crystal. We then have a free electron and a free
hole, and the two can move about as we have described.
E
k
S
E−
min
E−
Fig. 14-2. The energy E−is required to “create” a free electron.
The energy required to put an electron into a state S—we say to “create” the
state S—is the energy E−shown in Fig. 14-2. It is some energy above E−
min. The
energy required to “create” a hole in some state S′ is the energy E+ of Fig. 14-3,
which is some energy greater than E+
min. Now if we create a pair in the states S
and S′, the energy required is just E−+ E+.
E
k
S′
E+
min
E+
Fig. 14-3. The energy E+ is required to “create” a hole in the state S′.
14-4","141"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","371","The creation of pairs is a common process (as we will see later), so many
people like to put Fig. 14-2 and Fig. 14-3 together on the same graph—with the
hole energy plotted downward, although it is, of course a positive energy. We
have combined our two graphs in this way in Fig. 14-4. The advantage of such
a graph is that the energy Epair = E−+ E+ required to create a pair with the
electron in S and the hole in S′ is just the vertical distance between S and S′ as
shown in Fig. 14-4. The minimum energy required to create a pair is called the
“gap” energy and is equal to E−
min + E+
min.
k
E−(electron)
E+ (hole)
(Positive energy downward)
ELECTRON
HOLE
E−
min
E+
min
Epair
S
S′
Fig. 14-4. Energy diagrams for an electron and a hole drawn together.
14-5","148"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","372","Sometimes you will see a simpler diagram called an energy level diagram which
is drawn when people are not interested in the k variable. Such a diagram—shown
in Fig. 14-5—just shows the possible energies for the electrons and holes.†
How can electron-hole pairs be created? There are several ways. For example,
photons of light (or x-rays) can be absorbed and create a pair if the photon
E−(electron)
E+ (hole)
E−
min
E+
min
Egap
STATE S
STATE S′
ELECTRON
CONDUCTION
BAND
HOLE
CONDUCTION
BAND
Fig. 14-5. Energy level diagram for electrons and holes.
† In many books this same energy diagram is interpreted in a diﬀerent way. The energy
scale refers only to electrons. Instead of thinking of the energy of the hole, they think of the
energy an electron would have if it ﬁlled the hole. This energy is lower than the free-electron
energy—in fact, just the amount lower that you see in Fig. 14-5. With this interpretation of
the energy scale, the gap energy is the minimum energy which must be given to an electron to
move it from its bound state to the conduction band.
14-6","189"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","373","energy is above the energy of the gap. The rate at which pairs are produced is
proportional to the light intensity. If two electrodes are plated on a wafer of the
crystal and a “bias” voltage is applied, the electrons and holes will be drawn to
the electrodes. The circuit current will be proportional to the intensity of the
light. This mechanism is responsible for the phenomenon of photoconductivity
and the operation of photoconductive cells.
Electron hole pairs can also be produced by high-energy particles. When a
fast-moving charged particle—for instance, a proton or a pion with an energy
of tens or hundreds of MeV—goes through a crystal, its electric ﬁeld will knock
electrons out of their bound states creating electron-hole pairs. Such events occur
hundreds of thousands of times per millimeter of track. After the passage of
the particle, the carriers can be collected and in doing so will give an electrical
pulse. This is the mechanism at play in the semiconductor counters recently
put to use for experiments in nuclear physics. Such counters do not require
semiconductors, they can also be made with crystalline insulators. In fact, the
ﬁrst of such counters was made using a diamond crystal which is an insulator
at room temperature. Very pure crystals are required if the holes and electrons
are to be able to move freely to the electrodes without being trapped. The
semiconductors silicon and germanium are used because they can be produced
with high purity in reasonable large sizes (centimeter dimensions).
So far we have been concerned with semiconductor crystals at temperatures
near absolute zero. At any ﬁnite temperature there is still another mechanism
by which electron-hole pairs can be created. The pair energy can be provided
from the thermal energy of the crystal. The thermal vibrations of the crystal
can transfer their energy to a pair—giving rise to “spontaneous” creation.
The probability per unit time that the energy as large as the gap energy Egap
will be concentrated at one atomic site is proportional to e−Egap/κT , where T
is the temperature and κ is Boltzmann’s constant (see Chapter 40, Vol. I).
Near absolute zero there is no appreciable probability, but as the tempera-
ture rises there is an increasing probability of producing such pairs. At any
ﬁnite temperature the production should continue forever at a constant rate
giving more and more negative and positive carriers. Of course that does not
happen because after a while the electrons and holes accidentally ﬁnd each other—
the electron drops into the hole and the excess energy is given to the lattice.
We say that the electron and hole “annihilate.” There is a certain probability
per second that a hole meets an electron and the two things annihilate each
other.
14-7","459"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","374","If the number of electrons per unit volume is Nn (n for negative carriers) and
the density of positive carriers is Np, the chance per unit time that an electron
and a hole will ﬁnd each other and annihilate is proportional to the product NnNp.
In equilibrium this rate must equal the rate that pairs are created. You see that
in equilibrium the product of Nn and Np should be given by some constant times
the Boltzmann factor:
NnNp = const e−Egap/κT .
(14.4)
When we say constant, we mean nearly constant. A more complete theory—which
includes more details about how holes and electrons “ﬁnd” each other—shows that
the “constant” is slightly dependent upon temperature, but the major dependence
on temperature is in the exponential.
Let’s consider, as an example, a pure material which is originally neutral.
At a ﬁnite temperature you would expect the number of positive and negative
carriers to be equal, Nn = Np. Then each of them should vary with temperature
as e−Egap/2κT . The variation of many of the properties of a semiconductor—the
conductivity for example—is mainly determined by the exponential factor because
all the other factors vary much more slowly with temperature. The gap energy
for germanium is about 0.72 eV and for silicon 1.1 eV.
At room temperature κT is about 1/40 of an electron volt. At these tempera-
tures there are enough holes and electrons to give a signiﬁcant conductivity, while
at, say, 30◦K—one-tenth of room temperature—the conductivity is imperceptible.
The gap energy of diamond is 6 or 7 eV and diamond is a good insulator at room
temperature.
14-2 Impure semiconductors
So far we have talked about two ways that extra electrons can be put into
an otherwise ideally perfect crystal lattice. One way was to inject the electron
from an outside source; the other way, was to knock a bound electron oﬀa
neutral atom creating simultaneously an electron and a hole. It is possible to
put electrons into the conduction band of a crystal in still another way. Suppose
we imagine a crystal of germanium in which one of the germanium atoms is
replaced by an arsenic atom. The germanium atoms have a valence of 4 and
the crystal structure is controlled by the four valence electrons. Arsenic, on the
other hand, has a valence of 5. It turns out that a single arsenic atom can sit
in the germanium lattice (because it has approximately the correct size), but in
14-8","411"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","375","doing so it must act as a valence 4 atom—using four of its valence electrons to
form the crystal bonds and having one electron left over. This extra electron
is very loosely attached—the binding energy is only about 1/100 of an electron
volt. At room temperature the electron easily picks up that much energy from
the thermal energy of the crystal, and then takes oﬀon its own—moving about
in the lattice as a free electron. An impurity atom such as the arsenic is called a
donor site because it can give up a negative carrier to the crystal. If a crystal of
germanium is grown from a melt to which a very small amount of arsenic has
been added, the arsenic donor sites will be distributed throughout the crystal
and the crystal will have a certain density of negative carriers built in.
You might think that these carriers would get swept away as soon as any
small electric ﬁeld was put across the crystal. This will not happen, however,
because the arsenic atoms in the body of the crystal each have a positive charge.
If the body of the crystal is to remain neutral, the average density of negative
carrier electrons must be equal to the density of donor sites. If you put two
electrodes on the edges of such a crystal and connect them to a battery, a current
will ﬂow; but as the carrier electrons are swept out at one end, new conduction
electrons must be introduced from the electrode on the other end so that the
average density of conduction electrons is left very nearly equal to the density of
donor sites.
Since the donor sites are positively charged, there will be some tendency
for them to capture some of the conduction electrons as they diﬀuse around
inside the crystal. A donor site can, therefore, act as a trap such as those we
discussed in the last section. But if the trapping energy is suﬃciently small—as
it is for arsenic—the number of carriers which are trapped at any one time is
a small fraction of the total. For a complete understanding of the behavior of
semiconductors one must take into account this trapping. For the rest of our
discussion, however, we will assume that the trapping energy is suﬃciently low
and the temperature is suﬃciently high, that all of the donor sites have given up
their electrons. This is, of course, just an approximation.
It is also possible to build into a germanium crystal some impurity atom
whose valence is 3, such as aluminum. The aluminum atom tries to act as a
valence 4 object by stealing an extra electron. It can steal an electron from
some nearby germanium atom and end up as a negatively charged atom with an
eﬀective valence of 4. Of course, when it steals the electron from a germanium
atom, it leaves a hole there; and this hole can wander around in the crystal as a
positive carrier. An impurity atom which can produce a hole in this way is called
14-9","510"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","376","an acceptor because it “accepts” an electron. If a germanium or a silicon crystal
is grown from a melt to which a small amount of aluminum impurity has been
added, the crystal will have built-in a certain density of holes which can act as
positive carriers.
When a donor or an acceptor impurity is added to a semiconductor, we say
that the material has been “doped.”
When a germanium crystal with some built-in donor impurities is at room
temperature, some conduction electrons are contributed by the thermally induced
electron-hole pair creation as well as by the donor sites. The electrons from both
sources are, naturally, equivalent, and it is the total number Nn which comes
into play in the statistical processes that lead to equilibrium. If the temperature
is not too low, the number of negative carriers contributed by the donor impurity
atoms is roughly equal to the number of impurity atoms present. In equilibrium
Eq. (14.4) must still be valid; at a given temperature the product NnNp is
determined. This means that if we add some donor impurity which increases Nn,
the number Np of positive carriers will have to decrease by such an amount that
NnNp is unchanged. If the impurity concentration is high enough, the number Nn
of negative carriers is determined by the number of donor sites and is nearly
independent of temperature—all of the variation in the exponential factor is
supplied by Np, even though it is much less than Nn. An otherwise pure crystal
with a small concentration of donor impurity will have a majority of negative
carriers; such a material is called an “n-type” semiconductor.
If an acceptor-type impurity is added to the crystal lattice, some of the new
holes will drift around and annihilate some of the free electrons produced by
thermal ﬂuctuation. This process will go on until Eq. (14.4) is satisﬁed. Under
equilibrium conditions the number of positive carriers will be increased and the
number of negative carriers will be decreased, leaving the product a constant. A
material with an excess of positive carriers is called a “p-type” semiconductor.
If we put two electrodes on a piece of semiconductor crystal and connect
them to a source of potential diﬀerence, there will be an electric ﬁeld inside the
crystal. The electric ﬁeld will cause the positive and the negative carriers to
move, and an electric current will ﬂow. Let’s consider ﬁrst what will happen in
an n-type material in which there is a large majority of negative carriers. For
such material we can disregard the holes; they will contribute very little to the
current because there are so few of them. In an ideal crystal the carriers would
move across without any impediment. In a real crystal at a ﬁnite temperature,
however,—especially in a crystal with some impurities—the electrons do not move
14-10","472"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","377","completely freely. They are continually making collisions which knock them out
of their original trajectories, that is, changing their momentum. These collisions
are just exactly the scatterings we talked about in the last chapter and occur
at any irregularity in the crystal lattice. In an n-type material the main causes
of scattering are the very donor sites that are producing the carriers. Since the
conduction electrons have a very slightly diﬀerent energy at the donor sites, the
probability waves are scattered from that point. Even in a perfectly pure crystal,
however, there are (at any ﬁnite temperature) irregularities in the lattice due to
thermal vibrations. From the classical point of view we can say that the atoms
aren’t lined up exactly on a regular lattice, but are, at any instant, slightly out
of place due to their thermal vibrations. The energy E0 associated with each
lattice point in the theory we described in Chapter 13 varies a little bit from
place to place so that the waves of probability amplitude are not transmitted
perfectly but are scattered in an irregular fashion. At very high temperatures or
for very pure materials this scattering may become important, but in most doped
materials used in practical devices the impurity atoms contribute most of the
scattering. We would like now to make an estimate of the electrical conductivity
of such a material.
When an electric ﬁeld is applied to an n-type semiconductor, each negative
carrier will be accelerated in this ﬁeld, picking up velocity until it is scattered
from one of the donor sites. This means that the carriers which are ordinarily
moving about in a random fashion with their thermal energies will pick up an
average drift velocity along the lines of the electric ﬁeld and give rise to a current
through the crystal. The drift velocity is in general rather small compared with
the typical thermal velocities so that we can estimate the current by assuming
that the average time that the carrier travels between scatterings is a constant.
Let’s say that the negative carrier has an eﬀective electric charge qn. In an
electric ﬁeld E, the force on the carrier will be qnE. In Section 43-3 of Volume I
we calculated the average drift velocity under such circumstances and found that
it is given by Fτ/m, where F is the force on the charge, τ is the mean free
time between collisions, and m is the mass. We should use the eﬀective mass we
calculated in the last chapter but since we want to make a rough calculation we
will suppose that this eﬀective mass is the same in all directions. Here we will
call it mn. With this approximation the average drift velocity will be
vdrift = qnEτn
mn
.
(14.5)
14-11","462"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","378","Knowing the drift velocity we can ﬁnd the current. Electric current density j is
just the number of carriers per unit volume, Nn, multiplied by the average drift
velocity, and by the charge on each carrier. The current density is therefore
j = Nnvdriftqn = Nnq2
nτn
mn
E.
(14.6)
We see that the current density is proportional to the electric ﬁeld; such a
semiconductor material obeys Ohm’s law. The coeﬃcient of proportionality
between j and E, the conductivity σ, is
σ = Nnq2
nτn
mn
.
(14.7)
For an n-type material the conductivity is relatively independent of temperature.
First, the number of majority carriers Nn is determined primarily by the density
of donors in the crystal (so long as the temperature is not so low that too many of
the carriers are trapped). Second, the mean time between collisions τn is mainly
controlled by the density of impurity atoms, which is, of course, independent of
the temperature.
We can apply all the same arguments to a p-type material, changing only the
values of the parameters which appear in Eq. (14.7). If there are comparable
numbers of both negative and positive carriers present at the same time, we must
add the contributions from each kind of carrier. The total conductivity will be
given by
σ = Nnq2
nτn
mn
+ Npq2
pτp
mp
.
(14.8)
For very pure materials, Np and Nn will be nearly equal. They will be smaller
than in a doped material, so the conductivity will be less. Also they will vary
rapidly with temperature (like e−Egap/2κT , as we have seen), so the conductivity
may change extremely fast with temperature.
14-3 The Hall eﬀect
It is certainly a peculiar thing that in a substance where the only relatively free
objects are electrons, there should be an electrical current carried by holes that
behave like positive particles. We would like, therefore, to describe an experiment
that shows in a rather clear way that the sign of the carrier of electric current
14-12","335"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","379","+
−
I
j
B
+(−)
−(+)
Fig. 14-6. The Hall eﬀect comes from the magnetic forces on the
carriers.
is quite deﬁnitely positive. Suppose we have a block made of semiconductor
material—it could also be a metal—and we put an electric ﬁeld on it so as to draw
a current in some direction, say the horizontal direction as drawn in Fig. 14-6.
Now suppose we put a magnetic ﬁeld on the block pointing at a right angle to
the current, say into the plane of the ﬁgure. The moving carriers will feel a
magnetic force q(v × B). And since the average drift velocity is either right or
left—depending on the sign of the charge on the carrier—the average magnetic
force on the carriers will be either up or down. No, that is not right! For the
directions we have assumed for the current and the magnetic ﬁeld the magnetic
force on the moving charges will always be up. Positive charges moving in the
direction of j (to the right) will feel an upward force. If the current is carried by
negative charges, they will be moving left (for the same sign of the conduction
current) and they will also feel an upward force. Under steady conditions, however,
there is no upward motion of the carriers because the current can ﬂow only from
left to right. What happens is that a few of the charges initially ﬂow upward,
producing a surface charge density along the upper surface of semiconductor—
leaving an equal and opposite surface charge density along the bottom surface of
the crystal. The charges pile up on the top and bottom surfaces until the electric
forces they produce on the moving charges just exactly cancel the magnetic force
(on the average) so that the steady current ﬂows horizontally. The charges on
the top and bottom surfaces will produce a potential diﬀerence vertically across
the crystal which can be measured with a high-resistance voltmeter, as shown in
Fig. 14-7. The sign of the potential diﬀerence registered by the voltmeter will
depend on the sign of the carrier charges responsible for the current.
When such experiments were ﬁrst done it was expected that the sign of the
potential diﬀerence would be negative as one would expect for negative conduction
electrons. People were, therefore, quite surprised to ﬁnd that for some materials
14-13","393"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","380","I
ELECTRONIC
VOLTMETER
0
−
+
Fig. 14-7. Measuring the Hall eﬀect.
the sign of the potential diﬀerence was in the opposite direction. It appeared that
the current carrier was a particle with a positive charge. From our discussion of
doped semiconductors it is understandable that an n-type semiconductor should
produce the sign of potential diﬀerence appropriate to negative carriers, and that
a p-type semiconductor should give an opposite potential diﬀerence, since the
current is carried by the positively charged holes.
The original discovery of the anomalous sign of the potential diﬀerence in
the Hall eﬀect was made in a metal rather than a semiconductor. It had been
assumed that in metals the conduction was always by electron; however, it was
found out that for beryllium the potential diﬀerence had the wrong sign. It
is now understood that in metals as well as in semiconductors it is possible,
in certain circumstances, that the “objects” responsible for the conduction are
holes. Although it is ultimately the electrons in the crystal which do the moving,
nevertheless, the relationship of the momentum and the energy, and the response
to external ﬁelds is exactly what one would expect for an electric current carried
by positive particles.
Let’s see if we can make a quantitative estimate of the magnitude of the
voltage diﬀerence expected from the Hall eﬀect. If the voltmeter in Fig. 14-7 draws
a negligible current, then the charges inside the semiconductor must be moving
from left to right and the vertical magnetic force must be precisely cancelled by
a vertical electric ﬁeld which we will call Etr (the “tr” is for “transverse”). If this
electric ﬁeld is to cancel the magnetic forces, we must have
Etr = −vdrift × B.
(14.9)
14-14","290"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","381","Using the relation between the drift velocity and the electric current density
given in Eq. (14.6), we get
Etr = −1
qN jB.
The potential diﬀerence between the top and the bottom of the crystal is, of
course, this electric ﬁeld strength multiplied by the height of the crystal. The
electric ﬁeld strength Etr in the crystal is proportional to the current density and
to the magnetic ﬁeld strength. The constant of proportionality 1/qN is called the
Hall coeﬃcient and is usually represented by the symbol RH. The Hall coeﬃcient
depends just on the density of carriers—provided that carriers of one sign are
in a large majority. Measurement of the Hall eﬀect is, therefore, one convenient
way of determining experimentally the density of carriers in a semiconductor.
14-4 Semiconductor junctions
We would like to discuss now what happens if we take two pieces of germanium
or silicon with diﬀerent internal characteristics—say diﬀerent kinds or amounts
of doping—and put them together to make a “junction.” Let’s start out with
what is called a p-n junction in which we have p-type germanium on one side
of the boundary and n-type germanium on the other side of the boundary—as
sketched in Fig. 14-8. Actually, it is not practical to put together two separate
pieces of crystal and have them in uniform contact on an atomic scale. Instead,
junctions are made out of a single crystal which has been modiﬁed in the two
separate regions. One way is to add some suitable doping impurity to the “melt”
after only half of the crystal has grown. Another way is to paint a little of the
impurity element on the surface and then heat the crystal causing some impurity
atoms to diﬀuse into the body of the crystal. Junctions made in these ways do
not have a sharp boundary, although the boundaries can be made as thin as
10−4 centimeters or so. For our discussions we will imagine an ideal situation
p-type material
n-type material
Fig. 14-8. A p-n junction.
14-15","335"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","382","(a)
p-type
n-type
(b)
x
V
(c)
x
N
Np
Nn
Fig. 14-9. The electric potential and the carrier densities in an unbiased
semiconductor junction.
in which these two regions of the crystal with diﬀerent properties meeting at a
sharp boundary.
On the n-type side of the p-n junction there are free electrons which can move
about, as well as the ﬁxed donor sites which balance the overall electric charge.
On the p-type side there are free holes moving about and an equal number of
negative acceptor sites keeping the charge balanced. Actually, that describes the
situation before we put the two materials in contact. Once they are connected
together the situation will change near the boundary. When the electrons in
the n-type material arrive at the boundary they will not be reﬂected back as
they would at a free surface, but are able to go right on into the p-type material.
Some of the electrons of the n-type material will, therefore, tend to diﬀuse over
into the p-type material where there are fewer electrons. This cannot go on
forever because as we lose electrons from the n-side the net positive charge there
increases until ﬁnally an electric voltage is built up which retards the diﬀusion
14-16","207"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","383","of electrons into the p-side. In a similar way, the positive carriers of the p-type
material can diﬀuse across the junction into the n-type material. When they do
this they leave behind an excess of negative charge. Under equilibrium conditions
the net diﬀusion current must be zero. This is brought about by the electric
ﬁelds, which are established in such a way as to draw the positive carriers back
toward the p-type material.
The two diﬀusion processes we have been describing go on simultaneously
and, you will notice, both act in the direction which will charge up the n-type
material in a positive sense and the p-type material in a negative sense. Because
of the ﬁnite conductivity of the semiconductor material, the change in potential
from the p-side to the n-side will occur in a relatively narrow region near the
boundary; the main body of each block of material will have a uniform potential.
Let’s imagine an x-axis in a direction perpendicular to the boundary surface.
Then the electric potential will vary with x, as shown in Fig. 14-9(b). We have
also shown in part (c) of the ﬁgure the expected variation of the density Nn of
n-carriers and the density Np of p-carriers. Far away from the junction the carrier
densities Np and Nn should be just the equilibrium density we would expect for
individual blocks of materials at the same temperature. (We have drawn the
ﬁgure for a junction in which the p-type material is more heavily doped than the
n-type material.) Because of the potential gradient at the junction, the positive
carriers have to climb up a potential hill to get to the n-type side. This means
that under equilibrium conditions there can be fewer positive carriers in the
n-type material than there are in the p-type material. Remembering the laws of
statistical mechanics, we expect that the ratio of p-type carriers on the two sides
to be given by the following equation:
Np(n-side)
Np(p-side) = e−qpV/κT .
(14.10)
The product qpV in the numerator of the exponential is just the energy required
to carry a charge of qp through a potential diﬀerence V .
We have a precisely similar equation for the densities of the n-type carriers:
Nn(n-side)
Nn(p-side) = e−qnV/κT .
(14.11)
If we know the equilibrium densities in each of the two materials, we can use
either of the two equations above to determine the potential diﬀerence across the
junction.
14-17","407"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","384","Notice that if Eqs. (14.10) and (14.11) are to give the same value for the
potential diﬀerence V , the product NpNn must be the same for the p-side as
for the n-side. (Remember that qn = −qp.) We have seen earlier, however, that
this product depends only on the temperature and the gap energy of the crystal.
Provided both sides of the crystal are at the same temperature, the two equations
are consistent with the same value of the potential diﬀerence.
Since there is a potential diﬀerence from one side of the junction to the
other, it looks something like a battery. Perhaps if we connect a wire from the
n-type side to the p-type side we will get an electrical current. That would be
nice because then the current would ﬂow forever without using up any material
and we would have an inﬁnite source of energy in violation of the second law
of thermodynamics! There is, however, no current if you connect a wire from
the p-side to the n-side. And the reason is easy to see. Suppose we imagine
ﬁrst a wire made out of a piece of undoped material. When we connect this
wire to the n-type side, we have a junction. There will be a potential diﬀerence
across this junction. Let’s say that it is just one-half the potential diﬀerence
from the p-type material to the n-type material. When we connect our undoped
wire to the p-type side of the junction, there is also a potential diﬀerence at
this junction—again, one-half the potential drop across the p-n junction. At all
the junctions the potential diﬀerences adjust themselves so that there is no net
current ﬂow in the circuit. Whatever kind of wire you use to connect together
the two sides of the n-p junction, you are producing two new junctions, and so
long as all the junctions are at the same temperature, the potential jumps at the
junctions all compensate each other and no current will ﬂow in the circuit. It
does turn out, however—if you work out the details—that if some of the junctions
are at a diﬀerent temperature than the other junctions, currents will ﬂow. Some
of the junctions will be heated and others will be cooled by this current and
thermal energy will be converted into electrical energy. This eﬀect is responsible
for the operation of thermocouples which are used for measuring temperatures,
and of thermoelectric generators. The same eﬀect is also used to make small
refrigerators.
If we cannot measure the potential diﬀerence between the two sides of an
n-p junction, how can we really be sure that the potential gradient shown in
Fig. 14-9 really exists? One way is to shine light on the junction. When the
light photons are absorbed they can produce an electron-hole pair. In the strong
electric ﬁeld that exists at the junction (equal to the slope of the potential curve
of Fig. 14-9) the hole will be driven into the p-type region and the electron will be
14-18","503"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","385","driven into the n-type region. If the two sides of the junction are now connected
to an external circuit, these extra charges will provide a current. The energy of
the light will be converted into electrical energy in the junction. The solar cells
which generate electrical power for the operation of some of our satellites operate
on this principle.
In our discussion of the operation of a semiconductor junction we have
been assuming that the holes and the electrons act more-or-less independently—
except that they somehow get into proper statistical equilibrium. When we
were describing the current produced by light shining on the junction, we were
assuming that an electron or a hole produced in the junction region would
get into the main body of the crystal before being annihilated by a carrier of
the opposite polarity.
In the immediate vicinity of the junction, where the
density of carriers of both signs is approximately equal, the eﬀect of electron-hole
annihilation (or as it is often called, “recombination”) is an important eﬀect,
and in a detailed analysis of a semiconductor junction must be properly taken
into account. We have been assuming that a hole or an electron produced in a
junction region has a good chance of getting into the main body of the crystal
before recombining. The typical time for an electron or a hole to ﬁnd an opposite
partner and annihilate it is for typical semiconductor materials in the range
between 10−3 and 10−7 seconds. This time is, incidentally, much longer than the
mean free time τ between collisions with scattering sites in the crystal which we
used in the analysis of conductivity. In a typical n-p junction, the time for an
electron or hole formed in the junction region to be swept away into the body of
the crystal is generally much shorter than the recombination time. Most of the
pairs will, therefore, contribute to an external current.
14-5 Rectiﬁcation at a semiconductor junction
We would like to show next how it is that a p-n junction can act like a rectiﬁer.
If we put a voltage across the junction, a large current will ﬂow if the polarity is
in one direction, but a very small current will ﬂow if the same voltage is applied
in the opposite direction. If an alternating voltage is applied across the junction,
a net current will ﬂow in one direction—the current is “rectiﬁed.” Let’s look
again at what is going on in the equilibrium condition described by the graphs
of Fig. 14-9. In the p-type material there is a large concentration Np of positive
carriers. These carriers are diﬀusing around and a certain number of them each
second approach the junction. This current of positive carriers which approaches
14-19","457"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","386","the junction is proportional to Np. Most of them, however, are turned back by
the high potential hill at the junction and only the fraction e−qV/κT gets through.
There is also a current of positive carriers approaching the junction from the
other side. This current is also proportional to the density of positive carriers in
the n-type region, but the carrier density here is much smaller than the density
on the p-type side. When the positive carriers approach the junction from the
n-type side, they ﬁnd a hill with a negative slope and immediately slide downhill
to the p-type side of the junction. Let’s call this current I0. Under equilibrium
the currents from the two directions are equal. We expect then the following
relation:
I0 ∝Np(n-side) = Np(p-side)e−qV/κT .
(14.12)
You will notice that this equation is really just the same as Eq. (14.10). We have
just derived it in a diﬀerent way.
Suppose, however, that we lower the voltage on the n-side of the junction by
an amount ∆V —which we can do by applying an external potential diﬀerence
to the junction. Now the diﬀerence in potential across the potential hill is no
longer V but V −∆V . The current of positive carriers from the p-side to the
n-side will now have this potential diﬀerence in its exponential factor. Calling
this current I1, we have
I1 ∝Np(p-side)e−q(V −∆V )/κT .
This current is larger than I0 by just the factor eq∆V/κT . So we have the following
relation between I1 and I0:
I1 = I0e+q∆V/κT .
(14.13)
The current from the p-side increases exponentially with the externally applied
voltage ∆V . The current of positive carriers from the n-side, however, remains
constant so long as ∆V is not too large. When they approach the barrier, these
carriers will still ﬁnd a downhill potential and will all fall down to the p-side. (If
∆V is larger than the natural potential diﬀerence V , the situation would change,
but we will not consider what happens at such high voltages.) The net current I
of positive carriers which ﬂows across the junction is then the diﬀerence between
the currents from the two sides:
I1 = I0(e+q∆V/κT −1).
(14.14)
14-20","368"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","387","The net current I of holes ﬂows into the n-type region. There the holes diﬀuse
into the body of the n-region, where they are eventually annihilated by the
majority n-type carriers—the electrons. The electrons which are lost in this
annihilation will be made up by a current of electrons from the external terminal
of the n-type material.
When ∆V is zero, the net current in Eq. (14.14) is zero. For positive ∆V
the current increases rapidly with the applied voltage. For negative ∆V the
current reverses in sign, but the exponential term soon becomes negligible and
the negative current never exceeds I0—which under our assumptions is rather
small. This back current I0 is limited by the small density of the minority p-type
carriers on the n-side of the junction.
If you go through exactly the same analysis for the current of negative carriers
which ﬂows across the junction, ﬁrst with no potential diﬀerence and then with
a small externally applied potential diﬀerence ∆V , you get again an equation
just like (14.14) for the net electron current. Since the total current is the sum
of the currents contributed by the two carriers, Eq. (14.14) still applies for the
total current provided we identify I0 as the maximum current which can ﬂow for
a reversed voltage.
The voltage-current characteristic of Eq. (14.14) is shown in Fig. 14-10. It
shows the typical behavior of solid state diodes—such as those used in modern
computers. We should remark that Eq. (14.14) is true only for small voltages. For
voltages comparable to or larger than the natural internal voltage diﬀerence V ,
other eﬀects come into play and the current no longer obeys the simple equation.
You may remember, incidentally, that we got exactly the same equation we
have found here in Eq. (14.14) when we discussed the “mechanical rectiﬁer”—the
ratchet and pawl—in Chapter 46 of Volume I. We get the same equations in the
two situations because the basic physical processes are quite similar.
14-6 The transistor
Perhaps the most important application of semiconductors is in the transistor.
The transistor consists of two semiconductor junctions very close together. Its
operation is based in part on the same principles that we just described for the
semiconductor diode—the rectifying junction. Suppose we make a little bar of
germanium with three distinct regions, a p-type region, an n-type region, and
another p-type region, as shown in Fig. 14-11(a). This combination is called a
p-n-p transistor. Each of the two junctions in the transistor will behave much
14-21","420"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","388","I/I0
q∆V/κT
−4
−3
−2
−1
1
2
−2
−1
0
1
2
3
4
5
6
Fig. 14-10. The current through a junction as a function of the
voltage across it.
(a)
p
n
p
(b)
V
Fig. 14-11. The potential distribution in a transistor with no applied
voltages.
14-22","51"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","389","in the way we have described in the last section. In particular, there will be
a potential gradient at each junction having a certain potential drop from the
n-type region to each p-type region. If the two p-type regions have the same
internal properties, the variation in potential as we go across the crystal will be
as shown in the graph of Fig. 14-11(b).
Now let’s imagine that we connect each of the three regions to external
voltage sources as shown in part (a) of Fig. 14-12. We will refer all voltages to the
terminal connected to the left-hand p-region so it will be, by deﬁnition, at zero
potential. We will call this terminal the emitter. The n-type region is called the
base and it is connected to a slightly negative potential. The right-hand p-type
region is called the collector, and is connected to a somewhat larger negative
potential. Under these circumstances the variation of potential across the crystal
will be as shown in the graph of Fig. 14-12(b).
(a)
p
n
p
e
b
c
Ie
Ib
Ic
Ve = 0
Vb < 0
Vc ≪Vb
(b)
V
Vb
Vc
Fig. 14-12. The potential distribution in an operating transistor.
Let’s ﬁrst see what happens to the positive carriers, since it is primarily their
behavior which controls the operation of the p-n-p transistor. Since the emitter is
at a relatively more positive potential than the base, a current of positive carriers
will ﬂow from the emitter region into the base region. A relatively large current
14-23","256"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","390","ﬂows, since we have a junction operating with a “forward voltage”—corresponding
to the right-hand half of the graph in Fig. 14-10. With these conditions, positive
carriers or holes are being “emitted” from the p-type region into the n-type
region. You might think that this current would ﬂow out of the n-type region
through the base terminal b. Now, however, comes the secret of the transistor.
The n-type region is made very thin—typically 10−3 cm or less, much narrower
than its transverse dimensions. This means that as the holes enter the n-type
region they have a very good chance of diﬀusing across to the other junction
before they are annihilated by the electrons in the n-type region. When they
get to the right-hand boundary of the n-type region they ﬁnd a steep downward
potential hill and immediately fall into the right-hand p-type region. This side of
the crystal is called the collector because it “collects” the holes after they have
diﬀused across the n-type region. In a typical transistor, all but a fraction of
a percent of the hole current which leaves the emitter and enters the base is
collected in the collector region, and only the small remainder contributes to the
net base current. The sum of the base and collector currents is, of course, equal
to the emitter current.
Now imagine what happens if we vary slightly the potential Vb on the base
terminal. Since we are on a relatively steep part of the curve of Fig. 14-10, a
small variation of the potential Vb will cause a rather large change in the emitter
current Ie. Since the collector voltage Vc is much more negative than the base
voltage, these slight variations in potential will not eﬀect appreciably the steep
potential hill between the base and the collector. Most of the positive carriers
emitted into the n-region will still be caught by the collector. Thus as we vary
the potential of the base electrode, there will be a corresponding variation in
the collector current Ic. The essential point, however, is that the base current Ib
always remains a small fraction of the collector current. The transistor is an
ampliﬁer; a small current Ib introduced into the base electrode gives a large
current—100 or so times higher—at the collector electrode.
What about the electrons—the negative carriers that we have been neglecting
so far? First, note that we do not expect any signiﬁcant electron current to
ﬂow between the base and the collector. With a large negative voltage on the
collector, the electrons in the base would have to climb a very high potential
energy hill and the probability of doing that is very small. There is a very small
current of electrons to the collector.
On the other hand, the electrons in the base can go into the emitter region.
In fact, you might expect the electron current in this direction to be comparable
14-24","485"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","391","to the hole current from the emitter into the base. Such an electron current isn’t
useful, and, on the contrary, is bad because it increases the total base current
required for a given current of holes to the collector. The transistor is, therefore,
designed to minimize the electron current to the emitter. The electron current is
proportional to Nn(base), the density of negative carriers in the base material
while the hole current from the emitter depends on Np(emitter), the density of
positive carriers in the emitter region. By using relatively little doping in the
n-type material Nn(base) can be made much smaller than Np(emitter). (The
very thin base region also helps a great deal because the sweeping out of the holes
in this region by the collector increases signiﬁcantly the average hole current
from the emitter into the base, while leaving the electron current unchanged.)
The net result is that the electron current across the emitter-base junction can
be made much less than the hole current, so that the electrons do not play any
signiﬁcant role in operation of the p-n-p transistor. The currents are dominated
by motion of the holes, and the transistor performs as an ampliﬁer as we have
described above.
It is also possible to make a transistor by interchanging the p-type and n-type
materials in Fig. 14-11. Then we have what is called an n-p-n transistor. In
the n-p-n transistor the main currents are carried by the electrons which ﬂow
from the emitter into the base and from there to the collector. Obviously, all the
arguments we have made for the p-n-p transistor also apply to the n-p-n transistor
if the potentials of the electrodes are chosen with the opposite signs.
14-25","286"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","392","15
The Independent Particle Approximation
15-1 Spin waves
In Chapter 13 we worked out the theory for the propagation of an electron
or of some other “particle,” such as an atomic excitation, through a crystal
lattice. In the last chapter we applied the theory to semiconductors. But when
we talked about situations in which there are many electrons we disregarded any
interactions between them. To do this is of course only an approximation. In this
chapter we will discuss further the idea that you can disregard the interaction
between the electrons. We will also use the opportunity to show you some more
applications of the theory of the propagation of particles. Since we will generally
continue to disregard the interactions between particles, there is very little really
new in this chapter except for the new applications. The ﬁrst example to be
considered is, however, one in which it is possible to write down quite exactly the
correct equations when there is more than one “particle” present. From them
we will be able to see how the approximation of disregarding the interactions is
made. We will not, though, analyze the problem very carefully.
As our ﬁrst example we will consider a “spin wave” in a ferromagnetic crystal.
We have discussed the theory of ferromagnetism in Chapter 36 of Volume II.
At zero temperature all the electron spins that contribute to the magnetism in
the body of a ferromagnetic crystal are parallel. There is an interaction energy
between the spins, which is lowest when all the spins are down. At any nonzero
temperature, however, there is some chance that some of the spins are turned
over. We calculated the probability in an approximate manner in Chapter 36.
This time we will describe the quantum mechanical theory—so you will see what
you would have to do if you wanted to solve the problem more exactly. (We will
still make some idealizations by assuming that the electrons are localized at the
atoms and that the spins interact only with neighboring spins.)
15-1","338"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","393","We consider a model in which the electrons at each atom are all paired except
one, so that all of the magnetic eﬀects come from one spin- 1
2 electron per atom.
Further, we imagine that these electrons are localized at the atomic sites in the
lattice. The model corresponds roughly to metallic nickel.
We also assume that there is an interaction between any two adjacent spinning
electrons which gives a term in the energy of the system
E = −
X
i,j
Kσi · σj,
(15.1)
where σ’s represent the spins and the summation is over all adjacent pairs of
electrons. We have already discussed this kind of interaction energy when we
considered the hyperﬁne splitting of hydrogen due to the interaction of the
magnetic moments of the electron and proton in a hydrogen atom. We expressed
it then as Aσe · σp. Now, for a given pair, say the electrons at atom 4 and at
atom 5, the Hamiltonian would be −Kσ4 · σ5. We have a term for each such
pair, and the Hamiltonian is (as you would expect for classical energies) the
sum of these terms for each interacting pair. The energy is written with the
factor −K so that a positive K will correspond to ferromagnetism—that is, the
lowest energy results when adjacent spins are parallel. In a real crystal, there
may be other terms which are the interactions of next nearest neighbors, and so
on, but we don’t need to consider such complications at this stage.
With the Hamiltonian of Eq. (15.1) we have a complete description of the
ferromagnet—within our approximation—and the properties of the magnetization
should come out.
We should also be able to calculate the thermodynamic
properties due to the magnetization. If we can ﬁnd all the energy levels, the
properties of the crystal at a temperature T can be found from the principle
that the probability that a system will be found in a given state of energy E is
proportional to e−E/κT . This problem has never been completely solved.
We will show some of the problems by taking a simple example in which all
the atoms are in a line—a one-dimensional lattice. You can easily extend the
ideas to three dimensions. At each atomic location there is an electron which
has two possible states, either spin up or spin down, and the whole system is
described by telling how all of the spins are arranged. We take the Hamiltonian
of the system to be the operator of the interaction energy. Interpreting the spin
vectors of Eq. (15.1) as the sigma-operators—or the sigma-matrices—we write
15-2","436"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","394","for the linear lattice
ˆH =
X
n
−A
2 ˆσn · ˆσn+1.
(15.2)
In this equation we have written the constant as A/2 for convenience (so that
some of the later equations will be exactly the same as the ones in Chapter 13).
Now what is the lowest state of this system? The state of lowest energy is
the one in which all the spins are parallel—let’s say, all up.† We can write this
state as | · · · + + + + · · ·⟩, or | gnd⟩for the “ground,” or lowest, state. It’s easy
to ﬁgure out the energy for this state. One way is to write out all the vector
sigmas in terms of ˆσx, ˆσy, and ˆσz, and work through carefully what each term
of the Hamiltonian does to the ground state, and then add the results. We can,
however, also use a good short cut. We saw in Section 12-2, that ˆσi · ˆσj could
be written in terms of the Pauli spin exchange operator like this:
ˆσi · ˆσj = (2 ˆP spin ex
ij
−1),
(15.3)
where the operator ˆP spin ex
ij
interchanges the spins of the ith and jth electrons.
With this substitution the Hamiltonian becomes
ˆH = −A
X
n
( ˆP spin ex
n,n+1 −1
2).
(15.4)
It is now easy to work out what happens to diﬀerent states. For instance if
i and j are both up, then exchanging the spins leaves everything unchanged,
so ˆPij acting on the state just gives the same state back, and is equivalent to
multiplying by +1. The expression ( ˆPij −1
2) is just equal to one-half. (From now
on we will leave oﬀthe descriptive superscript on the ˆP.)
For the ground state all spins are up; so if you exchange a particular pair of
spins, you get back the original state. The ground state is a stationary state. If
you operate on it with the Hamiltonian you get the same state again multiplied
by a sum of terms, −(A/2) for each pair of spins. That is, the energy of the
system in the ground state is −A/2 per atom.
Next we would like to look at the energies of some of the excited states. It
will be convenient to measure the energies with respect to the ground state—that
† The ground state here is really “degenerate”; there are other states with the same energy—
for example, all spins down, or all in any other direction. The slightest external ﬁeld in the
z-direction will give a diﬀerent energy to all these states, and the one we have chosen will be
the true ground state.
15-3","446"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","395","is, to choose the ground state as our zero of energy. We can do that by adding
the energy A/2 to each term in the Hamiltonian. That just changes the “ 1
2” in
Eq. (15.4) to “1.” Our new Hamiltonian is
ˆH = −A
X
n
( ˆPn,n+1 −1).
(15.5)
With this Hamiltonian the energy of the lowest state is zero; the spin exchange
operator is equivalent to multiplying by unity (for the ground state) which is
cancelled by the “1” in each term.
For describing states other than the ground state we will need a suitable
set of base states. One convenient approach is to group the states according to
whether one electron has spin down, or two electrons have spin down, and so on.
There are, of course, many states with one spin down. The down spin could be at
atom “4,” or at atom “5,” or at atom “6,” . . . We can, in fact, choose just such
states for our base states. We could write them this way: | 4⟩, | 5⟩, | 6⟩, . . . It will,
however, be more convenient later if we label the “odd atom”—the one with the
down-spinning electron—by its coordinate x. That is, we’ll deﬁne the state | x5⟩
to be one with all the electrons spinning up except for the one on the atom at x5,
which has a down-spinning electron (see Fig. 15-1). In general, | xn⟩is the state
with one down spin that is located at the coordinate xn of the nth atom.
−3 −2 −1
0
1
2
3
4
5
6
7
b
x5
Fig. 15-1. The base state | x5⟩of a linear array of spins. All the spins
are up except the one at x5, which is down.
What is the action of the Hamiltonian (15.5) on the state | x5⟩? One term of
the Hamiltonian is say −A( ˆP7,8 −1). The operator ˆP7,8 exchanges the two spins
of the adjacent atoms 7, 8. But in the state | x5⟩these are both up, and nothing
happens; ˆP7,8 is equivalent to multiplying by 1:
ˆP7,8 | x5⟩= | x5⟩.
15-4","358"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","396","It follows that
( ˆP7,8 −1) | x5⟩= 0.
Thus all the terms of the Hamiltonian give zero—except those involving atom 5,
of course. On the state | x5⟩, the operation ˆP4,5 exchanges the spin of atom 4
(up) and atom 5 (down). The result is the state with all spins up except the
atom at 4. That is
ˆP4,5 | x5⟩= | x4⟩.
In the same way
ˆP5,6 | x5⟩= | x6⟩.
Hence, the only terms of the Hamiltonian which survive are −A( ˆP4,5 −1)
and −A( ˆP5,6 −1). Acting on | x5⟩they produce −A | x4⟩+ A | x5⟩and −A | x6⟩+
A | x5⟩, respectively. The result is
ˆH | x5⟩= −A
X
n
( ˆPn,n+1 −1) | x5⟩= −A{| x6⟩+ | x4⟩−2 | x5⟩}.
(15.6)
When the Hamiltonian acts on state | x5⟩it gives rise to some amplitude to
be in states | x4⟩and | x6⟩. That just means that there is a certain amplitude to
have the down spin jump over to the next atom. So because of the interaction
between the spins, if we begin with one spin down, then there is some probability
that at a later time another one will be down instead. Operating on the general
state | xn⟩, the Hamiltonian gives
ˆH | xn⟩= −A{| xn+1⟩+ | xn−1⟩−2 | xn⟩}.
(15.7)
Notice particularly that if we take a complete set of states with only one spin
down, they will only be mixed among themselves. The Hamiltonian will never
mix these states with others that have more spins down. So long as you only
exchange spins you never change the total number of down spins.
It will be convenient to use the matrix notation for the Hamiltonian, say
Hn,m ≡⟨xn | ˆH | xm⟩; Eq. (15.7) is equivalent to
Hn,n = 2A;
Hn,n+1 = Hn,n−1 = −A;
Hn,m = 0,
for
|n −m| > 1.
(15.8)
15-5","316"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","397","Now what are the energy levels for states with one spin down? As usual we
let Cn be the amplitude that some state | ψ⟩is in the state | xn⟩. If | ψ⟩is to be a
deﬁnite energy state, all the Cn’s must vary with time in the same way, namely,
Cn = ane−iEt/ℏ.
(15.9)
We can put this trial solution into our usual Hamiltonian equation
iℏdCn
dt
=
X
m
HnmCm,
(15.10)
using Eq. (15.8) for the matrix elements. Of course we get an inﬁnite number of
equations, but they can all be written as
Ean = 2Aan −Aan−1 −Aan+1.
(15.11)
We have again exactly the same problem we worked out in Chapter 13, except that
where we had E0 we now have 2A. The solutions correspond to amplitudes Cn
(the down-spin amplitude) which propagate along the lattice with a propagation
constant k and an energy
E = 2A(1 −cos kb),
(15.12)
where b is the lattice constant.
The deﬁnite energy solutions correspond to “waves” of down spin—called
“spin waves.” And for each wavelength there is a corresponding energy. For large
wavelengths (small k) this energy varies as
E = Ab2k2.
(15.13)
Just as before, we can consider a localized wave packet (containing, however,
only long wavelengths) which corresponds to a spin-down electron in one part of
the lattice. This down spin will behave like a “particle.” Because its energy is
related to k by (15.13) the “particle” will have an eﬀective mass:
meﬀ=
ℏ2
2Ab2 .
(15.14)
These “particles” are sometimes called “magnons.”
15-6","256"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","398","15-2 Two spin waves
Now we would like to discuss what happens if there are two down spins. Again
we pick a set of base states. We’ll choose states in which there are down spins at
two atomic locations, such as the state shown in Fig. 15-2. We can label such a
state by the x-coordinates of the two sites with down spins. The one shown can
be called | x2, x5⟩. In general the base states are | xn, xm⟩—a doubly inﬁnite set!
In this system of description, the state | x4, x9⟩and the state | x9, x4⟩are exactly
the same state, because each simply says that there is a down spin at 4 and one
at 9; there is no meaning to the order. Furthermore, the state | x4, x4⟩has no
meaning, there isn’t such a thing. We can describe any state | ψ⟩by giving the
amplitudes to be in each of the base states. Thus Cm,n = ⟨xm, xn | ψ⟩now means
the amplitude for a system in the state | ψ⟩to be in a state in which both the
mth and nth atoms have a down spin. The complications which now arise are
not complications of ideas—they are merely complexities in bookkeeping. (One
of the complexities of quantum mechanics is just the bookkeeping. With more
and more down spins, the notation becomes more and more elaborate with lots
of indices and the equations always look very horrifying; but the ideas are not
necessarily more complicated than in the simplest case.)
−3 −2 −1
0
1
2
3
4
5
6
7
Fig. 15-2. A state with two down spins.
The equations of motion of the spin system are the diﬀerential equations for
the Cn,m. They are
iℏdCn,m
dt
=
X
i,j
(Hnm,ij)Cij.
(15.15)
Suppose we want to ﬁnd the stationary states. As usual, the derivatives with
respect to time become E times the amplitudes and the Cm,n can be replaced by
the coeﬃcients am,n. Next we have to work out carefully the eﬀect of H on a state
with spins m and n down. It is not hard to ﬁgure out. Suppose for a moment that
m and n are far enough apart so that we don’t have to worry about the obvious
trouble. The operation of exchange at the location xn will move the down spin
either to the (n+1) or (n−1) atom, and so there’s an amplitude that the present
state has come from the state | xm, xn+1⟩and also an amplitude that it has come
15-7","423"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","399","from the state | xm, xn−1⟩. Or it may have been the other spin that moved; so
there’s a certain amplitude that Cm,n is fed from Cm+1,n or from Cm−1,n. These
eﬀects should all be equal. The ﬁnal result for the Hamiltonian equation on Cm,n
is
Eam,n = −A(am+1,n + am−1,n + am,n+1 + am,n−1) + 4Aam,n.
(15.16)
This equation is correct except in two situations.
If m = n there is no
equation at all, and if m = n ± 1, then two of the terms in Eq. (15.16) should
be missing. We are going to disregard these exceptions. We simply ignore the
fact that some few of these equations are slightly altered. After all, the crystal is
supposed to be inﬁnite, and we have an inﬁnite number of terms; neglecting a
few might not matter much. So for a ﬁrst rough approximation let’s forget about
the altered equations. In other words, we assume that Eq. (15.16) is true for all
m and n, even for m and n next to each other. This is the essential part of our
approximation.
Then the solution is not hard to ﬁnd. We get immediately
Cm,n = am,ne−iEt/ℏ,
(15.17)
with
am,n = (const.) eik1xmeik2xn,
(15.18)
where
E = 4A −2A cos k1b −2A cos k2b.
(15.19)
Think for a moment what would happen if we had two independent, single
spin waves (as in the previous section) corresponding to k = k1 and k = k2; they
would have energies, from Eq. (15.12), of
ϵ1 = (2A −2A cos k1b)
and
ϵ2 = (2A −2A cos k2b).
Notice that the energy E in Eq. (15.19) is just their sum,
E = ϵ(k1) + ϵ(k2).
(15.20)
In other words we can think of our solution in this way. There are two particles—
that is, two spin waves. One of them has a momentum described by k1, the other
15-8","316"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","400","by k2, and the energy of the system is the sum of the energies of the two objects.
The two particles act completely independently. That’s all there is to it.
Of course we have made some approximations, but we do not wish to discuss
the precision of our answer at this point. However, you might guess that in a
reasonable size crystal with billions of atoms—and, therefore, billions of terms
in the Hamiltonian—leaving out a few terms wouldn’t make much of an error.
If we had so many down spins that there was an appreciable density, then we
would certainly have to worry about the corrections.
[Interestingly enough, an exact solution can be written down if there are just
the two down spins. The result is not particularly important. But it is interesting
that the equations can be solved exactly for this case. The solution is:
am,n = exp[ikc(xm + xn)] sin(k|xm −xn|),
(15.21)
with the energy
E = 4A −2A cos k1b −2A cos k2b,
and with the wave numbers kc and k related to k1 and k2 by
k1 = kc −k,
k2 = kc + k.
(15.22)
This solution includes the “interaction” of the two spins. It describes the fact
that when the spins come together there is a certain chance of scattering. The
spins act very much like particles with an interaction. But the detailed theory of
their scattering goes beyond what we want to talk about here.]
15-3 Independent particles
In the last section we wrote down a Hamiltonian, Eq. (15.15), for a two-
particle system. Then using an approximation which is equivalent to neglecting
any “interaction” of the two particles, we found the stationary states described
by Eqs. (15.17) and (15.18). This state is just the product of two single-particle
states. The solution we have given for am,n in Eq. (15.18) is, however, really not
satisfactory. We have very carefully pointed out earlier that the state | x9, x4⟩is
not a diﬀerent state from | x4, x9⟩—the order of xm and xn has no signiﬁcance.
In general, the algebraic expression for the amplitude Cm,n must be unchanged
if we interchange the values of xm and xn, since that doesn’t change the state.
Either way, it should represent the amplitude to ﬁnd a down spin at xm and a
15-9","385"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","401","down spin at xn. But notice that (15.18) is not symmetric in xm and xn—since
k1 and k2 can in general be diﬀerent.
The trouble is that we have not forced our solution of Eq. (15.15) to satisfy
this additional condition. Fortunately it is easy to ﬁx things up. Notice ﬁrst that
a solution of the Hamiltonian equation just as good as (15.18) is
am,n = Keik2xmeik1xn.
(15.23)
It even has the same energy we got for (15.18). Any linear combination of (15.18)
and (15.23) is also a good solution, and has an energy still given by Eq. (15.19).
The solution we should have chosen—because of our symmetry requirement—is
just the sum of (15.18) and (15.23):
am,n = K[eik1xmeik2xn + eik2xmeik1xn].
(15.24)
Now, given any k1 and k2 the amplitude Cm,n is independent of which way
we put xm and xn—if we should happen to deﬁne xm and xn reversed we get
the same amplitude. Our interpretation of Eq. (15.24) in terms of “magnons”
must also be diﬀerent. We can no longer say that the equation represents one
particle with wave number k1 and a second particle with wave number k2. The
amplitude (15.24) represents one state with two particles (magnons). The state
is characterized by the two wave numbers k1 and k2. Our solution looks like
a compound state of one particle with the momentum p1 = ℏk1 and another
particle with the momentum p2 = ℏk2, but in our state we can’t say which
particle is which.
By now, this discussion should remind you of Chapter 4 and our story of
identical particles. We have just been showing that the particles of the spin
waves—the magnons—behave like identical Bose particles. All amplitudes must
be symmetric in the coordinates of the two particles—which is the same as
saying that if we “interchange the two particles,” we get back the same amplitude
and with the same sign. But, you may be thinking, why did we choose to add
the two terms in making Eq. (15.24). Why not subtract? With a minus sign,
interchanging xm and xn would just change the sign of am,n which doesn’t matter.
But interchanging xm and xn doesn’t change anything—all the electrons of the
crystal are exactly where they were before, so there is no reason for even the sign
of the amplitude to change. The magnons will behave like Bose particles.†
† In general, the quasi particles of the kind we are discussing may act like either Bose
15-10","414"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","402","The main points of this discussion have been twofold: First, to show you
something about spin waves, and, second, to demonstrate a state whose amplitude
is a product of two amplitudes, and whose energy is the sum of the energies
corresponding to the two amplitudes. For independent particles the amplitude
is the product and the energy is the sum. You can easily see why the energy is
the sum. The energy is the coeﬃcient of t in an imaginary exponential—it is
proportional to the frequency. If two objects are doing something, one of them
with the amplitude e−iE1t/ℏand the other with the amplitude e−iE2t/ℏ, and if the
amplitude for the two things to happen together is the product of the amplitudes
for each, then there is a single frequency in the product which is the sum of the
two frequencies. The energy corresponding to the amplitude product is the sum
of the two energies.
We have gone through a rather long-winded argument to tell you a simple
thing. When you don’t take into account any interaction between particles, you
can think of each particle independently. They can individually exist in the
various diﬀerent states they would have alone, and they will each contribute the
energy they would have had if they were alone. However, you must remember
that if they are identical particles, they may behave either as Bose or as Fermi
particles depending upon the problem. Two extra electrons added to a crystal,
for instance, would have to behave like Fermi particles. When the positions of
two electrons are interchanged, the amplitude must reverse sign. In the equation
corresponding to Eq. (15.24) there would have to be a minus sign between the
two terms on the right. As a consequence, two Fermi particles cannot be in
exactly the same condition—with equal spins and equal k’s. The amplitude for
this state is zero.
15-4 The benzene molecule
Although quantum mechanics provides the basic laws that determine the
structures of molecules, these laws can be applied exactly only to the most simple
compounds.
The chemists have, therefore, worked out various approximate
methods for calculating some of the properties of complicated molecules. We
would now like to show you how the independent particle approximation is used
by the organic chemists. We begin with the benzene molecule.
particles or Fermi particles, and as for free particles, the particles with integral spin are bosons
and those with half-integral spins are fermions. The “magnon” stands for a spin-up electron
turned over. The change in spin is one. The magnon has an integral spin, and is a boson.
15-11","433"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","403","| 1⟩
C
C
C
C
C
C
H
H
H
H
H
H
| 2⟩
C
C
C
C
C
C
H
H
H
H
H
H
Fig. 15-3. The two base states for the benzene molecules used in
Chapter 10.
We discussed the benzene molecule from another point of view in Chapter 10.
There we took an approximate picture of the molecule as a two-state system,
with the two base states shown in Fig. 15-3. There is a ring of six carbons with a
hydrogen bonded to the carbon at each location. With the conventional picture of
valence bonds it is necessary to assume double bonds between half of the carbon
atoms, and in the lowest energy condition there are the two possibilities shown
in the ﬁgure. There are also other, higher-energy states. When we discussed
benzene in Chapter 10, we just took the two states and forgot all the rest. We
found that the ground-state energy of the molecule was not the energy of one of
the states in the ﬁgure, but was lower than that by an amount proportional to
the amplitude to ﬂip from one of these states to the other.
Now we’re going to look at the same molecule from a completely diﬀerent
point of view—using a diﬀerent kind of approximation. The two points of view
will give us diﬀerent answers, but if we improve either approximation it should
lead to the truth, a valid description of benzene. However, if we don’t bother
to improve them, which is of course the usual situation, then you should not be
surprised if the two descriptions do not agree exactly. We shall at least show
that also with the new point-of-view the lowest energy of the benzene molecule
is lower than either of the three-bond structures of Fig. 15-3.
Now we want to use the following picture. Suppose we imagine the six carbon
atoms of a benzene molecule connected only by single bonds as in Fig. 15-4. We
have removed six electrons—since a bond stands for a pair of electrons—so we
15-12","345"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","404","C
C
C
C
C
C
H
H
H
H
H
H
6+
Fig. 15-4. A benzene ring with six electrons removed.
have a six-times ionized benzene molecule. Now we will consider what happens
when we put back the six electrons one at a time, imagining that each one can
run freely around the ring. We assume also that all the bonds shown in Fig. 15-4
are satisﬁed, and don’t need to be considered further.
What happens when we put one electron back into the molecular ion? It
might, of course, be located in any one of the six positions around the ring—
corresponding to six base states. It would also have a certain amplitude, say A,
to go from one position to the next. If we analyze the stationary states, there
would be certain possible energy levels. That’s only for one electron.
Next put a second electron in. And now we make the most ridiculous ap-
proximation that you can think of—that what one electron does is not aﬀected
by what the other is doing. Of course they really will interact; they repel each
other through the Coulomb force, and furthermore when they are both at the
same site, they must have considerably diﬀerent energy than twice the energy
for one being there. Certainly the approximation of independent particles is not
legitimate when there are only six sites—particularly when we want to put in
six electrons. Nevertheless the organic chemists have been able to learn a lot by
making this kind of an approximation.
H
H
C
C
H
H
Fig. 15-5. The ethylene molecule.
Before we work out the benzene molecule in detail, let’s consider a simpler
example—the ethylene molecule which contains just two carbon atoms with
two hydrogen atoms on either side as shown in Fig. 15-5. This molecule has
one “extra” bond involving two electrons between the two carbon atoms. Now
remove one of these electrons; what do we have? We can look at it as a two-state
15-13","333"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","405","system—the remaining electron can be at one carbon or the other. We can
analyze it as a two-state system. The possible energies for the single electron are
either (E0 −A) or (E0 + A), as shown in Fig. 15-6.
E
E0 + A
E0
E0 −A
Fig. 15-6. The possible energy levels for the “extra” electrons in the
ethylene molecule.
Now add the second electron. Good, if we have two electrons, we can put
the ﬁrst one in the lower state and the second one in the upper. Not quite; we
forgot something. Each one of the states is really double. When we say there’s
a possible state with the energy (E0 −A), there are really two. Two electrons
can go into the same state if one has its spin up and the other, its spin down.
(No more can be put in because of the exclusion principle.) So there really are
two possible states of energy (E0 −A). We can draw a diagram, as in Fig. 15-7,
which indicates both the energy levels and their occupancy. In the condition of
lowest energy both electrons will be in the lowest state with their spins opposite.
The energy of the extra bond in the ethylene molecule therefore is 2(E0 −A) if
we neglect the interaction between the two electrons.
Let’s go back to the benzene. Each of the two states of Fig. 15-3 has three
double bonds. Each of these is just like the bond in ethylene, and contributes
2(E0 −A) to the energy if E0 is now the energy to put an electron on a site in
benzene and A is the amplitude to ﬂip to the next site. So the energy should be
roughly 6(E0 −A). But when we studied benzene before, we got that the energy
15-14","299"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","406","E
E0 + A
E0
E0 −A
Fig. 15-7. In the extra bond of the ethylene molecule two electrons
(one spin up, one spin down) can occupy the lowest energy level.
was lower than the energy of the structure with three extra bonds. Let’s see if
the energy for benzene comes out lower than three bonds from our new point of
view.
We start with the six-times ionized benzene ring and add one electron. Now
we have a six-state system. We haven’t solved such a system yet, but we know
what to do. We can write six equations in the six amplitudes, and so on. But
let’s save some work—by noticing that we’ve already solved the problem, when
we worked out the problem of an electron on an inﬁnite line of atoms. Of course,
the benzene is not an inﬁnite line, it has 6 atomic sites in a circle. But imagine
that we open out the circle to a line, and number the atoms along the line from
1 to 6. In an inﬁnite line the next location would be 7, but if we insist that this
location be identical with number 1 and so on, the situation will be just like the
benzene ring. In other words we can take the solution for an inﬁnite line with an
added requirement that the solution must be periodic with a cycle six atoms long.
From Chapter 13 the electron on a line has states of deﬁnite energy when the
amplitude at each site is eikxn = eikbn. For each k the energy is
E = E0 −2A cos kb.
(15.25)
We want to use now only those solutions which repeat every 6 atoms. Let’s do
ﬁrst the general case for a ring of N atoms. If the solution is to have a period of
15-15","304"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","407","N atomic spacing, eikbN must be unity; or kbN must be a multiple of 2π. Taking
s to represent any integer, our condition is that
kbN = 2πs.
(15.26)
We have seen before that there is no meaning to taking k’s outside the range ±π/b.
This means that we get all possible states by taking values of s in the range ±N/2.
We ﬁnd then that for an N-atom ring there are N deﬁnite energy states† and
they have wave numbers ks given by
ks = 2π
Nb s.
(15.27)
Each state has the energy (15.25). We have a line spectrum of possible energy
levels. The spectrum for benzene (N = 6) is shown in Fig. 15-8(b). (The numbers
in parentheses indicate the number of diﬀerent states with the same energy.)
(a)
s=0
s=1
s=2
s=3
s=−2
s=−1
2π/6
2A
(b)
E
E0+2A
E0+A
E0
E0−A
E0−2A
(1)
(2)
(2)
(1)
Fig. 15-8. The energy levels in a ring with six electron locations (for
example, a benzene ring).
There’s a nice way to visualize the six energy levels, as we have shown in
part (a) of the ﬁgure. Imagine a circle centered on a level with E0, and with a
radius of 2A. If we start at the bottom and mark oﬀsix equal arcs (at angles
from the bottom point of ksb = 2πs/N, or 2πs/6 for benzene), then the vertical
† You might think that for N an even number there are N + 1 states. That is not so because
s = ±N/2 give the same state.
15-16","260"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","408","heights of the points on the circle are the solutions of Eq. (15.25). The six points
represent the six possible states. The lowest energy level is at (E0 −2A); there
are two states with the same energy (E0 −A), and so on.† These are possible
states for one electron. If we have more than one electron, two—with opposite
spins—can go into each state.
For the benzene molecule we have to put in six electrons. For the ground state
they will go into the lowest possible energy states—two at s = 0, two at s = +1,
and two at s = −1. According to the independent particle approximation the
energy of the ground state is
Eground = 2(E0 −2A) + 4(E0 −A)
= 6E0 −8A.
(15.28)
The energy is indeed less than that of three separate double bonds—by the
amount 2A.
By comparing the energy of benzene to the energy of ethylene it is possible to
determine A. It comes out to be 0.8 electron volt, or, in the units the chemists
like, 18 kilocalories per mole.
We can use this description to calculate or understand other properties of
benzene. For example, using Fig. 15-8 we can discuss the excitation of benzene
by light. What would happen if we tried to excite one of the electrons? It could
move up to one of the empty higher states. The lowest energy of excitation would
be a transition from the highest ﬁlled level to the lowest empty level. That takes
the energy 2A. Benzene will absorb light of frequency ν when hν = 2A. There
will also be absorption of photons with the energies 3A and 4A. Needless to
say, the absorption spectrum of benzene has been measured and the pattern of
spectral lines is more or less correct except that the lowest transition occurs in
the ultraviolet; and to ﬁt the data one would have to choose a value of A between
1.4 and 2.4 electron volts. That is, the numerical value of A is two or three times
larger than is predicted from the chemical binding energy.
What the chemist does in situations like this is to analyze many molecules
of a similar kind and get some empirical rules. He learns, for example: For
calculating binding energy use such and such a value of A, but for getting the
absorption spectrum approximately right use another value of A. You may feel
† When there are two states (which will have diﬀerent amplitude distributions) with the
same energy, we say that the two states are “degenerate.” Notice that four electrons can have
the energy E0 −A.
15-17","436"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","409","that this sounds a little absurd. It is not very satisfactory from the point of view
of a physicist who is trying to understand nature from ﬁrst principles. But the
problem of the chemist is diﬀerent. He must try to guess ahead of time what
is going to happen with molecules that haven’t been made yet, or which aren’t
understood completely. What he needs is a series of empirical rules; it doesn’t
make much diﬀerence where they come from. So he uses the theory in quite
a diﬀerent way than the physicist. He takes equations that have some shadow
of the truth in them, but then he must alter the constants in them—making
empirical corrections.
In the case of benzene, the principal reason for the inconsistency is our
assumption that the electrons are independent—the theory we started with is
really not legitimate. Nevertheless, it has some shadow of the truth because its
results seem to be going in the right direction. With such equations plus some
empirical rules—including various exceptions—the organic chemist makes his way
through the morass of complicated things he chooses to study. (Don’t forget that
the reason a physicist can really calculate from ﬁrst principles is that he chooses
only simple problems. He never solves a problem with 42 or even 6 electrons in
it. So far, he has been able to calculate reasonably accurately only the hydrogen
atom and the helium atom.)
15-5 More organic chemistry
Let’s see how the same ideas can be used to study other molecules. Consider
a molecule like butadiene (1, 3)—it is drawn in Fig. 15-9 according to the usual
valence bond picture.
H
H
C
C
C
C
H
H
Fig. 15-9. The valence band representation of the molecule butadi-
ene (1, 3).
We can play the same game with the extra four electrons corresponding to
the two double bonds. If we remove four electrons, we have four carbon atoms
in a line. You already know how to solve a line. You say, “Oh no, I only know
how to solve an inﬁnite line.” But the solutions for the inﬁnite line also include
the ones for a ﬁnite line. Watch. Let N be the number of atoms on the line and
number them from 1 to N as shown in Fig. 15-10. In writing the equations for
15-18","388"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","410","−1
0
1
2
3
4
5 . . . N−1 N
N+1
Fig. 15-10. A line of N molecules.
the amplitude at position 1 you would not have a term feeding from position 0.
Similarly, the equation for position N would diﬀer from the one that we used
for an inﬁnite line because there would be nothing feeding from position N + 1.
But suppose that we can obtain a solution for the inﬁnite line which has the
following property: the amplitude to be at atom 0 is zero and the amplitude to
be at atom (N + 1) is also zero. Then the set of equations for all the locations
from 1 to N on the ﬁnite line are also satisﬁed. You might think no such solution
exists for the inﬁnite line because our solutions all looked like eikxn which has
the same absolute value of the amplitude everywhere. But you will remember
that the energy depends only on the absolute value of k, so that another solution,
which is equally legitimate for the same energy, would be e−ikxn. And the same
is true of any superposition of these two solutions. By subtracting them we can
get the solution sin kxn, which satisﬁes the requirement that the amplitude be
zero at x = 0. It still corresponds to the energy (E0 −2A cos kb). Now by a
suitable choice for the value of k we can also make the amplitude zero at xN+1.
This requires that (N + 1)kb be a multiple of π, or that
kb =
π
(N + 1) s,
(15.29)
where s is an integer from 1 to N. (We take only positive k’s because each
solution contains +k and −k; changing the sign of k gives the same state all over
again.) For the butadiene molecule, N = 4, so there are four states with
kb = π/5,
2π/5,
3π/5,
and
4π/5.
(15.30)
We can represent the energy levels using a circle diagram similar to the one
for benzene. This time we use a semicircle divided into ﬁve equal parts as shown
in Fig. 15-11. The point at the bottom corresponds to s = 0, which gives no state
at all. The same is true of the point at the top, which corresponds to s = N + 1.
The remaining 4 points give us four allowed energies. There are four stationary
15-19","400"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","411","36◦
2A
E
E0+1.618A
E0+0.618A
E0
E0−0.618A
E0−1.618A
Fig. 15-11. The energy levels of butadiene.
states, which is what we expect having started with four base states. In the circle
diagram, the angular intervals are π/5 or 36 degrees. The lowest energy comes
out (E0 −1.618A). (Ah, what wonders mathematics holds; the golden mean of
the Greeks† gives us the lowest energy state of the butadiene molecule according
to this theory!)
Now we can calculate the energy of the butadiene molecule when we put in
four electrons. With four electrons, we ﬁll up the lowest two levels, each with
two electrons of opposite spin. The total energy is
E = 2(E0 −1.618A) + 2(E0 −0.618A) = 4(E0 −A) −0.472A.
(15.31)
This result seems reasonable. The energy is a little lower than for two simple
double bonds, but the binding is not so strong as in benzene. Anyway this is the
way the chemist analyzes some organic molecules.
The chemist can use not only the energies but the probability amplitudes as
well. Knowing the amplitudes for each state, and which states are occupied, he
can tell the probability of ﬁnding an electron anywhere in the molecule. Those
places where the electrons are more likely to be are apt to be reactive in chemical
substitutions which require that an electron be shared with some other group
of atoms. The other sites are more likely to be reactive in those substitutions
which have a tendency to yield an extra electron to the system.
The same ideas we have been using can give us some understanding of a
molecule even as complicated as chlorophyll, one version of which is shown in
† The ratio of the sides of a rectangle which can be divided into a square and a similar
rectangle.
15-20","298"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","412","H3C
H3C
CH
CH2
CH2
COOC20H39
N
N
CH2
Mg
C
COOCH3
N
N
CH3
C
C2H5
O
Fig. 15-12. A chlorophyll molecule.
Fig. 15-12. Notice that the double and single bonds we have drawn with heavy
lines form a long closed ring with twenty intervals. The extra electrons of the
double bonds can run around this ring. Using the independent particle method
we can get a whole set of energy levels. There are strong absorption lines from
transitions between these levels which lie in the visible part of the spectrum, and
give this molecule its strong color. Similar complicated molecules such as the
xanthophylls, which make leaves turn red, can be studied in the same way.
There is one more idea which emerges from the application of this kind of
theory in organic chemistry. It is probably the most successful or, at least in a
certain sense, the most accurate. This idea has to do with the question: In what
situations does one get a particularly strong chemical binding? The answer is
very interesting. Take the example, ﬁrst, of benzene, and imagine the sequence of
events that occurs as we start with the six-times ionized molecule and add more
and more electrons. We would then be thinking of various benzene ions—negative
or positive. Suppose we plot the energy of the ion (or neutral molecule) as a
function of the number of electrons. If we take E0 = 0 (since we don’t know what
15-21","245"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","413","n
Etotal
0
2
4
6
8
10
12
−8A
Fig. 15-13. The sum of all the electron energies when the lowest
states in Fig. 15-8 are occupied by n electrons if we take that E0 = 0.
it is), we get the curve shown in Fig. 15-13. For the ﬁrst two electrons the slope
of the function is a straight line. For each successive group the slope increases,
and there is a discontinuity in slope between the groups of electrons. The slope
changes when one has just ﬁnished ﬁlling a set of levels which all have the same
energy and must move up to the next higher set of levels for the next electron.
The actual energy of the benzene ion is really quite diﬀerent from the curve of
Fig. 15-13 because of the interactions of the electrons and because of electrostatic
energies we have been neglecting. These corrections will, however, vary with n
in a rather smooth way. Even if we were to make all these corrections, the
resulting energy curve would still have kinks at those values of n which just ﬁll
up a particular energy level.
Now consider a very smooth curve that ﬁts the points on the average like the
one drawn in Fig. 15-14. We can say that the points above this curve have “higher-
n
E
0
2
4
6
8
10
12
Fig. 15-14. The points of Fig. 15-13 with a smooth curve. Molecules
with n = 2, 6, 10 are more stable than the others.
15-22","255"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","414","than-normal” energies, and the points below the curve have “lower-than-normal”
energies. We would, in general, expect that those conﬁgurations with a lower-
than-normal energy would have an above average stability—chemically speaking.
Notice that the conﬁgurations farther below the curve always occur at the end of
one of the straight line segments—namely when there are enough electrons to ﬁll
up an “energy shell,” as it is called. This is the very accurate prediction of the
theory. Molecules—or ions—are particularly stable (in comparison with other
similar conﬁgurations) when the available electrons just ﬁll up an energy shell.
E
E0 + A
E0
E0 −A
(2)
(1)
Fig. 15-15. Energy diagram for a ring of three.
This theory has explained and predicted some very peculiar chemical facts.
To take a very simple example, consider a ring of three. It’s almost unbelievable
that the chemist can make a ring of three and have it stable, but it has been
done. The energy circle for three electrons is shown in Fig. 15-15. Now if you
put two electrons in the lower state, you have only two of the three electrons
that you require. The third electron must be put in at a much higher level.
By our argument this molecule should not be particularly stable, whereas the
two-electron structure should be stable. It does turn out, in fact, that the neutral
molecule of triphenyl cyclopropenyl is very hard to make, but that the positive
ion shown in Fig. 15-16 is relatively easy to make. The ring of three is never
really easy because there is always a large stress when the bonds in an organic
molecule make an equilateral triangle. To make a stable compound at all, the
structure must be stabilized in some way. Anyway if you add three benzene rings
on the corners, the positive ion can be made. (The reason for this requirement
of added benzene rings is not really understood.)
15-23","320"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","415","Fig. 15-16. The triphenyl cyclopropanyl cation.
In a similar way the ﬁve-sided ring can also be analyzed. If you draw the
energy diagram, you can see in a qualitative way that the six-electron structure
should be an especially stable structure, so that such a molecule should be most
stable as a negative ion. Now the ﬁve-ring is well known and easy to make and
always acts as a negative ion. Similarly, you can easily verify that a ring of 4
or 8 is not very interesting, but that a ring of 14 or 10—like a ring of 6—should
be especially stable as a neutral object.
15-6 Other uses of the approximation
There are two other similar situations which we will describe only brieﬂy.
In considering the structure of an atom, we can consider that the electrons ﬁll
successive shells. The Schrödinger theory of electron motion can be worked out
easily only for a single electron moving in a “central” ﬁeld—one which varies
only with the distance from a point. How can we then understand what goes on
in an atom which has 22 electrons?! One way is to use a kind of independent
particle approximation. First you calculate what happens with one electron. You
get a number of energy levels. You put an electron into the lowest energy state.
You can, for a rough model, continue to ignore the electron interactions and go
on ﬁlling successive shells, but there is a way to get better answers by taking into
account—in an approximate way at least—the eﬀect of the electric charge carried
by the electron. Each time you add an electron you compute its amplitude to be
at various places, and then use this amplitude to estimate a kind of spherically
symmetric charge distribution. You use the ﬁeld of this distribution—together
15-24","301"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","416","with the ﬁeld of the positive nucleus and all the previous electrons—to calculate
the states available for the next electron. In this way you can get reasonably
correct estimates for the energies for the neutral atom and for various ionized
states. You ﬁnd that there are energy shells, just as we saw for the electrons in a
ring molecule. With a partially ﬁlled shell, the atom will show a preference for
taking on one or more extra electrons, or for losing some electrons so as to get
into the most stable state of a ﬁlled shell.
This theory explains the machinery behind the fundamental chemical proper-
ties which show up in the periodic table of the elements. The inert gases are those
elements in which a shell has just been completed, and it is especially diﬃcult to
make them react. (Some of them do react of course—with ﬂuorine and oxygen,
for example; but such compounds are very weakly bound; the so-called inert gases
are nearly inert.) An atom which has one electron more or one electron less than
an inert gas will easily lose or gain an electron to get into the especially stable
(low-energy) condition which comes from having a completely ﬁlled shell—they
are the very active chemical elements of valence +1 or −1.
The other situation is found in nuclear physics. In atomic nuclei the protons
and neutrons interact with each other quite strongly. Even so, the independent
particle model can again be used to analyze nuclear structure.
It was ﬁrst
discovered experimentally that nuclei were especially stable if they contained
certain particular numbers of neutrons—namely 2, 8, 20, 28, 50, 82. Nuclei
containing protons in these numbers are also especially stable. Since there was
initially no explanation for these numbers they were called the “magic numbers” of
nuclear physics. It is well known that neutrons and protons interact strongly with
each other; people were, therefore, quite surprised when it was discovered that
an independent particle model predicted a shell structure which came out with
the ﬁrst few magic numbers. The model assumed that each nucleon (proton or
neutron) moved in a central potential which was created by the average eﬀects of
all the other nucleons. This model failed, however, to give the correct values for the
higher magic numbers. Then it was discovered by Maria Mayer, and independently
by Jensen and his collaborators, that by taking the independent particle model
and adding only a correction for what is called the “spin-orbit interaction,” one
could make an improved model which gave all of the magic numbers. (The
spin-orbit interaction causes the energy of a nucleon to be lower if its spin has the
same direction as its orbital angular momentum from motion in the nucleus.) The
theory gives even more—its picture of the so—called “shell structure” of the nuclei
enables us to predict certain characteristics of nuclei and of nuclear reactions.
15-25","484"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","417","The independent particle approximation has been found useful in a wide range
of subjects—from solid-state physics, to chemistry, to biology, to nuclear physics.
It is often only a crude approximation, but is able to give an understanding of
why there are especially stable conditions—in shells. Since it omits all of the
complexity of the interactions between the individual particles, we should not be
surprised that it often fails completely to give correctly many important details.
15-26","76"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","418","16
The Dependence of Amplitudes on Position
16-1 Amplitudes on a line
We are now going to discuss how the probability amplitudes of quantum
mechanics vary in space. In some of the earlier chapters you may have had a
rather uncomfortable feeling that some things were being left out. For example,
when we were talking about the ammonia molecule, we chose to describe it in
terms of two base states. For one base state we picked the situation in which the
nitrogen atom was “above” the plane of the three hydrogen atoms, and for the
other base state we picked the condition in which the nitrogen atom was “below”
the plane of the three hydrogen atoms. Why did we pick just these two states?
Why is it not possible that the nitrogen atom could be at 2 angstroms above
the plane of the three hydrogen atoms, or at 3 angstroms, or at 4 angstroms
above the plane? Certainly, there are many positions that the nitrogen atom
could occupy. Again when we talked about the hydrogen molecular ion, in which
there is one electron shared by two protons, we imagined two base states: one
for the electron in the neighborhood of proton number one, and the other for the
electron in the neighborhood of proton number two. Clearly we were leaving out
many details. The electron is not exactly at proton number two but is only in
the neighborhood. It could be somewhere above the proton, somewhere below
the proton, somewhere to the left of the proton, or somewhere to the right of the
proton.
We intentionally avoided discussing these details. We said that we were
interested in only certain features of the problem, so we were imagining that
when the electron was in the vicinity of proton number one, it would take up a
certain rather deﬁnite condition. In that condition the probability to ﬁnd the
electron would have some rather deﬁnite distribution around the proton, but we
were not interested in the details.
16-1","335"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","419","We can also put it another way. In our discussion of a hydrogen molecular ion
we chose an approximate description when we described the situation in terms of
two base states. In reality there are lots and lots of these states. An electron can
take up a condition around a proton in its lowest, or ground, state, but there are
also many excited states. For each excited state the distribution of the electron
around the proton is diﬀerent. We ignored these excited states, saying that we
were interested in only the conditions of low energy. But it is just these other
excited states which give the possibility of various distributions of the electron
around the proton. If we want to describe in detail the hydrogen molecular ion,
we have to take into account also these other possible base states. We could do
this in several ways, and one way is to consider in greater detail states in which
the location of the electron in space is more carefully described.
We are now ready to consider a more elaborate procedure which will allow
us to talk in detail about the position of the electron, by giving a probability
amplitude to ﬁnd the electron anywhere and everywhere in a given situation.
This more complete theory provides the underpinning for the approximations we
have been making in our earlier discussions. In a sense, our early equations can
be derived as a kind of approximation to the more complete theory.
You may be wondering why we did not begin with the more complete theory
and make the approximations as we went along. We have felt that it would be
much easier for you to gain an understanding of the basic machinery of quantum
mechanics by beginning with the two-state approximations and working gradually
up to the more complete theory than to approach the subject the other way
around. For this reason our approach to the subject appears to be in the reverse
order to the one you will ﬁnd in many books.
As we go into the subject of this chapter you will notice that we are breaking
a rule we have always followed in the past. Whenever we have taken up any
subject we have always tried to give a more or less complete description of the
physics—showing you as much as we could about where the ideas led to. We
have tried to describe the general consequences of a theory as well as describing
some speciﬁc detail so that you could see where the theory would lead. We are
now going to break that rule; we are going to describe how one can talk about
probability amplitudes in space and show you the diﬀerential equations which
they satisfy. We will not have time to go on and discuss many of the obvious
implications which come out of the theory. Indeed we will not even be able to
get far enough to relate this theory to some of the approximate formulations we
have used earlier—for example, to the hydrogen molecule or to the ammonia
16-2","512"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","420","molecule.
For once, we must leave our business unﬁnished and open-ended.
We are approaching the end of our course, and we must satisfy ourselves with
trying to give you an introduction to the general ideas and with indicating the
connections between what we have been describing and some of the other ways of
approaching the subject of quantum mechanics. We hope to give you enough of
an idea that you can go oﬀby yourself and by reading books learn about many
of the implications of the equations we are going to describe. We must, after all,
leave something for the future.
Let’s review once more what we have found out about how an electron can
move along a line of atoms. When an electron has an amplitude to jump from
one atom to the next, there are deﬁnite energy states in which the probability
amplitude for ﬁnding the electron is distributed along the lattice in the form of a
traveling wave. For long wavelengths—for small values of the wave number k—the
energy of the state is proportional to the square of the wave number. For a
crystal lattice with the spacing b, in which the amplitude per unit time for the
electron to jump from one atom to the next is iA/ℏ, the energy of the state is
related to k (for small kb) by
E = Ak2b2
(16.1)
(see Section 13-2). We also saw that groups of such waves with similar energies
would make up a wave packet which would behave like a classical particle with a
mass meﬀgiven by:
meﬀ=
ℏ2
2Ab2 .
(16.2)
Since waves of probability amplitude in a crystal behave like a particle, one
might well expect that the general quantum mechanical description of a particle
would show the same kind of wave behavior we observed for the lattice. Suppose
we were to think of a lattice on a line and imagine that the lattice spacing b
were to be made smaller and smaller. In the limit we would be thinking of a
case in which the electron could be anywhere along the line. We would have
gone over to a continuous distribution of probability amplitudes. We would have
the amplitude to ﬁnd an electron anywhere along the line. This would be one
way to describe the motion of an electron in a vacuum. In other words, if we
imagine that space can be labeled by an inﬁnity of points all very close together
and we can work out the equations that relate the amplitudes at one point to
the amplitudes at neighboring points, we will have the quantum mechanical laws
of motion of an electron in space.
16-3","444"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","421","Let’s begin by recalling some of the general principles of quantum mechanics.
Suppose we have a particle which can exist in various conditions in a quantum
mechanical system. Any particular condition an electron can be found in, we
call a “state,” which we label with a state vector, say | φ⟩. Some other condition
would be labeled with another state vector, say | ψ⟩. We then introduce the
idea of base states. We say that there is a set of states | 1⟩, | 2⟩, | 3⟩, | 4⟩, and
so on, which have the following properties. First, all of these states are quite
distinct—we say they are orthogonal. By this we mean that for any two of the
base states | i⟩and | j⟩the amplitude ⟨i | j⟩that an electron known to be in the
state | i⟩is also in the state | j⟩is equal to zero—unless, of course, | i⟩and | j⟩
stand for the same state. We represent this symbolically by
⟨i | j⟩= δij.
(16.3)
You will remember that δij = 0 if i and j are diﬀerent, and δij = 1 if i and j are
the same number.
Second, the base states | i⟩must be a complete set, so that any state at all
can be described in terms of them. That is, any state | φ⟩at all can be described
completely by giving all of the amplitudes ⟨i | φ⟩that a particle in the state | φ⟩
will also be found in the state | i⟩. In fact, the state vector | φ⟩is equal to the
sum of the base states each multiplied by a coeﬃcient which is the amplitude of
the state | φ⟩is also in the state | i⟩:
| φ⟩=
X
i
| i⟩⟨i | φ⟩.
(16.4)
Finally, if we consider any two states | φ⟩and | ψ⟩, the amplitude that the
state | ψ⟩will also be in the state | φ⟩can be found by ﬁrst projecting the state | ψ⟩
into the base states and then projecting from each base state into the state | φ⟩.
We write that in the following way:
⟨φ | ψ⟩=
X
i
⟨φ | i⟩⟨i | ψ⟩.
(16.5)
The summation is, of course, to be carried out over the whole set of base state | i⟩.
In Chapter 13 when we were working out what happens with an electron
placed on a linear array of atoms, we chose a set of base states in which the
electron was localized at one or other of the atoms in the line. The base state | n⟩
16-4","432"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","422","represented the condition in which the electron was localized at atom number “n.”
(There is, of course, no signiﬁcance to the fact that we called our base states | n⟩
instead of | i⟩.) A little later, we found it convenient to label the base states by
the coordinate xn of the atom rather than by the number of the atom in the array.
The state | xn⟩is just another way of writing the state | n⟩. Then, following the
general rules, any state at all, say | ψ⟩is described by giving the amplitudes and
that an electron in the state | ψ⟩is also in one of the states | xn⟩. For convenience
we have chosen to let the symbol Cn stand for these amplitudes,
Cn = ⟨xn | ψ⟩.
(16.6)
Since the base states are associated with a location along the line, we can
think of the amplitude Cn as a function of the coordinate x and write it as C(xn).
The amplitudes C(xn) will, in general, vary with time and are, therefore, also
functions of t. We will not generally bother to show explicitly this dependence.
In Chapter 13 we then proposed that the amplitudes C(xn) should vary with
time in a way described by the Hamiltonian equation (Eq. 13.3). In our new
notation this equation is
iℏ∂C(xn)
∂t
= E0C(xn) −AC(xn + b) −AC(xn −b).
(16.7)
The last two terms on the right-hand side represent the process in which an
electron at atom (n + 1) or at atom (n −1) can feed into atom n.
We found that Eq. (16.7) has solutions corresponding to deﬁnite energy states,
which we wrote as
C(xn) = e−iEt/ℏeikxn.
(16.8)
For the low-energy states the wavelengths are large (k is small), and the energy
is related to k by
E = (E0 −2A) + Ak2b2,
(16.9)
or, choosing our zero of energy so that (E0 −2A) = 0, the energy is given by
Eq. (16.1).
Let’s see what might happen if we were to let the lattice spacing b go to zero,
keeping the wave number k ﬁxed. If that is all that were to happen the last term
in Eq. (16.9) would just go to zero and there would be no physics. But suppose
A and b are varied together so that as b goes to zero the product Ab2 is kept
16-5","393"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","423","constant†—using Eq. (16.2) we will write Ab2 as the constant ℏ2/2meﬀ. Under
these circumstances, Eq. (16.9) would be unchanged, but what would happen to
the diﬀerential equation (16.7)?
First we will rewrite Eq. (16.7) as
iℏ∂C(xn)
∂t
= (E0 −2A)C(xn) + A[2C(xn) −C(xn + b) −C(xn −b)]. (16.10)
For our choice of E0, the ﬁrst term drops out. Next, we can think of a continuous
function C(x) that goes smoothly through the proper values C(xn) at each xn.
As the spacing b goes to zero, the points xn get closer and closer together, and
(if we keep the variation of C(x) fairly smooth) the quantity in the brackets is
just proportional to the second derivative of C(x). We can write—as you can see
by making a Taylor expansion of each term—the equality
2C(x) −C(x + b) −C(x −b) ≈−b2 ∂2C(x)
∂x2
.
(16.11)
In the limit, then, as b goes to zero, keeping b2A equal to K, Eq. (16.7) goes over
into
iℏ∂C(x)
∂t
= −ℏ2
2meﬀ
∂2C(x)
∂x2
.
(16.12)
We have an equation which says that the time rate of change of C(x)—the
amplitude to ﬁnd the electron at x—depends on the amplitude to ﬁnd the
electron at nearby points in a way which is proportional to the second derivative
of the amplitude with respect to position.
The correct quantum mechanical equation for the motion of an electron in
free space was ﬁrst discovered by Schrödinger. For motion along a line it has
exactly the form of Eq. (16.12) if we replace meﬀby m, the free-space mass of
the electron. For motion along a line in free space the Schrödinger equation is
iℏ∂C(x)
∂t
= −ℏ2
2m
∂2C(x)
∂x2
.
(16.13)
We do not intend to have you think we have derived the Schrödinger equation
but only wish to show you one way of thinking about it. When Schrödinger ﬁrst
† You can imagine that as the points xn get closer together, the amplitude A to jump from
xn±1 to xn will increase.
16-6","336"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","424","wrote it down, he gave a kind of derivation based on some heuristic arguments and
some brilliant intuitive guesses. Some of the arguments he used were even false,
but that does not matter; the only important thing is that the ultimate equation
gives a correct description of nature. The purpose of our discussion is then simply
to show you that the correct fundamental quantum mechanical equation (16.13)
has the same form you get for the limiting case of an electron moving along a line
of atoms. This means that we can think of the diﬀerential equation in (16.13) as
describing the diﬀusion of a probability amplitude from one point to the next
along the line. That is, if an electron has a certain amplitude to be at one point,
it will, a little time later, have some amplitude to be at neighboring points. In
fact, the equation looks something like the diﬀusion equations which we have
used in Volume I. But there is one main diﬀerence: the imaginary coeﬃcient in
front of the time derivative makes the behavior completely diﬀerent from the
ordinary diﬀusion such as you would have for a gas spreading out along a thin
tube. Ordinary diﬀusion gives rise to real exponential solutions, whereas the
solutions of Eq. (16.13) are complex waves.
16-2 The wave function
Now that you have some idea about how things are going to look, we want to
go back to the beginning and study the problem of describing the motion of an
electron along a line without having to consider states connected with atoms on
a lattice. We want to go back to the beginning and see what ideas we have to
use if we want to describe the motion of a free particle in space. Since we are
interested in the behavior of a particle, along a continuum, we will be dealing
with an inﬁnite number of possible states and, as you will see, the ideas we have
developed for dealing with a ﬁnite number of states will need some technical
modiﬁcations.
We begin by letting the state vector | x⟩stand for a state in which a particle
is located precisely at the coordinate x. For every value x along the line—for
instance 1.73, or 9.67, or 10.00—there is the corresponding state. We will take
these states | x⟩as our base states and, if we include all the points on the line,
we will have a complete set for motion in one dimension. Now suppose we have a
diﬀerent kind of a state, say | ψ⟩, in which an electron is distributed in some way
along the line. One way of describing this state is to give all the amplitudes that
the electron will be also found in each of the base states | x⟩. We must give an
inﬁnite set of amplitudes, one for each value of x. We will write these amplitudes
16-7","484"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","425","as ⟨x | ψ⟩. Each of these amplitudes is a complex number and since there is one
such complex number for each value of x, the amplitude ⟨x | ψ⟩is indeed just a
function of x. We will also write it as C(x),
C(x) ≡⟨x | ψ⟩.
(16.14)
We have already considered such amplitudes which vary in a continuous way
with the coordinates when we talked about the variations of amplitude with
time in Chapter 7. We showed there, for example, that a particle with a deﬁnite
momentum should be expected to have a particular variation of its amplitude
in space. If a particle has a deﬁnite momentum p and a corresponding deﬁnite
energy E, the amplitude to be found at any position x would look like
⟨x | ψ⟩= C(x) ∝e+ipx/ℏ.
(16.15)
This equation expresses an important general principle of quantum mechanics
which connects the base states corresponding to diﬀerent positions in space to
another system of base states—all the states of deﬁnite momentum. The deﬁnite
momentum states are often more convenient than the states in x for certain
kinds of problems. Either set of base states is, of course, equally acceptable for a
description of a quantum mechanical situation. We will come back later to the
matter of the connection between them. For the moment we want to stick to our
discussion of a description in terms of the states | x⟩.
Before proceeding, we want to make one small change in notation which we
hope will not be too confusing. The function C(x), deﬁned in Eq. (16.14), will of
course have a form which depends on the particular state | ψ⟩under consideration.
We should indicate that in some way. We could, for example, specify which
function C(x) we are talking about by a subscript say, Cψ(x). Although this
would be a perfectly satisfactory notation, it is a little bit cumbersome and is
not the one you will ﬁnd in most books. Most people simply omit the letter C
and use the symbol ψ to deﬁne the function
ψ(x) ≡Cψ(x) = ⟨x | ψ⟩.
(16.16)
Since this is the notation used by everybody else in the world, you might as
well get used to it so that you will not be frightened when you come across it
somewhere else. Remember though, that we will now be using ψ in two diﬀerent
ways. In Eq. (16.14), ψ stands for a label we have given to a particular physical
16-8","410"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","426","state of the electron. On the left-hand side of Eq. (16.16), on the other hand,
the symbol ψ is used to deﬁne a mathematical function of x which is equal to
the amplitude to be associated with each point x along the line. We hope it
will not be too confusing once you get accustomed to the idea. Incidentally, the
function ψ(x) is usually called “the wave function”—because it more often than
not has the form of a complex wave in its variables.
Since we have deﬁned ψ(x) to be the amplitude that an electron in the state ψ
will be found at the location x, we would like to interpret the absolute square
of ψ to be the probability of ﬁnding an electron at the position x. Unfortunately,
the probability of ﬁnding a particle exactly at any particular point is zero. The
electron will, in general, be smeared out in a certain region of the line, and
since, in any small piece of the line, there are an inﬁnite number of points, the
probability that it will be at any one of them cannot be a ﬁnite number. We
can only describe the probability of ﬁnding an electron in terms of a probability
distribution† which gives the relative probability of ﬁnding the electron at various
approximate locations along the line. Let’s let prob (x, ∆x) stand for the chance
of ﬁnding the electron in a small interval ∆x located near x. If we go to a small
enough scale in any physical situation, the probability will be varying smoothly
from place to place, and the probability of ﬁnding the electron in any small ﬁnite
line segment ∆x will be proportional to ∆x. We can modify our deﬁnitions to
take this into account.
We can think of the amplitude ⟨x | ψ⟩as representing a kind of “amplitude
density” for all the base states | x⟩in a small region. Since the probability of
ﬁnding an electron in a small interval ∆x at x should be proportional to the
interval ∆x, we choose our deﬁnition of ⟨x | ψ⟩so that the following relation
holds:
prob (x, ∆x) = |⟨x | ψ⟩|2 ∆x.
The amplitude ⟨x | ψ⟩is therefore proportional to the amplitude that an electron
in the state ψ will be found in the base state x and the constant of proportionality
is chosen so that the absolute square of the amplitude ⟨x | ψ⟩gives the probability
density of ﬁnding an electron in any small region. We can write, equivalently,
prob (x, ∆x) = |ψ(x)|2 ∆x.
(16.17)
We will now have to modify some of our earlier equations to make them
compatible with this new deﬁnition of a probability amplitude. Suppose we have
† For a discussion of probability distributions see Vol. I, Section 6-4.
16-9","464"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","427","an electron in the state | ψ⟩and we want to know the amplitude for ﬁnding it in
a diﬀerent state | φ⟩which may correspond to a diﬀerent spread-out condition
of the electron. When we were talking about a ﬁnite set of discrete states, we
would have used Eq. (16.5). Before modifying our deﬁnition of the amplitudes
we would have written
⟨φ | ψ⟩=
X
all x
⟨φ | x⟩⟨x | ψ⟩.
(16.18)
Now if both of these amplitudes are normalized in the same way as we have
described above, then a sum of all the states in a small region of x would be
equivalent to multiplying by ∆x, and the sum over all values of x simply becomes
an integral. With our modiﬁed deﬁnitions, the correct form becomes
⟨φ | ψ⟩=
Z
all x
⟨φ | x⟩⟨x | ψ⟩dx.
(16.19)
The amplitude ⟨x | ψ⟩is what we are now calling ψ(x) and, in a similar way,
we will choose to let the amplitude ⟨x | φ⟩be represented by φ(x). Remembering
that ⟨φ | x⟩is the complex conjugate of ⟨x | φ⟩, we can write Eq. (16.19) as
⟨φ | ψ⟩=
Z
φ∗(x)ψ(x) dx.
(16.20)
With our new deﬁnitions everything follows with the same formulas as before if
you always replace a summation sign by an integral over x.
We should mention one qualiﬁcation to what we have been saying. Any
suitable set of base states must be complete if it is to be used for an adequate
description of what is going on. For an electron in one dimension it is not really
suﬃcient to specify only the base states | x⟩, because for each of these states
the electron may have a spin which is either up or down. One way of getting a
complete set is to take two sets of states in x, one for up spin and the other for
down spin. We will, however, not worry about such complications for the time
being.
16-3 States of deﬁnite momentum
Suppose we have an electron in a state | ψ⟩which is described by the probability
amplitude ⟨x | ψ⟩= ψ(x). We know that this represents a state in which the
16-10","365"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","428","electron is spread out along the line in a certain distribution so that the probability
of ﬁnding the electron in a small interval dx at the location x is just
prob (x, dx) = |ψ(x)|2 dx.
What can we say about the momentum of this electron? We might ask what
is the probability that this electron has the momentum p? Let’s start out by
calculating the amplitude that the state | ψ⟩is in another state | mom p⟩which
we deﬁne to be a state with the deﬁnite momentum p. We can ﬁnd this amplitude
by using our basic equation for the resolution of amplitudes, Eq. (16.19). In
terms of the state | mom p⟩
⟨mom p | ψ⟩=
Z +∞
x=−∞
⟨mom p | x⟩⟨x | ψ⟩dx.
(16.21)
And the probability that the electron will be found with the momentum p should
be given in terms of the absolute square of this amplitude. We have again,
however, a small problem about the normalizations. In general we can only ask
about the probability of ﬁnding an electron with a momentum in a small range dp
at the momentum p. The probability that the momentum is exactly some value p
must be zero (unless the state | ψ⟩happens to be a state of deﬁnite momentum).
Only if we ask for the probability of ﬁnding the momentum in a small range dp
at the momentum p will we get a ﬁnite probability. There are several ways the
normalizations can be adjusted. We will choose one of them which we think to
be the most convenient, although that may not be apparent to you just now.
We take our normalizations so that the probability is related to the amplitude
by
prob (p, dp) = |⟨mom p | ψ⟩|2 dp
2πℏ.
(16.22)
With this deﬁnition the normalization of the amplitude ⟨mom p | x⟩is determined.
The amplitude ⟨mom p | x⟩is, of course, just the complex conjugate of the am-
plitude ⟨x | mom p⟩, which is just the one we have written down in Eq. (16.15).
With the normalization we have chosen, it turns out that the proper constant of
proportionality in front of the exponential is just 1. Namely,
⟨mom p | x⟩= ⟨x | mom p⟩∗= e−ipx/ℏ.
(16.23)
Equation (16.21) then becomes
⟨mom p | ψ⟩=
Z +∞
−∞
e−ipx/ℏ⟨x | ψ⟩dx.
(16.24)
16-11","392"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","429","This equation together with Eq. (16.22) allows us to ﬁnd the momentum distri-
bution for any state | ψ⟩.
Let’s look at a particular example—for instance one in which an electron is
localized in a certain region around x = 0. Suppose we take a wave function
which has the following form:
ψ(x) = Ke−x2/4σ2.
(16.25)
The probability distribution in x for this wave function is the absolute square, or
prob (x, dx) = P(x) dx = K2e−x2/2σ2 dx.
(16.26)
The probability density function P(x) is the Gaussian curve shown in Fig. 16-1.
Most of the probability is concentrated between x = +σ and x = −σ. We say
that the “half-width” of the curve is σ. (More precisely, σ is equal to the root-
mean-square of the coordinate x for something spread out according to this
distribution.) We would normally choose the constant K so that the probability
density P(x) is not merely proportional to the probability per unit length in x of
ﬁnding the electron, but has a scale such that P(x) ∆x is equal to the probability
of ﬁnding the electron in ∆x near x. The constant K which does this can be
found by requiring that
R +∞
−∞P(x) dx = 1, since there must be unit probability
that the electron is found somewhere. Here, we get that K = (2πσ2)−1/4. [We
have used the fact that
R +∞
−∞e−t2 dt = √π; see Vol. I, page 40-12.]
x
−3σ
−2σ
−σ
0
σ
2σ
3σ
σP (x)
0.1
0.2
0.3
0.4
Fig. 16-1. The probability density for the wave function of Eq. (16.25).
16-12","269"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","430","Now let’s ﬁnd the distribution in momentum. Let’s let φ(p) stand for the
amplitude to ﬁnd the electron with the momentum p,
φ(p) ≡⟨mom p | ψ⟩.
(16.27)
Substituting Eq. (16.25) into Eq. (16.24) we get
φ(p) =
Z +∞
−∞
e−ipx/ℏ· Ke−x2/4σ2 dx.
(16.28)
The integral can also be rewritten as
Ke−p2σ2/ℏ2 Z +∞
−∞
e−(1/4σ2)(x+2ipσ2/ℏ)2 dx.
(16.29)
We can now make the substitution u = x + 2ipσ2/ℏ, and the integral is
Z +∞
−∞
e−u2/4σ2 du = 2σ√π.
(16.30)
(The mathematicians would probably object to the way we got there, but the
result is, nevertheless, correct.)
φ(p) = (8πσ2)1/4e−p2σ2/ℏ2.
(16.31)
We have the interesting result that the amplitude function in p has precisely
the same mathematical form as the amplitude function in x; only the width of
the Gaussian is diﬀerent. We can write this as
φ(p) = (η2/2πℏ2)−1/4e−p2/4η2,
(16.32)
where the half-width η of the p-distribution function is related to the half-width σ
of the x-distribution by
η = ℏ
2σ .
(16.33)
Our result says: if we make the width of the distribution in x very small by
making σ small, η becomes large and the distribution in p is very much spread
out. Or, conversely: if we have a narrow distribution in p, it must correspond
to a spread-out distribution in x. We can, if we like, consider η and σ to be
16-13","230"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","431","some measure of the uncertainty in the localization of the momentum and of the
position of the electron in the state we are studying. If we call them ∆p and ∆x
respectively Eq. (16.33) becomes
∆p ∆x = ℏ
2.
(16.34)
Interestingly enough, it is possible to prove that for any other form of a
distribution in x or in p, the product ∆p ∆x cannot be smaller than the one we
have found here. The Gaussian distribution gives the smallest possible value for
the product of the root-mean-square widths. In general, we can say
∆p ∆x ≥ℏ
2.
(16.35)
This is a quantitative statement of the Heisenberg uncertainty principle, which
we have discussed qualitatively many times before. We have usually made the
approximate statement that the minimum value of the product ∆p ∆x is of the
same order as ℏ.
16-4 Normalization of the states in x
We return now to the discussion of the modiﬁcations of our basic equations
which are required when we are dealing with a continuum of base states. When
we have a ﬁnite number of discrete states, a fundamental condition which must
be satisﬁed by the set of base states is
⟨i | j⟩= δij.
(16.36)
If a particle is in one base state, the amplitude to be in another base state is 0.
By choosing a suitable normalization, we have deﬁned the amplitude ⟨i | i⟩to
be 1. These two conditions are described by Eq. (16.36). We want now to see
how this relation must be modiﬁed when we use the base states | x⟩of a particle
on a line. If the particle is known to be in one of the base states | x⟩, what is
the amplitude that it will be in another base state | x′⟩? If x and x′ are two
diﬀerent locations along the line, then the amplitude ⟨x | x′⟩is certainly 0, so that
is consistent with Eq. (16.36). But if x and x′ are equal, the amplitude ⟨x | x′⟩
will not be 1, because of the same old normalization problem. To see how we
have to patch things up, we go back to Eq. (16.19), and apply this equation to
16-14","366"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","432","the special case in which the state | φ⟩is just the base state | x′⟩. We would have
then
⟨x′ | ψ⟩=
Z
⟨x′ | x⟩ψ(x) dx.
(16.37)
Now the amplitude ⟨x | ψ⟩is just what we have been calling the function ψ(x).
Similarly the amplitude ⟨x′ | ψ⟩, since it refers to the same state | ψ⟩, is the same
function of the variable x′, namely ψ(x′). We can, therefore, rewrite Eq. (16.37)
as
ψ(x′) =
Z
⟨x′ | x⟩ψ(x) dx.
(16.38)
This equation must be true for any state | ψ⟩and, therefore, for any arbitrary
function ψ(x). This requirement should completely determine the nature of the
amplitude ⟨x | x′⟩—which is, of course, just a function that depends on x and x′.
Our problem now is to ﬁnd a function f(x, x′), which when multiplied
into ψ(x), and integrated over all x gives just the quantity ψ(x′).
It turns
out that there is no mathematical function which will do this! At least nothing
like what we ordinarily mean by a “function.”
Suppose we pick x′ to be the special number 0 and deﬁne the amplitude ⟨0 | x⟩
to be some function of x, let’s say f(x). Then Eq. (16.38) would read as follows:
ψ(0) =
Z
f(x)ψ(x) dx.
(16.39)
What kind of function f(x) could possibly satisfy this equation? Since the integral
must not depend on what values ψ(x) takes for values of x other than 0, f(x)
must clearly be 0 for all values of x except 0. But if f(x) is 0 everywhere, the
integral will be 0, too, and Eq. (16.39) will not be satisﬁed. So we have an
impossible situation: we wish a function to be 0 everywhere but at a point, and
still to give a ﬁnite integral. Since we can’t ﬁnd a function that does this, the
easiest way out is just to say that the function f(x) is deﬁned by Eq. (16.39).
Namely, f(x) is that function which makes (16.39) correct. The function which
does this was ﬁrst invented by Dirac and carries his name. We write it δ(x).
All we are saying is that the function δ(x) has the strange property that if it is
substituted for f(x) in the Eq. (16.39), the integral picks out the value that ψ(x)
takes on when x is equal 0; and, since the integral must be independent of ψ(x)
for all values of x other than 0, the function δ(x) must be 0 everywhere except
16-15","414"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","433","at x = 0. Summarizing, we write
⟨0 | x⟩= δ(x),
(16.40)
where δ(x) is deﬁned by
ψ(0) =
Z
δ(x)ψ(x) dx.
(16.41)
Notice what happens if we use the special function “1” for the function ψ in
Eq. (16.41). Then we have the result
1 =
Z
δ(x) dx.
(16.42)
That is, the function δ(x) has the property that it is 0 everywhere except at x = 0
but has a ﬁnite integral equal to unity. We must imagine that the function δ(x)
has such a fantastic inﬁnity at one point that the total area comes out equal to
one.
One way of imagining what the Dirac δ-function is like is to think of a sequence
of rectangles—or any other peaked function you care to—which gets narrower
and narrower and higher and higher, always keeping a unit area, as sketched
in Fig. 16-2. The integral of this function from −∞to +∞is always 1. If you
multiply it by any function ψ(x) and integrate the product, you get something
which is approximately the value of the function at x = 0, the approximation
getting better and better as you use the narrower and narrower rectangles. You
can if you wish, imagine the δ-function in terms of this kind of limiting process.
The only important thing, however, is that the δ-function is deﬁned so that
Eq. (16.41) is true for every possible function ψ(x). That uniquely deﬁnes the
δ-function. Its properties are then as we have described.
If we change the argument of the δ-function from x to x−x′, the corresponding
relations are
δ(x −x′) = 0,
x′ ̸= x,
Z
δ(x −x′)ψ(x) dx = ψ(x′).
(16.43)
If we use δ(x−x′) for the amplitude ⟨x | x′⟩in Eq. (16.38), that equation is satisﬁed.
Our result then is that for our base states in x, the condition corresponding
to (16.36) is
⟨x′ | x⟩= δ(x −x′).
(16.44)
16-16","316"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","434","x
0
f (x)
1
2
3
Fig. 16-2. The probability density for the wave function of Eq. (16.25).
We have now completed the necessary modiﬁcations of our basic equations
which are necessary for dealing with the continuum of base states corresponding to
the points along a line. The extension to three dimensions is fairly obvious; ﬁrst we
replace the coordinate x by the vector r. Then integrals over x become replaced by
integrals over x, y, and z. In other words, they become volume integrals. Finally,
the one-dimensional δ-function must be replaced by just the product of three δ-
functions, one in x, one in y, and the other in z, δ(x−x′) δ(y−y′) δ(z−z′). Putting
everything together we get the following set of equations for the amplitudes for
particle in three dimensions:
⟨φ | ψ⟩=
Z
⟨φ | r⟩⟨r | ψ⟩dV,
(16.45)
16-17","144"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","435","⟨r | ψ⟩= ψ(r),
⟨r | φ⟩= φ(r),
(16.46)
⟨φ | ψ⟩=
Z
φ∗(r)ψ(r) dV,
(16.47)
⟨r′ | r⟩= δ(x −x′) δ(y −y′) δ(z −z′).
(16.48)
What happens when there is more than one particle? We will tell you about
how to handle two particles and you will easily see what you must do if you want
to deal with a larger number. Suppose there are two particles, which we can call
particle No. 1 and particle No. 2. What shall we use for the base states? One
perfectly good set can be described by saying that particle 1 is at x1 and particle 2
is at x2, which we can write as | x1, x2⟩. Notice that describing the position of only
one particle does not deﬁne a base state. Each base state must deﬁne the condition
of the entire system. You must not think that each particle moves independently as
a wave in three dimensions. Any physical state | ψ⟩can be deﬁned by giving all of
the amplitudes ⟨x1, x2 | ψ⟩to ﬁnd the two particles at x1 and x2. This generalized
amplitude is therefore a function of the two sets of coordinates x1 and x2. You see
that such a function is not a wave in the sense of an oscillation that moves along in
three dimensions. Neither is it generally simply a product of two individual waves,
one for each particle. It is, in general, some kind of a wave in the six dimensions
deﬁned by x1 and x2. If there are two particles in nature which are interacting,
there is no way of describing what happens to one of the particles by trying to
write down a wave function for it alone. The famous paradoxes that we considered
in earlier chapters—where the measurements made on one particle were claimed
to be able to tell what was going to happen to another particle, or were able to
destroy an interference—have caused people all sorts of trouble because they have
tried to think of the wave function of one particle alone, rather than the correct
wave function in the coordinates of both particles. The complete description can
be given correctly only in terms of functions of the coordinates of both particles.
16-5 The Schrödinger equation
So far we have just been worrying about how we can describe states which
may involve an electron being anywhere at all in space. Now we have to worry
about putting into our description the physics of what can happen in various
16-18","421"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","436","circumstances. As before, we have to worry about how states can change with
time. If we have a state | ψ⟩which goes over into another state | ψ′⟩sometime
later, we can describe the situation for all times by making the wave function—
which is just the amplitude ⟨r | ψ⟩—a function of time as well as a function of
the coordinate. A particle in a given situation can then be described by giving
a time-varying wave function ψ(r, t) = ψ(x, y, z, t). This time-varying wave
function describes the evolution of successive states that occur as time develops.
This so-called “coordinate representation”—which gives the projections of the
state | ψ⟩into the base states | r⟩may not always be the most convenient one to
use—but we will consider it ﬁrst.
In Chapter 8 we described how states varied in time in terms of the Hamilto-
nian Hij. We saw that the time variation of the various amplitudes was given in
terms of the matrix equation
iℏdCi
dt =
X
j
HijCj.
(16.49)
This equation says that the time variation of each amplitude Ci is proportional
to all of the other amplitudes Cj, with the coeﬃcients Hij.
How would we expect Eq. (16.49) to look when we are using the continuum
of base states | x⟩? Let’s ﬁrst remember that Eq. (16.49) can also be written as
iℏd
dt ⟨i | ψ⟩=
X
j
⟨i | ˆH | j⟩⟨j | ψ⟩.
Now it is clear what we should do. For the x-representation we would expect
iℏ∂
∂t ⟨x | ψ⟩=
Z
⟨x | ˆH | x′⟩⟨x′ | ψ⟩dx′.
(16.50)
The sum over the base states | j⟩, gets replaced by an integral over x′. Since
⟨x | ˆH | x′⟩should be some function of x and x′, we can write it as H(x, x′)—which
corresponds to Hij in Eq. (16.49). Then Eq. (16.50) is the same as
iℏ∂
∂t ψ(x) =
Z
H(x, x′)ψ(x′) dx′
with
(16.51)
H(x, x′) ≡⟨x | ˆH | x′⟩.
16-19","334"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","437","According to Eq. (16.51), the rate of change of ψ at x would depend on the
value of ψ at all other points x′; the factor H(x, x′) is the amplitude per unit
time that the electron will jump from x′ to x. It turns out in nature, however,
that this amplitude is zero except for points x′ very close to x. This means—as
we saw in the example of the chain of atoms at the beginning of the chapter,
Eq. (16.12)—that the right-hand side of Eq. (16.51) can be expressed completely
in terms of ψ and the derivatives of ψ with respect to x, all evaluated at the
position x.
For a particle moving freely in space with no forces, no disturbances, the
correct law of physics is
Z
H(x, x′)ψ(x′) dx′ = −ℏ2
2m
∂2
∂x2 ψ(x).
Where did we get that from? Nowhere. It’s not possible to derive it from anything
you know. It came out of the mind of Schrödinger, invented in his struggle to ﬁnd
an understanding of the experimental observations of the real world. You can
perhaps get some clue of why it should be that way by thinking of our derivation
of Eq. (16.12) which came from looking at the propagation of an electron in a
crystal.
Of course, free particles are not very exciting. What happens if we put forces
on the particle? Well, if the force of a particle can be described in terms of a
scalar potential V (x)—which means we are thinking of electric forces but not
magnetic forces—and if we stick to low energies so that we can ignore complexities
which come from relativistic motions, then the Hamiltonian which ﬁts the real
world gives
Z
H(x, x′)ψ(x′) dx′ = −ℏ2
2m
∂2
∂x2 ψ(x) + V (x)ψ(x).
(16.52)
Again, you can get some clue as to the origin of this equation if you go back to
the motion of an electron in a crystal, and see how the equations would have to
be modiﬁed if the energy of the electron varied slowly from one atomic site to
the other—as it might do if there were an electric ﬁeld across the crystal. Then
the term E0 in Eq. (16.7) would vary slowly with position and would correspond
to the new term we have added in (16.52).
[You may be wondering why we went straight from Eq. (16.51) to Eq. (16.52)
instead of just giving you the correct function for the amplitude H(x, x′) =
16-20","416"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","438","⟨x | ˆH | x′⟩. We did that because H(x, x′) can only be written in terms of strange al-
gebraic functions, although the whole integral on the right-hand side of Eq. (16.51)
comes out in terms of things you are used to. If you are really curious, H(x, x′)
can be written in the following way:
H(x, x′) = −ℏ2
2m δ′′(x −x′) + V (x) δ(x −x′),
where δ′′ means the second derivative of the delta function. This rather strange
function can be replaced by a somewhat more convenient algebraic diﬀerential
operator, which is completely equivalent:
H(x, x′) =

−ℏ2
2m
∂2
∂x2 + V (x)

δ(x −x′).
We will not be using these forms, but will work directly with the form in
Eq. (16.52).]
If we now use the expression we have in (16.52) for the integral in (16.50) we
get the following diﬀerential equation for ψ(x) = ⟨x | ψ⟩:
iℏ∂ψ
∂t = −ℏ2
2m
∂2
∂x2 ψ(x) + V (x)ψ(x).
(16.53)
It is fairly obvious what we should use instead of Eq. (16.53) if we are interested
in motion in three dimensions. The only changes are that ∂2/∂x2 gets replaced
by
∇2 = ∂2
∂x2 + ∂2
∂y2 + ∂2
∂z2 ,
and V (x) gets replaced by V (x, y, z). The amplitude ψ(x, y, z) for an electron
moving in a potential V (x, y, z) obeys the diﬀerential equation
iℏ∂ψ
∂t = −ℏ2
2m ∇2ψ + V ψ.
(16.54)
It is called the Schrödinger equation, and was the ﬁrst quantum-mechanical
equation ever known. It was written down by Schrödinger before any of the other
quantum equations we have described in this book were discovered.
Although we have approached the subject along a completely diﬀerent route,
the great historical moment marking the birth of the quantum mechanical descrip-
tion of matter occurred when Schrödinger ﬁrst wrote down his equation in 1926.
16-21","321"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","439","For many years the internal atomic structure of matter had been a great mystery.
No one had been able to understand what held matter together, why there was
chemical binding, and especially how it could be that atoms could be stable.
Although Bohr had been able to give a description of the internal motion of an
electron in a hydrogen atom which seemed to explain the observed spectrum of
light emitted by this atom, the reason that electrons moved in this way remained
a mystery. Schrödinger’s discovery of the proper equations of motion for electrons
on an atomic scale provided a theory from which atomic phenomena could be
calculated quantitatively, accurately, and in detail. In principle, Schrödinger’s
equation is capable of explaining all atomic phenomena except those involving
magnetism and relativity. It explains the energy levels of an atom, and all the
facts of chemical binding. This is, however, true only in principle—the math-
ematics soon becomes too complicated to solve exactly any but the simplest
problems. Only the hydrogen and helium atoms have been calculated to a high
accuracy. However, with various approximations, some fairly sloppy, many of
the facts of more complicated atoms and of the chemical binding of molecules
can be understood. We have shown you some of these approximations in earlier
chapters.
The Schrödinger equation as we have written it does not take into account any
magnetic eﬀects. It is possible to take such eﬀects into account in an approximate
way by adding some more terms to the equation. However, as we have seen
in Volume II, magnetism is essentially a relativistic eﬀect, and so a correct
description of the motion of an electron in an arbitrary electromagnetic ﬁeld
can only be discussed in a proper relativistic equation. The correct relativistic
equation for the motion of an electron was discovered by Dirac a year after
Schrödinger brought forth his equation, and takes on quite a diﬀerent form. We
will not be able to discuss it at all here.
Before we go on to look at some of the consequences of the Schrödinger
equation, we would like to show you what it looks like for a system with a large
number of particles. We will not be making any use of the equation, but just
want to show it to you to emphasize that the wave function ψ is not simply an
ordinary wave in space, but is a function of many variables. If there are many
particles, the equation becomes
iℏ∂ψ(r1, r2, r3, . . . )
∂t
=
X
i
−ℏ2
2mi
∂2ψ
∂x2
i
+ ∂2ψ
∂y2
i
+ ∂2ψ
∂z2
i

+ V (r1, r2, . . . )ψ.
(16.55)
16-22","449"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","440","The potential function V is what corresponds classically to the total potential
energy of all the particles. If there are no external forces acting on the particles,
the function V is simply the electrostatic energy of interaction of all the particles.
That is, if the ith particle carries the charge Ziqe, then the function V is simply†
V (r1, r2, r3, . . . ) =
X
all
pairs
ZiZj
rij
e2.
(16.56)
16-6 Quantized energy levels
In a later chapter we will look in detail at a solution of Schrödinger’s equation
for a particular example. We would like now, however, to show you how one of the
most remarkable consequence of Schrödinger’s equation comes about—namely,
the surprising fact that a diﬀerential equation involving only continuous functions
of continuous variables in space can give rise to quantum eﬀects such as the
discrete energy levels in an atom. The essential fact to understand is how it
can be that an electron which is conﬁned to a certain region of space by some
kind of a potential “well” must necessarily have only one or another of a certain
well-deﬁned set of discrete energies.
Suppose we think of an electron in a one-dimensional situation in which its
potential energy varies with x in a way described by the graph in Fig. 16-3. We
will assume that this potential is static—it doesn’t vary with time. As we have
done so many times before, we would like to look for solutions corresponding to
x
x1
x2
V (x)
E
Fig. 16-3. A potential well for a particle moving along x.
† We are using the convention of the earlier volumes according to which e2 ≡q2
e/4πϵ0.
16-23","282"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","441","states of deﬁnite energy, which means, of deﬁnite frequency. Let’s try a solution
of the form
ψ = a(x)e−iEt/ℏ.
(16.57)
If we substitute this function into the Schrödinger equation, we ﬁnd that the
function a(x) must satisfy the following diﬀerential equation:
d2a(x)
dx2
= 2m
ℏ2 [V (x) −E]a(x).
(16.58)
This equation says that at each x the second derivative of a(x) with respect
to x is proportional to a(x), the coeﬃcient of proportionality being given by the
quantity (2m/ℏ2)(V −E). The second derivative of a(x) is the rate of change
of its slope. If the potential V is greater than the energy E of the particle, the
rate of change of the slope of a(x) will have the same sign as a(x). That means
that the curve of a(x) will be concave away from the x-axis. That is, it will have,
more or less, the character of the positive or negative exponential function, e±x.
This means that in the region to the left of x1, in Fig. 16-3, where V is greater
than the assumed energy E, the function a(x) would have to look like one or
another of the curves shown in part (a) of Fig. 16-4.
If, on the other hand, the potential function V is less than the energy E,
the second derivative of a(x) with respect to x has the opposite sign from a(x)
itself, and the curve of a(x) will always be concave toward the x-axis like one
of the pieces shown in part (b) of Fig. 16-4. The solution in such a region has,
piece-by-piece, roughly the form of a sinusoidal curve.
Now let’s see if we can construct graphically a solution for the function a(x)
which corresponds to a particle of energy Ea in the potential V shown in Fig. 16-3.
Since we are trying to describe a situation in which a particle is bound inside
the potential well, we want to look for solutions in which the wave amplitude
takes on very small values when x is way outside the potential well. We can
easily imagine a curve like the one shown in Fig. 16-5 which tends toward zero
for large negative values of x, and grows smoothly as it approaches x1. Since V
is equal to Ea at x1, the curvature of the function becomes zero at this point.
Between x1 and x2, the quantity V −Ea is always a negative number, so the
function a(x) is always concave toward the axis, and the curvature is larger the
larger the diﬀerence between Ea and V . If we continue the curve into the region
between x1 and x2, it should go more or less as shown in Fig. 16-5.
Now let’s continue this curve into the region to the right of x2. There it
curves away from the axis and takes oﬀtoward large positive values, as drawn in
16-24","479"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","442","V > E
(a)
x
a(x)
V < E
(b)
x
a(x)
Fig. 16-4. Possible shapes of the wave function a(x) for V > E and
for V < E.
x
x1
x2
V
Ea
x
a(x)
Fig. 16-5. A wave function for the energy Ea which goes to zero for
negative x.
16-25","54"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","443","x
x1
x2
a(x)
Fig. 16-6. The wave function a(x) of Fig. 16-5 continued beyond x2.
x
x1
x2
V
Ea
Eb
x
a(x)
Fig. 16-7. The wave function a(x) for an energy Eb greater than Ea.
16-26","38"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","444","Fig. 16-6. For the energy Ea we have chosen, the solution for a(x) gets larger and
larger with increasing x. In fact, its curvature is also increasing (if the potential
continues to stay ﬂat). The amplitude rapidly grows to immense proportions.
What does this mean? It simply means that the particle is not “bound” in the
potential well. It is inﬁnitely more likely to be found outside of the well, than
inside. For the solution we have manufactured, the electron is more likely to be
found at x = +∞than anywhere else. We have failed to ﬁnd a solution for a
bound particle.
Let’s try another energy, say one a little bit higher than Ea—say the energy Eb
in Fig. 16-7. If we start with the same conditions on the left, we get the solution
drawn in the lower half of Fig. 16-7. It looked at ﬁrst as though it were going to
be better, but it ends up just as bad as the solution for Ea—except that now
a(x) is getting more and more negative as we go toward large values of x.
Maybe that’s the clue. Since changing the energy a little bit from Ea to Eb
causes the curve to ﬂip from one side of the axis to the other, perhaps there is
some energy lying between Ea and Eb for which the curve will approach zero for
large values of x. There is, indeed, and we have sketched how the solution might
look in Fig. 16-8.
You should appreciate that the solution we have drawn in the ﬁgure is a very
special one. If we were to raise or lower the energy ever so slightly, the function
would go over into curves like one or the other of the two broken-line curves
shown in Fig. 16-8, and we would not have the proper conditions for a bound
particle. We have obtained a result that if a particle is to be bound in a potential
well, it can do so only if it has a very deﬁnite energy.
Does that mean that there is only one energy for a particle bound in a
potential well? No. Other energies are possible, but not energies too close to Ec.
x
x1
x2
a(x)
V > Ec
V < Ec
V > Ec
Fig. 16-8. A wave function for the energy Ec between Ea and Eb.
16-27","397"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","445","x
E
0
1
2
3
4
5
V
a(x)
E0
E1
E2
E3
E4
Fig. 16-9. The function a(x) for the ﬁve lowest energy bound states.
16-28","28"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","446","Notice that the wave function we have drawn in Fig. 16-8 crosses the axis four
times in the region between x1 and x2. If we were to pick an energy quite a bit
lower than Ec, we could have a solution which crosses the axis only three times,
only two times, only once, or not at all. The possible solutions are sketched in
Fig. 16-9. (There may also be other solutions corresponding to values of the
energy higher than the ones shown.) Our conclusion is that if a particle is bound
in a potential well, its energy can take on only the certain special values in a
discrete energy spectrum. You see how a diﬀerential equation can describe the
basic fact of quantum physics.
We might remark one other thing. If the energy E is above the top of the
potential well, then there are no longer any discrete solutions, and any possible
energy is permitted. Such solutions correspond to the scattering of free particles
by a potential well. We have seen an example of such solutions when we considered
the eﬀects of impurity atoms in a crystal.
16-29","189"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","447","17
Symmetry and Conservation Laws
Review: Chapter 52, Vol. I, Symmetry in Physical Laws
Reference: Angular Momentum in Quantum Mechanics: A. R. Edmonds,
Princeton University Press, 1957
17-1 Symmetry
In classical physics there are a number of quantities which are conserved—
such as momentum, energy, and angular momentum. Conservation theorems
about corresponding quantities also exist in quantum mechanics. The most
beautiful thing of quantum mechanics is that the conservation theorems can, in
a sense, be derived from something else, whereas in classical mechanics they are
practically the starting points of the laws. (There are ways in classical mechanics
to do an analogous thing to what we will do in quantum mechanics, but it can
be done only at a very advanced level.) In quantum mechanics, however, the
conservation laws are very deeply related to the principle of superposition of
amplitudes, and to the symmetry of physical systems under various changes.
This is the subject of the present chapter. Although we will apply these ideas
mostly to the conservation of angular momentum, the essential point is that the
theorems about the conservation of all kinds of quantities are—in the quantum
mechanics—related to the symmetries of the system.
We begin, therefore, by studying the question of symmetries of systems.
A very simple example is the hydrogen molecular ion—we could equally well
take the ammonia molecule—in which there are two states. For the hydrogen
molecular ion we took as our base states one in which the electron was located
near proton number 1, and another in which the electron was located near proton
number 2. The two states—which we called | 1⟩and | 2⟩—are shown again in
Fig. 17-1(a). Now, so long as the two nuclei are both exactly the same, then
17-1","290"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","448","(a)
p
e
p
p
p
e
| 1⟩
| 2⟩
(b)
p
p
e
p
e
p
ˆP | 1⟩
ˆP | 2⟩
P
P
Fig. 17-1. If the states | 1⟩and | 2⟩are reﬂected in the plane P-P,
they go into | 2⟩and | 1⟩, respectively.
there is a certain symmetry in this physical system. That is to say, if we were
to reﬂect the system in the plane halfway between the two protons—by which
we mean that everything on one side of the plane gets moved to the symmetric
position on the other side—we would get the situations in Fig. 17-1(b). Since
the protons are identical, the operation of reﬂection changes | 1⟩into | 2⟩and | 2⟩
into | 1⟩. We’ll call this reﬂection operation ˆP and write
ˆP | 1⟩= | 2⟩,
ˆP | 2⟩= | 1⟩.
(17.1)
17-2","143"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","449","So our ˆP is an operator in the sense that it “does something” to a state to make
a new state. The interesting thing is that ˆP operating on any state produces
some other state of the system.
Now ˆP, like any of the other operators we have described, has matrix elements
which can be deﬁned by the usual obvious notation. Namely,
P11 = ⟨1 | ˆP | 1⟩
and
P12 = ⟨1 | ˆP | 2⟩
are the matrix elements we get if we multiply ˆP | 1⟩and ˆP | 2⟩on the left by ⟨1 |.
From Eq. (17.1) they are
⟨1 | ˆP | 1⟩= P11 = ⟨1 | 2⟩= 0,
⟨1 | ˆP | 2⟩= P12 = ⟨1 | 1⟩= 1.
(17.2)
In the same way we can get P21 and P22. The matrix of ˆP—with respect to the
base system | 1⟩and | 2⟩—is
P =

0
1
1
0

.
(17.3)
We see once again that the words operator and matrix in quantum mechanics
are practically interchangeable. There are slight technical diﬀerences—like the
diﬀerence between a “numeral” and a “number”—but the distinction is something
pedantic that we don’t have to worry about. So whether ˆP deﬁnes an operation,
or is actually used to deﬁne a matrix of numbers, we will call it interchangeably
an operator or a matrix.
Now we would like to point out something. We will suppose that the physics of
the whole hydrogen molecular ion system is symmetrical. It doesn’t have to be—it
depends, for instance, on what else is near it. But if the system is symmetrical,
the following idea should certainly be true. Suppose we start at t = 0 with
the system in the state | 1⟩and ﬁnd after an interval of time t that the system
turns out to be in a more complicated situation—in some linear combination of
the two base states. Remember that in Chapter 8 we used to represent “going
for a period of time” by multiplying by the operator ˆU. That means that the
system would after a while—say 15 seconds to be deﬁnite—be in some other
state. For example, it might be
p
2/3 parts of the state | 1⟩and i
p
1/3 parts of
the state | 2⟩, and we would write
| ψ at 15 sec⟩= ˆU(15, 0) | 1⟩=
p
2/3 | 1⟩+ i
p
1/3 | 2⟩.
(17.4)
17-3","402"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","450","(a)
PROB.
| 1⟩
| 2⟩
| 1⟩
| 2⟩
AFTER TIME
t
(b)
PROB.
| 1⟩
| 2⟩
| 1⟩
| 2⟩
AFTER TIME
t
Fig. 17-2. In a symmetric system, if a pure | 1⟩state develops as
shown in part (a), a pure | 2⟩state will develop as in part (b).
Now we ask what happens if we start the system in the symmetric state | 2⟩and
wait for 15 seconds under the same conditions? It is clear that if the world is
symmetric—as we are supposing—we should get the state symmetric to (17.4):
| ψ at 15 sec⟩= ˆU(15, 0) | 2⟩=
p
2/3 | 2⟩+ i
p
1/3 | 1⟩.
(17.5)
The same ideas are sketched diagrammatically in Fig. 17-2. So if the physics of a
system is symmetrical with respect to some plane, and we work out the behavior
of a particular state, we also know the behavior of the state we would get by
reﬂecting the original state in the symmetry plane.
We would like to say the same things a little bit more generally—which means
a little more abstractly. Let ˆQ be any one of a number of operations that you
could perform on a system without changing the physics. For instance, for ˆQ we
might be thinking of ˆP, the operation of a reﬂection in the plane between the two
atoms in the hydrogen molecule. Or, in a system with two electrons, we might be
thinking of the operation of interchanging the two electrons. Another possibility
would be, in a spherically symmetric system, the operation of a rotation of the
whole system through a ﬁnite angle around some axis—which wouldn’t change
the physics. Of course, we would normally want to give each special case some
special notation for ˆQ. Speciﬁcally, we will normally deﬁne the ˆRy(θ) to be the
17-4","308"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","451","operation “rotate the system about the y-axis by the angle θ”. By ˆQ we mean
just any one of the operators we have described or any other one—which leaves
the basic physical situation unchanged.
Let’s think of some more examples. If we have an atom with no external
magnetic ﬁeld or no external electric ﬁeld, and if we were to turn the coordinates
around any axis, it would be the same physical system. Again, the ammonia
molecule is symmetrical with respect to a reﬂection in a plane parallel to that
of the three hydrogens—so long as there is no electric ﬁeld. When there is an
electric ﬁeld, when we make a reﬂection we would have to change the electric
ﬁeld also, and that changes the physical problem. But if we have no external
ﬁeld, the molecule is symmetrical.
Now we consider a general situation. Suppose we start with the state | ψ1⟩
and after some time or other under given physical conditions it has become the
state | ψ2⟩. We can write
| ψ2⟩= ˆU | ψ1⟩.
(17.6)
[You can be thinking of Eq. (17.4).] Now imagine we perform the operation ˆQ
on the whole system. The state | ψ1⟩will be transformed to a state | ψ′
1⟩, which
we can also write as ˆQ | ψ1⟩. Also the state | ψ2⟩is changed into | ψ′
2⟩= ˆQ | ψ2⟩.
Now if the physics is symmetrical under ˆQ (don’t forget the if ; it is not a general
property of systems), then, waiting for the same time under the same conditions,
we should have
| ψ′
2⟩= ˆU | ψ′
1⟩.
(17.7)
[Like Eq. (17.5).] But we can write ˆQ | ψ1⟩for | ψ′
1⟩and ˆQ | ψ2⟩for | ψ′
2⟩so (17.7)
can also be written
ˆQ | ψ2⟩= ˆU ˆQ | ψ1⟩.
(17.8)
If we now replace | ψ2⟩by ˆU | ψ1⟩—Eq. (17.6)—we get
ˆQ ˆU | ψ1⟩= ˆU ˆQ | ψ1⟩.
(17.9)
It’s not hard to understand what this means. Thinking of the hydrogen ion it
says that: “making a reﬂection and waiting a while”—the expression on the right
of Eq. (17.9)—is the same as “waiting a while and then making a reﬂection”—the
expression on the left of (17.9). These should be the same so long as U doesn’t
change under the reﬂection.
Since (17.9) is true for any starting state | ψ1⟩, it is really an equation about
the operators:
ˆQ ˆU = ˆU ˆQ.
(17.10)
17-5","410"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","452","This is what we wanted to get—it is a mathematical statement of symmetry.
When Eq. (17.10) is true, we say that the operators ˆU and ˆQ commute. We can
then deﬁne “symmetry” in the following way: A physical system is symmetric
with respect to the operation ˆQ when ˆQ commutes with ˆU, the operation of the
passage of time. [In terms of matrices, the product of two operators is equivalent
to the matrix product, so Eq. (17.10) also holds for the matrices Q and U for a
system which is symmetric under the transformation Q.]
Incidentally, since for inﬁnitesimal times ϵ we have ˆU = 1 −i ˆHϵ/ℏ—where ˆH
is the usual Hamiltonian (see Chapter 8)—you can see that if (17.10) is true, it
is also true that
ˆQ ˆH = ˆH ˆQ.
(17.11)
So (17.11) is the mathematical statement of the condition for the symmetry of a
physical situation under the operator ˆQ. It deﬁnes a symmetry.
17-2 Symmetry and conservation
Before applying the result we have just found, we would like to discuss the
idea of symmetry a little more. Suppose that we have a very special situation:
after we operate on a state with ˆQ, we get the same state. This is a very special
case, but let’s suppose it happens to be true for a state | ψ0⟩that | ψ′⟩= ˆQ | ψ0⟩
is physically the same state as | ψ0⟩. That means that | ψ′⟩is equal to | ψ0⟩
except for some phase factor.† How can that happen? For instance, suppose that
we have an H+
2 ion in the state which we once called | I⟩.‡ For this state there
is equal amplitude to be in the base states | 1⟩and | 2⟩. The probabilities are
shown as a bar graph in Fig. 17-3(a). If we operate on | I⟩with the reﬂection
operator ˆP, it ﬂips the state over changing | 1⟩to | 2⟩and | 2⟩to | 1⟩—we get the
probabilities shown in Fig. 17-3(b). But that’s just the state | I⟩all over again.
If we start with state | II⟩the probabilities before and after reﬂection look just
the same. However, there is a diﬀerence if we look at the amplitudes. For the
state | I⟩the amplitudes are the same after the reﬂection, but for the state | II⟩
† Incidentally, you can show that ˆQ is necessarily a unitary operator—which means that if
it operates on | ψ⟩to give some number times | ψ⟩, the number must be of the form eiδ, where
δ is real. It’s a small point, and the proof rests on the following observation. Any operation
like a reﬂection or a rotation doesn’t lose any particles, so the normalization of | ψ′⟩and | ψ⟩
must be the same; they can only diﬀer by a pure imaginary phase factor.
‡ See Section 10-1. The states | I⟩and | II⟩are reversed in this Section relative to the earlier
discussion.
17-6","489"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","453","(a)
| I⟩
| 1⟩
| 2⟩
Prob.
0
1/2
1
(b)
ˆP | I⟩
| 1⟩
| 2⟩
Prob.
0
1/2
1
Fig. 17-3. The state | I⟩and the state ˆP | I⟩obtained by reﬂecting
| I⟩in the central plane.
the amplitudes have the opposite sign. In other words,
ˆP | I⟩= ˆP
| 1⟩+ | 2⟩
√
2

= | 2⟩+ | 1⟩
√
2
= | I⟩,
ˆP | II⟩= ˆP
| 1⟩−| 2⟩
√
2

= | 2⟩−| 1⟩
√
2
= −| II⟩.
(17.12)
If we write ˆP | ψ0⟩= eiδ | ψ0⟩, we have that eiδ = 1 for the state | I⟩and eiδ = −1
for the state | II⟩.
Let’s look at another example. Suppose we have a RHC polarized photon
propagating in the z-direction. If we do the operation of a rotation around the
z-axis, we know that this just multiplies the amplitude by eiφ when φ is the angle
of the rotation. So for the rotation operation in this case, δ is just equal to the
angle of rotation.
Now it is clear that if it happens to be true that an operator ˆQ just changes
the phase of a state at some time, say t = 0, it is true forever. In other words, if
17-7","219"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","454","the state | ψ1⟩goes over into the state | ψ2⟩after a time t, or
ˆU(t, 0) | ψ1⟩= | ψ2⟩
(17.13)
and if the symmetry of the situation makes it so that
ˆQ | ψ1⟩= eiδ | ψ1⟩,
(17.14)
then it is also true that
ˆQ | ψ2⟩= eiδ | ψ2⟩.
(17.15)
This is clear, since
ˆQ | ψ2⟩= ˆQ ˆU | ψ1⟩= ˆU ˆQ | ψ1⟩,
and if ˆQ | ψ1⟩= eiδ | ψ1⟩, then
ˆQ | ψ2⟩= ˆUeiδ | ψ1⟩= eiδ ˆU | ψ1⟩= eiδ | ψ2⟩.
[The sequence of equalities follows from (17.13) and (17.10) for a symmetrical
system, from (17.14), and from the fact that a number like eiδ commutes with
an operator.]
So with certain symmetries something which is true initially is true for all
times. But isn’t that just a conservation law? Yes! It says that if you look
at the original state and by making a little computation on the side discover
that an operation which is a symmetry operation of the system produces only a
multiplication by a certain phase, then you know that the same property will
be true of the ﬁnal state—the same operation multiplies the ﬁnal state by the
same phase factor. This is always true even though we may not know anything
else about the inner mechanism of the universe which changes a system from
the initial to the ﬁnal state. Even if we do not care to look at the details of the
machinery by which the system gets from one state to another, we can still say
that if a thing is in a state with a certain symmetry character originally, and if
the Hamiltonian for this thing is symmetrical under that symmetry operation,
then the state will have the same symmetry character for all times. That’s the
basis of all the conservation laws of quantum mechanics.
Let’s look at a special example. Let’s go back to the ˆP operator. We would
like ﬁrst to modify a little our deﬁnition of ˆP. We want to take for ˆP not just a
mirror reﬂection, because that requires deﬁning the plane in which we put the
mirror. There is a special kind of a reﬂection that doesn’t require the speciﬁcation
17-8","373"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","455","of a plane. Suppose we redeﬁne the operation ˆP this way: First you reﬂect in a
mirror in the z-plane so that z goes to −z, x stays x, and y stays y; then you
turn the system 180◦about the z-axis so that x is made to go to −x and y to −y.
The whole thing is called an inversion. Every point is projected through the
origin to the diametrically opposite position. All the coordinates of everything
are reversed. We will still use the symbol ˆP for this operation. It is shown in
Fig. 17-4. It is a little more convenient than a simple reﬂection because it doesn’t
require that you specify which coordinate plane you used for the reﬂection—you
need specify only the point which is at the center of symmetry.
Now let’s suppose that we have a state | ψ0⟩which under the inversion
operation goes into eiδ | ψ0⟩—that is,
| ψ′
0⟩= ˆP | ψ0⟩= eiδ | ψ0⟩.
(17.16)
Then suppose that we invert again. After two inversions we are right back where
we started from—nothing is changed at all. We must have that
ˆP | ψ′
0⟩= ˆP · ˆP | ψ0⟩= | ψ0⟩.
But
ˆP · ˆP | ψ0⟩= ˆPeiδ | ψ0⟩= eiδ ˆP | ψ0⟩= (eiδ)2 | ψ0⟩.
It follows that
(eiδ)2 = 1.
So if the inversion operator is a symmetry operation of a state, there are only
two possibilities for eiδ:
eiδ = ±1,
which means that
ˆP | ψ0⟩= | ψ0⟩
or
ˆP | ψ0⟩= −| ψ0⟩.
(17.17)
Classically, if a state is symmetric under an inversion, the operation gives back
the same state. In quantum mechanics, however, there are the two possibilities:
we get the same state or minus the same state. When we get the same state,
ˆP | ψ0⟩= | ψ0⟩, we say that the state | ψ0⟩has even parity. When the sign is
reversed so that ˆP | ψ0⟩= −| ψ0⟩, we say that the state has odd parity. (The
inversion operator ˆP is also known as the parity operator.) The state | I⟩of the
H+
2 ion has even parity; and the state | II⟩has odd parity—see Eq. (17.12). There
17-9","366"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","456","(a)
x
y
z
r
A
(b)
x
y
z
r
−r
A′
Fig. 17-4. The operation of inversion, ˆP . Whatever is at the point A
at (x, y, z) is moved to the point A′ at (−x, −y, −z).
are, of course, states which are not symmetric under the operation ˆP; these are
states with no deﬁnite parity. For instance, in the H+
2 system the state | I⟩has
even parity, the state | II⟩has odd parity, and the state | 1⟩has no deﬁnite parity.
When we speak of an operation like inversion being performed “on a physical
system” we can think about it in two ways. We can think of physically moving
whatever is at r to the inverse point at −r, or we can think of looking at
17-10","133"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","457","the same system from a new frame of reference x′, y′, z′ related to the old by
x′ = −x, y′ = −y, and z′ = −z. Similarly, when we think of rotations, we can
think of rotating bodily a physical system, or of rotating the coordinate frame
with respect to which we measure the system, keeping the “system” ﬁxed in space.
Generally, the two points of view are essentially equivalent. For rotation they
are equivalent except that rotating a system by the angle θ is like rotating the
reference frame by the negative of θ. In these lectures we have usually considered
what happens when a projection is made into a new set of axes. What you get
that way is the same as what you get if you leave the axes ﬁxed and rotate the
system backwards by the same amount. When you do that, the signs of the angles
are reversed.†
Many of the laws of physics—but not all—are unchanged by a reﬂection or
an inversion of the coordinates. They are symmetric with respect to an inversion.
The laws of electrodynamics, for instance, are unchanged if we change x to −x, y
to −y, and z to −z in all the equations. The same is true for the laws of gravity,
and for the strong interactions of nuclear physics. Only the weak interactions—
responsible for β-decay—do not have this symmetry. (We discussed this in some
detail in Chapter 52, Vol. I.) We will for now leave out any consideration of
the β-decays. Then in any physical system where β-decays are not expected to
produce any appreciable eﬀect—an example would be the emission of light by
an atom—the Hamiltonian ˆH and the operator ˆP will commute. Under these
circumstances we have the following proposition. If a state originally has even
parity, and if you look at the physical situation at some later time, it will again
have even parity. For instance, suppose an atom about to emit a photon is in
a state known to have even parity. You look at the whole thing—including the
photon—after the emission; it will again have even parity (likewise if you start
with odd parity). This principle is called the conservation of parity. You can see
why the words “conservation of parity” and “reﬂection symmetry” are closely
intertwined in the quantum mechanics. Although until a few years ago it was
thought that nature always conserved parity, it is now known that this is not
true. It has been discovered to be false because the β-decay reaction does not
have the inversion symmetry which is found in the other laws of physics.
Now we can prove an interesting theorem (which is true so long as we can
disregard weak interactions): Any state of deﬁnite energy which is not degenerate
† In other books you may ﬁnd formulas with diﬀerent signs; they are probably using a
diﬀerent deﬁnition of the angles.
17-11","488"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","458","must have a deﬁnite parity.
It must have either even parity or odd parity.
(Remember that we have sometimes seen systems in which several states have
the same energy—we say that such states are degenerate. Our theorem will not
apply to them.)
For a state | ψ0⟩of deﬁnite energy, we know that
ˆH | ψ0⟩= E | ψ0⟩,
(17.18)
where E is just a number—the energy of the state. If we have any operator ˆQ
which is a symmetry operator of the system we can prove that
ˆQ | ψ0⟩= eiδ | ψ0⟩
(17.19)
so long as | ψ0⟩is a unique state of deﬁnite energy. Consider the new state | ψ′
0⟩
that you get from operating with ˆQ. If the physics is symmetric, then | ψ′
0⟩must
have the same energy as | ψ0⟩. But we have taken a situation in which there is
only one state of that energy, namely | ψ0⟩, so | ψ′
0⟩must be the same state—it
can only diﬀer by a phase. That’s the physical argument.
The same thing comes out of our mathematics. Our deﬁnition of symmetry
is Eq. (17.10) or Eq. (17.11) (good for any state ψ),
ˆH ˆQ | ψ⟩= ˆQ ˆH | ψ⟩.
(17.20)
But we are considering only a state | ψ0⟩which is a deﬁnite energy state, so that
ˆH | ψ0⟩= E | ψ0⟩. Since E is just a number that ﬂoats through ˆQ if we want, we
have
ˆQ ˆH | ψ0⟩= ˆQE | ψ0⟩= E ˆQ | ψ0⟩.
So
ˆH{ ˆQ | ψ0⟩} = E{ ˆQ | ψ0⟩}.
(17.21)
So | ψ′
0⟩= ˆQ | ψ0⟩is also a deﬁnite energy state of ˆH—and with the same E. But
by our hypothesis, there is only one such state; it must be that | ψ′
0⟩= eiδ | ψ0⟩.
What we have just proved is true for any operator ˆQ that is a symmetry
operator of the physical system. Therefore, in a situation in which we consider
only electrical forces and strong interactions—and no β-decay—so that inversion
symmetry is an allowed approximation, we have that ˆP | ψ⟩= eiδ | ψ⟩. But we
have also seen that eiδ must be either +1 or −1. So any state of a deﬁnite energy
(which is not degenerate) has got either an even parity or an odd parity.
17-12","388"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","459","17-3 The conservation laws
We turn now to another interesting example of an operation: a rotation. We
consider the special case of an operator that rotates an atomic system by angle φ
around the z-axis. We will call this operator† ˆRz(φ). We are going to suppose
that we have a physical situation where we have no inﬂuences lined up along the
x- and y-axes. Any electric ﬁeld or magnetic ﬁeld is taken to be parallel to the
z-axis‡ so that there will be no change in the external conditions if we rotate
the whole physical system about the z-axis. For example, if we have an atom in
empty space and we turn the atom around the z-axis by an angle φ, we have the
same physical system.
Now then, there are special states which have the property that such an
operation produces a new state which is the original state multiplied by some
phase factor. Let us make a quick side remark to show you that when this is
true the phase change must always be proportional to the angle φ. Suppose that
you would rotate twice by the angle φ. That’s the same thing as rotating by
the angle 2φ. If a rotation by φ has the eﬀect of multiplying the state | ψ0⟩by a
phase eiδ so that
ˆRz(φ) | ψ0⟩= eiδ | ψ0⟩,
two such rotations in succession would multiply the state by the factor (eiδ)2 =
ei2δ, since
ˆRz(φ) ˆRz(φ) | ψ0⟩= ˆRz(φ)eiδ | ψ0⟩= eiδ ˆRz(φ) | ψ0⟩= eiδeiδ | ψ0⟩.
The phase change δ must be proportional to φ.¶ We are considering then those
special states | ψ0⟩for which
ˆRz(φ) | ψ0⟩= eimφ | ψ0⟩,
(17.22)
where m is some real number.
We also know the remarkable fact that if the system is symmetrical for
a rotation around z and if the original state happens to have the property
† Very precisely, we will deﬁne ˆRz(φ) as a rotation of the physical system by −φ about the
z-axis, which is the same as rotating the coordinate frame by +φ.
‡ We can always choose z along the direction of the ﬁeld provided there is only one ﬁeld at
a time, and its direction doesn’t change.
¶ For a fancier proof we should make this argument for small rotations ϵ. Since any angle φ
is the sum of a suitable n number of these, φ = nϵ, ˆRz(φ) = [ ˆRz(ϵ)]n and the total phase
change is n times that for the small angle ϵ, and is, therefore, proportional to φ.
17-13","427"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","460","that (17.22) is true, then it will also have the same property later on. So this
number m is a very important one. If we know its value initially, we know its
value at the end of the game. It is a number which is conserved—m is a constant
of the motion. The reason that we pull out m is because it hasn’t anything to
do with any special angle φ, and also because it corresponds to something in
classical mechanics. In quantum mechanics we choose to call mℏ—for such states
as | ψ0⟩—the angular momentum about the z-axis. If we do that we ﬁnd that
in the limit of large systems the same quantity is equal to the z-component of
the angular momentum of classical mechanics. So if we have a state for which a
rotation about the z-axis just produces a phase factor eimφ, then we have a state
of deﬁnite angular momentum about that axis—and the angular momentum is
conserved. It is mℏnow and forever. Of course, you can rotate about any axis,
and you get the conservation of angular momentum for the various axes. You see
that the conservation of angular momentum is related to the fact that when you
turn a system you get the same state with only a new phase factor.
We would like to show you how general this idea is. We will apply it to two
other conservation laws which have exact correspondence in the physical ideas
to the conservation of angular momentum. In classical physics we also have
conservation of momentum and conservation of energy, and it is interesting to
see that both of these are related in the same way to some physical symmetry.
Suppose that we have a physical system—an atom, some complicated nucleus,
or a molecule, or something—and it doesn’t make any diﬀerence if we take the
whole system and move it over to a diﬀerent place. So we have a Hamiltonian
which has the property that it depends only on the internal coordinates in some
sense, and does not depend on the absolute position in space. Under those
circumstances there is a special symmetry operation we can perform which is a
translation in space. Let’s deﬁne ˆDx(a) as the operation of a displacement by
the distance a along the x-axis. Then for any state we can make this operation
and get a new state. But again there can be very special states which have the
property that when you displace them by a along the x-axis you get the same
state except for a phase factor. It’s also possible to prove, just as we did above,
that when this happens, the phase must be proportional to a. So we can write
for these special states | ψ0⟩
ˆDx(a) | ψ0⟩= eika | ψ0⟩.
(17.23)
The coeﬃcient k, when multiplied by ℏ, is called the x-component of the momen-
tum. And the reason it is called that is that this number is numerically equal to
17-14","499"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","461","the classical momentum px when we have a large system. The general statement
is this: If the Hamiltonian is unchanged when the system is displaced, and if the
state starts with a deﬁnite momentum in the x-direction, then the momentum in
the x-direction will remain the same as time goes on. The total momentum of a
system before and after collisions—or after explosions or what not—will be the
same.
There is another operation that is quite analogous to the displacement in space:
a delay in time. Suppose that we have a physical situation where there is nothing
external that depends on time, and we start something oﬀat a certain moment
in a given state and let it roll. Now if we were to start the same thing oﬀagain
(in another experiment) two seconds later—or/say, delayed by a time τ—and if
nothing in the external conditions depends on the absolute time, the development
would be the same and the ﬁnal state would be the same as the other ﬁnal state,
except that it will get there later by the time τ. Under those circumstances we
can also ﬁnd special states which have the property that the development in time
has the special characteristic that the delayed state is just the old, multiplied
by a phase factor. Once more it is clear that for these special states the phase
change must be proportional to τ. We can write
ˆDt(τ) | ψ0⟩= e−iωτ | ψ0⟩.
(17.24)
It is conventional to use the negative sign in deﬁning ω; with this convention ωℏ
is the energy of the system, and it is conserved. So a system of deﬁnite energy is
one which when displaced τ in time reproduces itself multiplied by e−iωτ. (That’s
what we have said before when we deﬁned a quantum state of deﬁnite energy, so
we’re consistent with ourselves.) It means that if a system is in a state of deﬁnite
energy, and if the Hamiltonian doesn’t depend on t, then no matter what goes
on, the system will have the same energy at all later times.
You see, therefore, the relation between the conservation laws and the sym-
metry of the world. Symmetry with respect to displacements in time implies the
conservation of energy; symmetry with respect to position in x, y, or z implies
the conservation of that component of momentum. Symmetry with respect to
rotations around the x-, y-, and z-axes implies the conservation of the x-, y-,
and z-components of angular momentum. Symmetry with respect to reﬂection
implies the conservation of parity. Symmetry with respect to the interchange
of two electrons implies the conservation of something we don’t have a name
for, and so on. Some of these principles have classical analogs and others do
17-15","458"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","462","not. There are more conservation laws in quantum mechanics than are useful in
classical mechanics—or, at least, than are usually made use of.
In order that you will be able to read other books on quantum mechanics,
we must make a small technical aside—to describe the notation that people
use. The operation of a displacement with respect to time is, of course, just the
operation ˆU that we talked about before:
ˆDt(τ) = ˆU(t + τ, t).
(17.25)
Most people like to discuss everything in terms of inﬁnitesimal displacements in
time, or in terms of inﬁnitesimal displacements in space, or in terms of rotations
through inﬁnitesimal angles.
Since any ﬁnite displacement or angle can be
accumulated by a succession of inﬁnitesimal displacements or angles, it is often
easier to analyze ﬁrst the inﬁnitesimal case. The operator of an inﬁnitesimal
displacement ∆t in time is—as we have deﬁned it in Chapter 8—
ˆDt(∆t) = 1 −i
ℏ∆t ˆH.
(17.26)
Then ˆH is analogous to the classical quantity we call energy, because if ˆH | ψ⟩
happens to be a constant times | ψ⟩namely, ˆH | ψ⟩= E | ψ⟩then that constant
is the energy of the system.
The same thing is done for the other operations. If we make a small displace-
ment in x, say by the amount ∆x, a state | ψ⟩will, in general, go over into some
other state | ψ′⟩. We can write
| ψ′⟩= ˆDx(∆x) | ψ⟩=

1 + i
ℏˆpx∆x

| ψ⟩,
(17.27)
since as ∆x goes to zero, the | ψ′⟩should become just | ψ⟩or ˆDx(0) = 1, and for
small ∆x the change of ˆDx(∆x) from 1 should be proportional to ∆x. Deﬁned
this way, the operator ˆpx is called the momentum operator—for the x-component,
of course.
For identical reasons, people usually write for small rotations
ˆRz(∆φ) | ψ⟩=

1 + i
ℏ
ˆJz∆φ

| ψ⟩
(17.28)
and call ˆJz the operator of the z-component of angular momentum. For those
17-16","332"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","463","special states for which ˆRz(φ) | ψ0⟩= eimφ | ψ0⟩, we can for any small angle—
say ∆φ—expand the right-hand side to ﬁrst order in ∆φ and get
ˆRz(∆φ) | ψ0⟩= eim∆φ | ψ0⟩= (1 + im∆φ) | ψ0⟩.
Comparing this with the deﬁnition of ˆJz in Eq. (17.28), we get that
ˆJz | ψ0⟩= mℏ| ψ0⟩.
(17.29)
In other words, if you operate with ˆJz on a state with a deﬁnite angular momentum
about the z-axis, you get mℏtimes the same state, where mℏis the amount
of z-component of angular momentum. It is quite analogous to operating on a
deﬁnite energy state with ˆH to get E | ψ⟩.
We would now like to make some applications of the ideas of the conservation
of angular momentum—to show you how they work. The point is that they are
really very simple. You knew before that angular momentum is conserved. The
only thing you really have to remember from this chapter is that if a state | ψ0⟩
has the property that upon a rotation through an angle φ about the z-axis, it
becomes eimφ | ψ0⟩; it has a z-component of angular momentum equal to mℏ.
That’s all we will need to do a number of interesting things.
17-4 Polarized light
First of all we would like to check on one idea. In Section 11-4 we showed
that when RHC polarized light is viewed in a frame rotated by the angle φ about
the z-axis† it gets multiplied by eiφ. Does that mean then that the photons
of light that are right circularly polarized carry an angular momentum of one
unit‡ along the z-axis? Indeed it does. It also means that if we have a beam
of light containing a large number of photons all circularly polarized the same
way—as we would have in a classical beam—it will carry angular momentum.
If the total energy carried by the beam in a certain time is W, then there are
N = W/ℏω photons. Each one carries the angular momentum ℏ, so there is a
total angular momentum of
Jz = Nℏ= W
ω .
(17.30)
† Sorry! This angle is the negative of the one we used in Section 11-4.
‡ It is usually very convenient to measure angular momentum of atomic systems in units
of ℏ. Then you can say that a spin one-half particle has angular momentum ±1/2 with respect
to any axis. Or, in general, that the z-component of angular momentum is m. You don’t need
to repeat the ℏall the time.
17-17","425"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","464","Can we prove classically that light which is right circularly polarized carries an
energy and angular momentum in proportion to W/ω? That should be a classical
proposition if everything is right. Here we have a case where we can go from
the quantum thing to the classical thing. We should see if the classical physics
checks. It will give us an idea whether we have a right to call m the angular
momentum. Remember what right circularly polarized light is, classically. It’s
described by an electric ﬁeld with an oscillating x-component and an oscillating
y-component 90◦out of phase so that the resultant electric vector E goes in a
circle—as drawn in Fig. 17-5(a). Now suppose that such light shines on a wall
which is going to absorb it—or at least some of it—and consider an atom in the
wall according to the classical physics. We have often described the motion of the
electron in the atom as a harmonic oscillator which can be driven into oscillation
by an external electric ﬁeld. We’ll suppose that the atom is isotropic, so that
it can oscillate equally well in the x- or y-directions. Then in the circularly
polarized light, the x-displacement and the y-displacement are the same, but one
is 90◦behind the other. The net result is that the electron moves in a circle, as
shown in Fig. 17-5(b). The electron is displaced at some displacement r from
its equilibrium position at the origin and goes around with some phase lag with
respect to the vector E. The relation between E and r might be as shown in
Fig. 17-5(b). As time goes on, the electric ﬁeld rotates and the displacement
rotates with the same frequency, so their relative orientation stays the same. Now
let’s look at the work being done on this electron. The rate that energy is being
put into this electron is v, its velocity, times the component of qE parallel to the
velocity:
dW
dt = qEtv.
(17.31)
But look, there is angular momentum being poured into this electron, because
there is always a torque about the origin. The torque is qEtr, which must be
equal to the rate of change of angular momentum dJz/dt:
dJz
dt = qEtr.
(17.32)
Remembering that v = ωr, we have that
dJz
dW = 1
ω .
Therefore, if we integrate the total angular momentum which is absorbed, it
is proportional to the total energy—the constant of proportionality being 1/ω,
17-18","409"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","465","(a)
x
y
φ = ωt
E
(b)
x
y
ELECTRON
r
φ = ωt −φ0
v
E
Et
Fig. 17-5. (a) The electric ﬁeld E in a circularly polarized light wave.
(b) The motion of an electron being driven by the circularly polarized
light.
which agrees with Eq. (17.30). Light does carry angular momentum—1 unit
(times ℏ) if it is right circularly polarized along the z-axis, and −1 unit along the
z-axis if it is left circularly polarized.
Now let’s ask the following question: If light is linearly polarized in the x-
direction, what is its angular momentum? Light polarized in the x-direction can
be represented as the superposition of RHC and LHC polarized light. Therefore,
there is a certain amplitude that the angular momentum is +ℏand another
17-19","130"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","466","amplitude that the angular momentum is −ℏ, so it doesn’t have a deﬁnite angular
momentum. It has an amplitude to appear with +ℏand an equal amplitude
to appear with −ℏ.
The interference of these two amplitudes produces the
linear polarization, but it has equal probabilities to appear with plus or minus
one unit of angular momentum. Macroscopic measurements made on a beam
of linearly polarized light will show that it carries zero angular momentum,
because in a large number of photons there are nearly equal numbers of RHC
and LHC photons contributing opposite amounts of angular momentum—the
average angular momentum is zero. And in the classical theory you don’t ﬁnd
the angular momentum unless there is some circular polarization.
We have said that any spin-one particle can have three values of Jz, namely
+1, 0, −1 (the three states we saw in the Stern-Gerlach experiment). But light
is screwy; it has only two states. It does not have the zero case. This strange
lack is related to the fact that light cannot stand still. For a particle of spin j
which is standing still, there must be the 2j + 1 possible states with values
of jz going in steps of 1 from −j to +j. But it turns out that for something of
spin j with zero mass only the states with the components +j and −j along the
direction of motion exist. For example, light does not have three states, but only
two—although a photon is still an object of spin one. How is this consistent with
our earlier proofs—based on what happens under rotations in space—that for
spin-one particles three states are necessary? For a particle at rest, rotations
can be made about any axis without changing the momentum state. Particles
with zero rest mass (like photons and neutrinos) cannot be at rest; only rotations
about the axis along the direction of motion do not change the momentum state.
Arguments about rotations around one axis only are insuﬃcient to prove that
three states are required, given that one of them varies as eiφ under rotations by
the angle φ.†
One further side remark. For a zero rest mass particle, in general, only
one of the two spin states with respect to the line of motion (+j, −j) is really
necessary. For neutrinos—which are spin one-half particles—only the states with
the component of angular momentum opposite to the direction of motion (−ℏ/2)
exist in nature [and only along the motion (+ℏ/2) for antineutrinos]. When a
† We have tried to ﬁnd at least a proof that the component of angular momentum along
the direction of motion must for a zero mass particle be an integral multiple of ℏ/2—and not
something like ℏ/3. Even using all sorts of properties of the Lorentz transformation and what
not, we failed. Maybe it’s not true. We’ll have to talk about it with Prof. Wigner, who knows
all about such things.
17-20","489"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","467","system has inversion symmetry (so that parity is conserved, as it is for light)
both components (+j, and −j) are required.
17-5 The disintegration of the Λ0
Now we want to give an example of how we use the theorem of conservation
of angular momentum in a speciﬁcally quantum physical problem. We look at
break-up of the lambda particle (Λ0), which disintegrates into a proton and a
π−meson by a “weak” interaction:
Λ0 →p + π−.
Assume we know that the pion has spin zero, that the proton has spin one-half,
and that the Λ0 has spin one-half. We would like to solve the following problem:
Suppose that a Λ0 were to be produced in a way that caused it to be completely
polarized—by which we mean that its spin is, say “up,” with respect to some
suitably chosen z-axis—see Fig. 17-6(a). The question is, with what probability
will it disintegrate so that the proton goes oﬀat an angle θ with respect to the
z-axis—as in Fig. 17-6(b)? In other words, what is the angular distribution of
the disintegrations? We will look at the disintegration in the coordinate system
(a)
BEFORE
z
Λ0
(b)
AFTER
z
p
vp
π−
vπ−
θ
Fig. 17-6. A Λ0 with spin “up” decays into a proton and a pion (in
the CM system). What is the probability that the proton will go oﬀat
the angle θ?
17-21","233"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","468","BEFORE
(a)
z
Λ0
AFTER
or
(b)
z
p
π−
vp
vπ−
YES
(c)
z
p
π−
vp
vπ−
NO
Fig. 17-7. Two possibilities for the decay of a spin “up” Λ0 with the
proton going along the +z-axis. Only (b) conserves angular momentum.
in which the Λ0 is at rest—we will measure the angles in this rest frame; then
they can always be transformed to another frame if we want.
We begin by looking at the special circumstance in which the proton is emitted
into a small solid angle ∆Ωalong the z-axis (Fig. 17-7). Before the disintegration
we have a Λ0 with its spin “up,” as in part (a) of the ﬁgure. After a short
time—for reasons unknown to this day, except that they are connected with the
weak decays—the Λ0 explodes into a proton and a pion. Suppose the proton goes
up along the +z-axis. Then, from the conservation of momentum, the pion must
go down. Since the proton is a spin one-half particle, its spin must be either “up”
or “down”—there are, in principle, the two possibilities shown in parts (b) and (c)
of the ﬁgure. The conservation of angular momentum, however, requires that the
proton have spin “up.” This is most easily seen from the following argument. A
particle moving along the z-axis cannot contribute any angular momentum about
this axis by virtue of its motion; therefore, only the spins can contribute to Jz.
The spin angular momentum about the z-axis is +ℏ/2 before the disintegration,
17-22","251"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","469","so it must also be +ℏ/2 afterward. We can say that since the pion has no spin,
the proton spin must be “up.”
If you are worried that arguments of this kind may not be valid in quantum
mechanics, we can take a moment to show you that they are. The initial state
(before the disintegration), which we can call | Λ0, spin +z⟩has the property that if
it is rotated about the z-axis by the angle φ, the state vector gets multiplied by the
phase factor eiφ/2. (In the rotated system the state vector is eiφ/2 | Λ0, spin +z⟩.)
That’s what we mean by spin “up” for a spin one-half particle. Since nature’s
behavior doesn’t depend on our choice of axes, the ﬁnal state (the proton plus
pion) must have the same property. We could write the ﬁnal state as, say,
| proton going +z, spin +z; pion going −z⟩.
But we really do not need to specify the pion motion, since in the frame we
have chosen the pion always moves opposite the proton; we can simplify our
description of the ﬁnal state to
| proton going +z, spin +z⟩.
Now what happens to this state vector if we rotate the coordinates about the
z-axis by the angle φ?
Since the proton and pion are moving along the z-axis, their motion isn’t
changed by the rotation. (That’s why we picked this special case; we couldn’t
make the argument otherwise.) Also, nothing happens to the pion, because it
is spin zero. The proton, however, has spin one-half. If its spin is “up” it
will contribute a phase change of eiφ/2 in response to the rotation. (If its spin
were “down” the phase change due to the proton would be e−iφ/2.) But the
phase change with rotation before and after the excitement must be the same if
angular momentum is to be conserved. (And it will be, since there are no outside
inﬂuences in the Hamiltonian.) So the only possibility is that the proton spin
will be “up.” If the proton goes up, its spin must also be “up.”
We conclude, then, that the conservation of angular momentum permits the
process shown in part (b) of Fig. 17-7, but does not permit the process shown in
part (c). Since we know that the disintegration occurs, there is some amplitude for
process (b)—proton going up with spin “up.” We’ll let a stand for the amplitude
that the disintegration occurs in this way in any inﬁnitesimal interval of time.†
† We are now assuming that the machinery of the quantum mechanics is suﬃciently familiar
17-23","434"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","470","BEFORE
(a)
z
Λ0
AFTER
or
(b)
z
p
π−
vp
vπ−
NO
(c)
z
p
π−
vp
vπ−
YES
Fig. 17-8. The decay along the z-axis for a Λ0 with spin “down.”
Now let’s see what would happen if the Λ0 spin were initially “down.” Again
we ask about the decays in which the proton goes up along the z-axis, as shown
in Fig. 17-8. You will appreciate that in this case the proton must have spin
“down” if angular momentum is conserved. Let’s say that the amplitude for such
a disintegration is b.
We can’t say anything more about the two amplitudes a and b. They depend
on the inner machinery of Λ0, and the weak decays, and nobody yet knows how to
calculate them. We’ll have to get them from experiment. But with just these two
amplitudes we can ﬁnd out all we want to know about the angular distribution
of the disintegration. We only have to be careful always to deﬁne completely the
states we are talking about.
We want to know the probability that the proton will go oﬀat the angle θ
with respect to the z-axis (into a small solid angle ∆Ω) as drawn in Fig. 17-6.
to you that we can speak about things in a physical way without taking the time to write down
all the mathematical details. In case what we are saying here is not clear to you, we have put
some of the missing details in a note at the end of the section.
17-24","256"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","471","Let’s put a new z-axis in this direction and call it the z′-axis. We know how to
analyze what happens along this axis. With respect to this new axis, the Λ0 no
longer has its spin “up,” but has a certain amplitude to have its spin “up” and
another amplitude to have its spin “down.” We have already worked these out
in Chapter 6, and again in Chapter 10, Eq. (10.30). The amplitude to be spin
“up” is cos θ/2, and the amplitude to be spin “down” is† −sin θ/2. When the Λ0
(a)
z
Λ0
z
z ′
p
π−
vp
vπ−
θ
sz′= 1
2
Amplitude a cos θ/2
(b)
z
Λ0
z
z ′
p
π−
vp
vπ−
θ
sz′=−1
2
Amplitude −b sin θ/2
Fig. 17-9. Two possible decay states for the Λ0.
† We have chosen to let z′ be in the xz-plane and use the matrix elements for Ry(θ). You
would get the same answer for any other choice.
17-25","166"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","472","spin is “up” along the z′-axis it will emit a proton in the +z′-direction with the
amplitude a. So the amplitude to ﬁnd an “up”-spinning proton coming out along
the z′-direction is
a cos θ
2.
(17.33)
Similarly, the amplitude to ﬁnd a “down”-spinning proton coming along the
positive z′-axis is
−b sin θ
2.
(17.34)
The two processes that these amplitudes refer to are shown in Fig. 17-9.
Let’s now ask the following easy question. If the Λ0 has spin up along the
z-axis, what is the probability that the decay proton will go oﬀat the angle θ?
The two spin states (“up” or “down” along z′) are distinguishable even though we
are not going to look at them. So to get the probability we square the amplitudes
and add. The probability f(θ) of ﬁnding a proton in a small solid angle ∆Ωat θ
is
f(θ) = |a|2 cos2 θ
2 + |b|2 sin2 θ
2.
(17.35)
Remembering that sin2 θ/2 = 1
2(1 −cos θ) and that cos2 θ/2 = 1
2(1 + cos θ), we
can write f(θ) as
f(θ) =
|a|2 + |b|2
2

+
|a|2 −|b|2
2

cos θ.
(17.36)
The angular distribution has the form
f(θ) = β(1 + α cos θ).
(17.37)
The probability has one part that is independent of θ and one part that varies
linearly with cos θ. From measuring the angular distribution we can get α and β,
and therefore, |a| and |b|.
Now there are many other questions we can answer. Are we interested only
in protons with spin “up” along the old z-axis? Each of the terms in (17.33)
and (17.34) will give an amplitude to ﬁnd a proton with spin “up” and with spin
“down” with respect to the z′-axis (+z′ and −z′). Spin “up” with respect to
the old axis | +z⟩can be expressed in terms of the base states | +z′⟩and | −z′⟩.
17-26","322"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","473","We can then combine the two amplitudes (17.33) and (17.34) with the proper
coeﬃcients (cos θ/2 and −sin θ/2) to get the total amplitude

a cos2 θ
2 + b sin2 θ
2

.
Its square is the probability that the proton comes out at the angle θ with its
spin the same as the Λ0 (“up” along the z-axis).
If parity were conserved, we could say one more thing. The disintegration of
Fig. 17-8 is just the reﬂection—in the xy-plane of the disintegration—of Fig. 17-7.†
If parity were conserved, b would have to be equal to a or to −a. Then the
coeﬃcient α of (17.37) would be zero, and the disintegration would be equally
likely to occur in all directions.
The experimental results show, however, that there is an asymmetry in the
disintegration. The measured angular distribution does go as cos θ as we predict—
and not as cos2 θ or any other power. In fact, since the angular distribution
has this form, we can deduce from these measurements that the spin of the Λ0
is 1/2. Also, we see that parity is not conserved. In fact, the coeﬃcient α is
found experimentally to be −0.62 ± 0.05, so b is about twice as large as a. The
lack of symmetry under a reﬂection is quite clear.
You see how much we can get from the conservation of angular momentum.
We will give some more examples in the next chapter.
Parenthetical note. By the amplitude a in this section we mean the amplitude that
the state | proton going +z, spin +z⟩is generated in an inﬁnitesimal time dt from the
state | Λ, spin +z⟩, or, in other words, that
⟨proton going +z, spin +z | H | Λ, spin +z⟩= iℏa,
(17.38)
where H is the Hamiltonian of the world—or, at least, of whatever is responsible for
the Λ-decay. The conservation of angular momentum means that the Hamiltonian must
have the property that
⟨proton going +z, spin −z | H | Λ, spin +z⟩= 0.
(17.39)
By the amplitude b we mean that
⟨proton going +z, spin −z | H | Λ, spin −z⟩= iℏb.
(17.40)
† Remembering that the spin is an axial vector and doesn’t ﬂip over in the reﬂection.
17-27","378"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","474","Conservation of angular momentum implies that
⟨proton going +z, spin +z | H | Λ, spin −z⟩= 0.
(17.41)
If the amplitudes written in (17.33) and (17.34) are not clear, we can express them
more mathematically as follows. By (17.33) we intend the amplitude that the Λ with
spin along +z will disintegrate into a proton moving along the +z′-direction with its
spin also in the +z′-direction, namely the amplitude
⟨proton going +z′, spin +z′ | H | Λ, spin +z⟩.
(17.42)
By the general theorems of quantum mechanics, this amplitude can be written as
X
i
⟨proton going +z′, spin +z′ | H | Λ, i⟩⟨Λ, i | Λ, spin +z⟩,
(17.43)
where the sum is to be taken over the base states | Λ, i⟩of the Λ-particle at rest. Since
the Λ-particle is spin one-half, there are two such base states which can be in any
reference base we wish. If we use for base states spin “up” and spin “down” with respect
to z′ (+z′, −z′), the amplitude of (17.43) is equal to the sum
⟨proton going +z′, spin +z′ | H | Λ, +z′⟩⟨Λ, +z′ | Λ, +z⟩
+ ⟨proton going +z′, spin +z′ | H | Λ, −z′⟩⟨Λ, −z′ | Λ, +z⟩.
(17.44)
The ﬁrst factor of the ﬁrst term is a, and the ﬁrst factor of the second term is zero—
from the deﬁnition of (17.38), and from (17.41), which in turn follows from angular
momentum conservation. The remaining factor ⟨Λ, +z′ | Λ, +z⟩of the ﬁrst term is just
the amplitude that a spin one-half particle which has spin “up” along one axis will
also have spin “up” along an axis tilted at the angle θ, which is cos θ/2—see Table 6-2.
So (17.44) is just a cos θ/2, as we wrote in (17.33). The amplitude of (17.34) follows
from the same kind of arguments for a spin “down” Λ-particle.
17-6 Summary of the rotation matrices
We would like now to bring together in one place the various things we have
learned about the rotations for particles of spin one-half and spin one—so they
will be convenient for future reference. On the next page you will ﬁnd tables
of the two rotation matrices Rz(φ) and Ry(θ) for spin one-half particles, for
spin-one particles, and for photons (spin-one particles with zero rest mass). For
each spin we will give the terms of the matrix ⟨j | R | i⟩for rotations about the
z-axis or the y-axis. They are, of course, exactly equivalent to the amplitudes
17-28","422"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","475","like ⟨+T | 0S⟩we have used in earlier chapters. We mean by Rz(φ) that the state
is projected into a new coordinate system which is rotated through the angle φ
about the z-axis—using always the right-hand rule to deﬁne the positive sense
of the rotation. By Ry(θ) we mean that the reference axes are rotated by the
angle θ about the y-axis. Knowing these two rotations, you can, of course, work
out any arbitrary rotation. As usual, we write the matrix elements so that the
state on the left is a base state of the new (rotated) frame and the state on the
right is a base state of the old (unrotated) frame. You can interpret the entries in
the tables in many ways. For instance, the entry e−iφ/2 in Table 17-1 means that
the matrix element ⟨−| R | −⟩= e−iφ/2. It also means that ˆR | −⟩= e−iφ/2 | −⟩,
or that ⟨−| ˆR = ⟨−| e−iφ/2. It’s all the same thing.
Table 17-1
Rotation matrices for spin one-half
Two states: | +⟩, “up” along the z-axis, m = +1/2
| −⟩, “down” along the z-axis, m = −1/2
Rz(φ)
| +⟩
| −⟩
⟨+ |
e+iφ/2
0
⟨−|
0
e−iφ/2
Ry(θ)
| +⟩
| −⟩
⟨+ |
cos θ/2
sin θ/2
⟨−|
−sin θ/2
cos θ/2
17-29","220"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","476","Table 17-2
Rotation matrices for spin one
Three states: | +⟩, m = +1
| 0 ⟩, m = 0
| −⟩, m = −1
Rz(φ)
| +⟩
| 0 ⟩
| −⟩
⟨+ |
e+iφ
0
0
⟨0 |
0
1
0
⟨−|
0
0
e−iφ
Ry(θ)
| +⟩
| 0 ⟩
| −⟩
⟨+ |
1
2(1 + cos θ)
+ 1
√
2
sin θ
1
2(1 −cos θ)
⟨0 |
−1
√
2
sin θ
cos θ
+ 1
√
2
sin θ
⟨−|
1
2(1 −cos θ)
−1
√
2
sin θ
1
2(1 + cos θ)
Table 17-3
Photons
Two states: | R⟩=
1
√
2
(| x⟩+ i | y⟩), m = +1 (RHC polarized)
| L⟩=
1
√
2
(| x⟩−i | y⟩), m = −1 (LHC polarized)
Rz(φ)
| R⟩
| L⟩
⟨R |
e+iφ
0
⟨L |
0
e−iφ
17-30","150"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","477","18
Angular Momentum
18-1 Electric dipole radiation
In the last chapter we developed the idea of the conservation of angular
momentum in quantum mechanics, and showed how it might be used to predict
the angular distribution of the proton from the disintegration of the Λ-particle. We
want now to give you a number of other, similar, illustrations of the consequences
of momentum conservation in atomic systems. Our ﬁrst example is the radiation
of light from an atom. The conservation of angular momentum (among other
things) will determine the polarization and angular distribution of the emitted
photons.
Suppose we have an atom which is in an excited state of deﬁnite angular
momentum—say with a spin of one—and it makes a transition to a state of
angular momentum zero at a lower energy, emitting a photon. The problem is to
ﬁgure out the angular distribution and polarization of the photons. (This problem
is almost exactly the same as the Λ0 disintegration, except that we have spin-one
instead of spin one-half particles.) Since the upper state of the atom is spin one,
there are three possibilities for its z-component of angular momentum. The value
of m could be +1, or 0, or −1. We will take m = +1 for our example. Once you
see how it goes, you can work out the other cases. We suppose that the atom is
sitting with its angular momentum along the +z-axis—as in Fig. 18-1(a)—and
ask with what amplitude it will emit right circularly polarized light upward along
the z-axis, so that the atom ends up with zero angular momentum—as shown
in part (b) of the ﬁgure. Well, we don’t know the answer to that. But we do
know that right circularly polarized light has one unit of angular momentum
about its direction of propagation. So after the photon is emitted, the situation
would have to be as shown in Fig. 18-1(b)—the atom is left with zero angular
momentum about the z-axis, since we have assumed an atom whose lower state is
spin zero. We will let a stand for the amplitude for such an event. More precisely,
18-1","353"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","478","(a)
BEFORE
z
j = 1
m = 1
ATOM IN
EXCITED
STATE
AMPLITUDE
a
(b)
AFTER
z
j = 0
m = 0
ATOM IN
GROUND
STATE
RHC
PHOTON
Fig. 18-1. An atom with m = +1 emits a RHC photon along the
+z-axis.
we let a be the amplitude to emit a photon into a certain small solid angle ∆Ω,
centered on the z-axis, during a time dt. Notice that the amplitude to emit a
LHC photon in the same direction is zero. The net angular momentum about
the z-axis would be −1 for such a photon and zero for the atom for a total of −1,
which would not conserve angular momentum.
Similarly, if the spin of the atom is initially “down” (−1 along the z-axis),
it can emit only a LHC polarized photon in the direction of the +z-axis, as
shown in Fig. 18-2. We will let b stand for the amplitude for this event—meaning
again the amplitude that the photon goes into a certain solid angle ∆Ω. On the
other hand, if the atom is in the m = 0 state, it cannot emit a photon in the
+z-direction at all, because a photon can have only the angular momentum +1
or −1 along its direction of motion.
Next, we can show that b is related to a. Suppose we perform an inversion of
the situation in Fig. 18-1, which means that we should imagine what the system
would look like if we were to move each part of the system to an equivalent point
18-2","261"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","479","(a)
BEFORE
z
j = 1
m = −1
ATOM IN
EXCITED
STATE
AMPLITUDE
b
(b)
AFTER
z
j = 0
m = 0
ATOM IN
GROUND
STATE
LHC
PHOTON
Fig. 18-2. An atom with m = −1 emits a LHC photon along the
+z-axis.
on the opposite side of the origin. This does not mean that we should reﬂect the
angular momentum vectors, because they are artiﬁcial. We should, rather, invert
the actual character of the motion that would correspond to such an angular
momentum. In Fig. 18-3(a) and (b) we show what the process of Fig. 18-1 looks
like before and after an inversion with respect to the center of the atom. Notice
that the sense of rotation of the atom is unchanged.† In the inverted system of
Fig. 18-3(b) we have an atom with m = +1 emitting a LHC photon downward.
If we now rotate the system of Fig. 18-3(b) by 180◦about the x- or y-axis, it
becomes identical to Fig. 18-2. The combination of the inversion and rotation
turns the second process into the ﬁrst. Using Table 17-2, we see that a rotation
of 180◦about the y-axis just throws an m = −1 state into an m = +1 state,
† When we change x, y, z into −x, −y, −z, you might think that all vectors get reversed.
That is true for polar vectors like displacements and velocities, but not for an axial vector like
angular momentum—or any vector which is derived from a cross product of two polar vectors.
Axial vectors have the same components after an inversion.
18-3","266"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","480","(a)
(b)
Fig. 18-3. If the process of (a) is transformed by an inversion through
the center of the atom, it appears as in (b).
18-4","26"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","481","so the amplitude b must be equal to the amplitude a except for a possible sign
change due to the inversion. The sign change in the inversion will depend on the
parities of the initial and ﬁnal state of the atom.
In atomic processes, parity is conserved, so the parity of the whole system
must be the same before and after the photon emission. What happens will
depend on whether the parities of the initial and ﬁnal states of the atom are even
or odd—the angular distribution of the radiation will be diﬀerent for diﬀerent
cases. We will take the common case of odd parity for the initial state and even
parity for the ﬁnal state; it will give what is called “electric dipole radiation.” (If
the initial and ﬁnal states have the same parity we say there is “magnetic dipole
radiation,” which has the character of the radiation from an oscillating current in
a loop.) If the parity of the initial state is odd, its amplitude reverses its sign in
the inversion which takes the system from (a) to (b) of Fig. 18-3. The ﬁnal state
of the atom has even parity, so its amplitude doesn’t change sign. If the reaction
is going to conserve parity, the amplitude b must be equal to a in magnitude but
of the opposite sign.
We conclude that if the amplitude is a that an m = +1 state will emit a
photon upward, then for the assumed parities of the initial and ﬁnal states the
amplitude that an m = −1 state will emit a LHC photon upward is −a.†
We have all we need to know to ﬁnd the amplitude for a photon to be emitted
at any angle θ with respect to the z-axis. Suppose we have an atom originally
polarized with m = +1. We can resolve this state into +1, 0, and −1 states with
respect to a new z′-axis in the direction of the photon emission. The amplitudes
for these three states are just the ones given in the lower half of Table 17-2. The
amplitude that a RHC photon is emitted in the direction θ is then a times the
amplitude to have m = +1 in that direction, namely,
a⟨+ | Ry(θ) | +⟩= a
2 (1 + cos θ).
(18.1)
The amplitude that a LHC photon is emitted in the same direction is −a times
the amplitude to have m = −1 in the new direction. Using Table 17-2, it is
−a⟨−| Ry(θ) | +⟩= −a
2 (1 −cos θ).
(18.2)
† Some of you may object to the argument we have just made, on the basis that the ﬁnal
states we have been considering do not have a deﬁnite parity. You will ﬁnd in Added Note 2 at
the end of this chapter another demonstration, which you may prefer.
18-5","477"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","482","If you are interested in other polarizations you can ﬁnd out the amplitude for
them from the superposition of these two amplitudes. To get the intensity of any
component as a function of angle, you must, of course, take the absolute square
of the amplitudes.
18-2 Light scattering
Let’s use these results to solve a somewhat more complicated problem—but
also one which is somewhat more real. We suppose that the same atoms are
sitting in their ground state (j = 0), and scatter an incoming beam of light. Let’s
say that the light is going initially in the +z-direction, so that we have photons
coming up to the atom from the −z-direction, as shown in Fig. 18-4(a). We can
consider the scattering of light as a two-step process: The photon is absorbed,
and then is re-emitted. If we start with a RHC photon as in Fig. 18-4(a), and
angular momentum is conserved, the atom will be in an m = +1 state after the
absorption—as shown in Fig. 18-4(b). We call the amplitude for this process c.
The atom can then emit a RHC photon in the direction θ—as in Fig. 18-4(c).
The total amplitude that a RHC photon is scattered in the direction θ is just
c times (18.1). Let’s call this scattering amplitude ⟨R′ | S | R⟩; we have
⟨R′ | S | R⟩= ac
2 (1 + cos θ).
(18.3)
(a)
z
j = 0
m = 0
(b)
z
j = 1
m = 1
(c)
z
j = 0
m = 0
θ
Fig. 18-4. The scattering of light by an atom seen as a two-step process.
18-6","274"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","483","There is also an amplitude that a RHC photon will be absorbed and that a
LHC photon will be emitted. The product of the two amplitudes is the ampli-
tude ⟨L′ | S | R⟩that a RHC photon is scattered as a LHC photon. Using (18.2),
we have
⟨L′ | S | R⟩= −ac
2 (1 −cos θ).
(18.4)
Now let’s ask about what happens if a LHC photon comes in. When it is
absorbed, the atom will go into an m = −1 state. By the same kind of arguments
we used in the preceding section, we can show that this amplitude must be −c.
The amplitude that an atom in the m = −1 state will emit a RHC photon at the
angle θ is a times the amplitude ⟨+ | Ry(θ) | −⟩, which is 1
2(1 −cos θ). So we have
⟨R′ | S | L⟩= −ac
2 (1 −cos θ).
(18.5)
Finally, the amplitude for a LHC photon to be scattered as a LHC photon is
⟨L′ | S | L⟩= ac
2 (1 + cos θ).
(18.6)
(There are two minus signs which cancel.)
If we make a measurement of the scattered intensity for any given combination
of circular polarizations it will be proportional to the square of one of our four
amplitudes. For instance, with an incoming beam of RHC light the intensity of
the RHC light in the scattered radiation will vary as (1 + cos θ)2.
That’s all very well, but suppose we start out with linearly polarized light.
What then? If we have x-polarized light, it can be represented as a superposition
of RHC and LHC light. We write (see Section 11-4)
| x⟩=
1
√
2 (| R⟩+ | L⟩).
(18.7)
Or, if we have y-polarized light, we would have
| y⟩= −i
√
2 (| R⟩−| L⟩).
(18.8)
Now what do you want to know? Do you want the amplitude that an x-polarized
photon will scatter into a RHC photon at the angle θ? You can get it by the
usual rule for combining amplitudes. First, multiply (18.7) by ⟨R′ | S to get
⟨R′ | S | x⟩=
1
√
2 (⟨R′ | S | R⟩+ ⟨R′ | S | L⟩),
(18.9)
18-7","376"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","484","and then use (18.3) and (18.5) for the two amplitudes. You get
⟨R′ | S | x⟩= ac
√
2 cos θ.
(18.10)
If you wanted the amplitude that an x-photon would scatter into a LHC photon,
you would get
⟨L′ | S | x⟩= ac
√
2 cos θ.
(18.11)
Finally, suppose you wanted to know the amplitude that an x-polarized photon
will scatter while keeping its x-polarization. What you want is ⟨x′ | S | x⟩. This
can be written as
⟨x′ | S | x⟩= ⟨x′ | R′⟩⟨R′ | S | x⟩+ ⟨x′ | L′⟩⟨L′ | S | x⟩.
(18.12)
If you then use the relations
| R′⟩=
1
√
2 (| x′⟩+ i | y′⟩),
(18.13)
| L′⟩=
1
√
2 (| x′⟩−i | y′⟩),
(18.14)
it follows that
⟨x′ | R′⟩=
1
√
2,
(18.15)
⟨x′ | L′⟩=
1
√
2.
(18.16)
So you get that
⟨x′ | S | x⟩= ac cos θ.
(18.17)
The answer is that a beam of x-polarized light will be scattered at the direction θ
(in the xz-plane) with an intensity proportional to cos2 θ. If you ask about
y-polarized light, you ﬁnd that
⟨y′ | S | x⟩= 0.
(18.18)
So the scattered light is completely polarized in the x-direction.
Now we notice something interesting. The results (18.17) and (18.18) cor-
respond exactly to the classical theory of light scattering we gave in Vol. I,
18-8","238"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","485","Section 32-5, where we imagined that the electron was bound to the atom by a
linear restoring force—so that it acted like a classical oscillator. Perhaps you are
thinking: “It’s so much easier in the classical theory; if it gives the right answer
why bother with the quantum theory?” For one thing, we have considered so far
only the special—though common—case of an atom with a j = 1 excited state
and a j = 0 ground state. If the excited state had spin two, you would get a
diﬀerent result. Also, there is no reason why the model of an electron attached
to a spring and driven by an oscillating electric ﬁeld should work for a single
photon. But we have found that it does in fact work, and that the polarization
and intensities come out right. So in a certain sense we are bringing the whole
course around to the real truth. Whereas we have, in Vol. I, done the theory of
the index of refraction, and of light scattering, by the classical theory, we have
now shown that the quantum theory gives the same result for the most common
case. In eﬀect we have now done the polarization of sky light, for instance, by
quantum mechanical arguments, which is the only truly legitimate way.
It should be, of course, that all the classical theories which work are supported
ultimately by legitimate quantum arguments. Naturally, those things which we
have spent a great deal of time in explaining to you were selected from just those
parts of classical physics which still maintain validity in quantum mechanics.
You’ll notice that we did not discuss in great detail any model of the atom
which has electrons going around in orbits. That’s because such a model doesn’t
give results which agree with the quantum mechanics. But the electron on a
spring—which is not, in a sense, at all the way an atom “looks”—does work, and
so we used that model for the theory of the index of refraction.
18-3 The annihilation of positronium
We would like next to take an example which is very pretty. It is quite
interesting and, although somewhat complicated, we hope not too much so. Our
example is the system called positronium, which is an “atom” made up of an
electron and a positron—a bound state of an e+ and an e−. It is like a hydrogen
atom, except that a positron replaces the proton. This object has—like the
hydrogen atom—many states. Also like the hydrogen, the ground state is split
into a “hyperﬁne structure” by the interaction of the magnetic moments. The
spins of the electron and positron are each one-half, and they can be either
parallel or antiparallel to any given axis. (In the ground state there is no other
angular momentum due to orbital motion.) So there are four states: three are
18-9","479"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","486","the substates of a spin-one system, all with the same energy; and one is a state
of spin zero with a diﬀerent energy. The energy splitting is, however, much larger
than the 1420 megacycles of hydrogen because the positron magnetic moment is
so much stronger—1000 times stronger—than the proton moment.
The most important diﬀerence, however, is that positronium cannot last
forever. The positron is the antiparticle of the electron; they can annihilate each
other. The two particles disappear completely—converting their rest energy into
radiation, which appears as γ-rays (photons). In the disintegration, two particles
with a ﬁnite rest mass go into two or more objects which have zero rest mass.†
We begin by analyzing the disintegration of the spin-zero state of the positro-
nium. It disintegrates into two γ-rays with a lifetime of about 10−10 second.
Initially, we have a positron and an electron close together and with spins an-
tiparallel, making the positronium system. After the disintegration there are two
photons going out with equal and opposite momenta (Fig. 18-5). The momenta
must be equal and opposite, because the total momentum after the disintegration
(a)
BEFORE
POSITRONIUM
e+e−
(b)
AFTER
Fig. 18-5. The two-photon annihilation of positronium.
† In the deeper understanding of the world today, we do not have an easy way to distinguish
whether the energy of a photon is less “matter” than the energy of an electron, because as you
remember all the particles behave very similarly. The only distinction is that the photon has
zero rest mass.
18-10","253"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","487","must be zero, as it was before, if we are taking the case of annihilation at rest. If
the positronium is not at rest, we can ride with it, solve the problem, and then
transform everything back to the lab system. (See, we can do anything now; we
have all the tools.)
First, we note that the angular distribution is not very interesting. Since
the initial state has spin zero, it has no special axis—it is symmetric under all
rotations. The ﬁnal state must then also be symmetric under all rotations. That
means that all angles for the disintegration are equally likely—the amplitude is
the same for a photon to go in any direction. Of course, once we ﬁnd one of the
photons in some direction the other must be opposite.
The only remaining question, which we now want to look at, is about the
polarization of the photons. Let’s call the directions of motion of the two photons
the plus and minus z-axes. We can use any representations we want for the
polarization states of the photons; we will choose for our description right and
left circular polarization—always with respect to the directions of motion.† Right
away, we can see that if the photon going upward is RHC, then angular momentum
will be conserved if the downward going photon is also RHC. Each will carry
+1 unit of angular momentum with respect to its momentum direction, which
means plus and minus one unit about the z-axis. The total will be zero, and
the angular momentum after the disintegration will be the same as before. See
Fig. 18-6.
The same arguments show that if the upward going photon is RHC, the
downward cannot be LHC. Then the ﬁnal state would have two units of angular
momentum. This is not permitted if the initial state has spin zero. Note that
such a ﬁnal state is also not possible for the other positronium ground state of
spin one, because it can have a maximum of one unit of angular momentum in
any direction.
Now we want to show that two-photon annihilation is not possible at all from
the spin-one state. You might think that if we took the j = 1, m = 0 state—which
has zero angular momentum about the z-axis—it should be like the spin-zero
state, and could disintegrate into two RHC photons. Certainly, the disintegration
† Note that we always analyze the angular momentum about the direction of motion of
the particle. If we were to ask about the angular momentum about any other axis, we would
have to worry about the possibility of “orbital” angular momentum—from a p × r term. For
instance, we can’t say that the photons leave exactly from the center of the positronium. They
could leave like two things shot out from the rim of a spinning wheel. We don’t have to worry
about such possibilities when we take our axis along the direction of motion.
18-11","493"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","488","z
POSITRONIUM
e+e−
j = 0
m = 0
z
RHC
m = +1
RHC
m = −1
Fig. 18-6. One possibility for positronium annihilation along the z-axis.
(a)
e+e−
j = 1
m = 0
(b)
e+e−
j = 1
m = 0
Fig. 18-7. For the j = 1 state of positronium, the process (a) and its
180◦rotation about y (b) are exactly the same.
18-12","68"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","489","sketched in Fig. 18-7(a) conserves angular momentum about the z-axis. But
now look what happens if we rotate this system around the y-axis by 180◦; we
get the picture shown in Fig. 18-7(b). It is exactly the same as in part (a) of
the ﬁgure. All we have done is interchange the two photons. Now photons are
Bose particles; if we interchange them, the amplitude has the same sign, so the
amplitude for the disintegration in part (b) must be the same as in part (a).
But we have assumed that the initial object is spin one. And when we rotate
a spin-one object in a state with m = 0 by 180◦about the y-axis, its amplitudes
change sign (see Table 17-2 for θ = π). So the amplitudes for (a) and (b) in
Fig. 18-7 should have opposite signs; the spin-one state cannot disintegrate into
two photons.
When positronium is formed you would expect it to end up in the spin-zero
state 1/4 of the time and in the spin-one state (with m = −1, 0, or +1) 3/4 of
the time. So 1/4 of the time you would get two-photon annihilations. The
other 3/4 of the time there can be no two-photon annihilations. There is still
an annihilation, but it has to go with three photons. It is harder for it to do
that and the lifetime is 1000 times longer—about 10−7 second. This is what
is observed experimentally. We will not go into any more of the details of the
spin-one annihilation.
So far we have that if we only worry about angular momentum, the spin-zero
state of the positronium can go into two RHC photons. There is also another
possibility: it can go into two LHC photons as shown in Fig. 18-8. The next
question is, what is the relation between the amplitudes for these two possible
decay modes? We can ﬁnd out from the conservation of parity.
To do that, however, we need to know the parity of the positronium. Now
theoretical physicists have shown in a way that is not easy to explain that the
parity of the electron and the positron—its antiparticle—must be opposite, so
that the spin-zero ground state of positronium must be odd. We will just assume
that it is odd, and since we will get agreement with experiment, we can take that
as suﬃcient proof.
Let’s see then what happens if we make an inversion of the process in Fig. 18-6.
When we do that, the two photons reverse directions and polarizations. The
inverted picture looks just like Fig. 18-8.
Assuming that the parity of the
positronium is odd, the amplitudes for the two processes in Figs. 18-6 and 18-8
must have the opposite sign. Let’s let | R1R2⟩stand for the ﬁnal state of Fig. 18-6
in which both photons are RHC, and let | L1L2⟩stand for the ﬁnal state of
Fig. 18-8, in which both photons are LHC.
The true ﬁnal state—let’s call
18-13","495"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","490","e+e−
j = 0
m = 0
LHC
LHC
Fig. 18-8. Another possible process for positronium annihilation.
it | F⟩—must be
| F⟩= | R1R2⟩−| L1L2⟩.
(18.19)
Then an inversion changes the R’s into L’s and gives the state
P | F⟩= | L1L2⟩−| R1R2⟩= −| F⟩,
(18.20)
which is the negative of (18.19). So the ﬁnal state | F⟩has negative parity, which
is the same as the initial spin-zero state of the positronium. This is the only
ﬁnal state that conserves both angular momentum and parity. There is some
amplitude that the disintegration into this state will occur, which we don’t need
to worry about now, however, since we are only interested in questions about the
polarization.
What does the ﬁnal state of (18.19) mean physically? One thing it means
is the following: If we observe the two photons in two detectors which can be
set to count separately the RHC or LHC photons, we will always see two RHC
photons together, or two LHC photons together. That is, if you stand on one
side of the positronium and someone else stands on the opposite side, you can
measure the polarization and tell the other guy what polarization he will get.
You have a 50-50 chance of catching a RHC photon or a LHC photon; whichever
one you get, you can predict that he will get the same.
18-14","230"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","491","Since there is a 50-50 chance for RHC or LHC polarization, it sounds as
though it might be like linear polarization. Let’s ask what happens if we observe
the photon in counters that accept only linearly polarized light. For γ-rays it
is not as easy to measure the polarization as it is for light; there is no polarizer
which works well for such short wavelengths. But let’s imagine that there is, to
make the discussion easier. Suppose that you have a counter that only accepts
light with x-polarization, and that there is a guy on the other side that also
looks for linear polarized light with, say, y-polarization. What is the chance you
will pick up the two photons from an annihilation? What we need to ask is the
amplitude that | F⟩will be in the state | x1y2⟩. In other words, we want the
amplitude
⟨x1y2 | F⟩,
which is, of course, just
⟨x1y2 | R1R2⟩−⟨x1y2 | L1L2⟩.
(18.21)
Now although we are working with two-particle amplitudes for the two photons,
we can handle them just as we did the single particle amplitudes, since each particle
acts independently of the other. That means that the amplitude ⟨x1y2 | R1R2⟩is
just the product of the two independent amplitudes ⟨x1 | R1⟩and ⟨y2 | R2⟩. Using
Table 17-3, these two amplitudes are 1/
√
2 and i/
√
2—so
⟨x1y2 | R1R2⟩= + i
2.
Similarly, we ﬁnd that
⟨x1y2 | L1L2⟩= −i
2.
Subtracting these two amplitudes according to (18.21), we get that
⟨x1y2 | F⟩= +i.
(18.22)
So there is a unit probability† that if you get a photon in your x-polarized
detector, the other guy will get a photon in his y-polarized detector.
† We have not normalized our amplitudes, or multiplied them by the amplitude for the
disintegration into any particular ﬁnal state, but we can see that this result is correct because
we get zero probability when we look at the other alternative—see Eq. (18.23).
18-15","330"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","492","Now suppose that the other guy sets his counter for x-polarization the same
as yours. He would never get a count when you got one. If you work it through,
you will ﬁnd that
⟨x1x2 | F⟩= 0.
(18.23)
It will, naturally, also work out that if you set your counter for y-polarization he
will get coincident counts only if he is set for x-polarization.
Now this all leads to an interesting situation. Suppose you were to set up
something like a piece of calcite which separated the photons into x-polarized and
y-polarized beams, and put a counter in each beam. Let’s call one the x-counter
and the other the y-counter. If the guy on the other side does the same thing,
you can always tell him which beam his photon is going to go into. Whenever
you and he get simultaneous counts, you can see which of your detectors caught
the photon and then tell him which of his counters had a photon. Let’s say that
in a certain disintegration you ﬁnd that a photon went into your x-counter; you
can tell him that he must have had a count in his y-counter.
Now many people who learn quantum mechanics in the usual (old-fashioned)
way ﬁnd this disturbing. They would like to think that once the photons are
emitted it goes along as a wave with a deﬁnite character. They would think
that since “any given photon” has some “amplitude” to be x-polarized or to be
y-polarized, there should be some chance of picking it up in either the x- or y-
counter and that this chance shouldn’t depend on what some other person ﬁnds
out about a completely diﬀerent photon. They argue that “someone else making
a measurement shouldn’t be able to change the probability that I will ﬁnd some-
thing.” Our quantum mechanics says, however, that by making a measurement
on photon number one, you can predict precisely what the polarization of photon
number two is going to be when it is detected. This point was never accepted
by Einstein, and he worried about it a great deal—it became known as the
“Einstein-Podolsky-Rosen paradox.” But when the situation is described as we
have done it here, there doesn’t seem to be any paradox at all; it comes out quite
naturally that what is measured in one place is correlated with what is measured
somewhere else. The argument that the result is paradoxical runs something like
this:
(1) If you have a counter which tells you whether your photon is RHC or LHC,
you can predict exactly what kind of a photon (RHC or LHC) he will ﬁnd.
(2) The photons he receives must, therefore, each be purely RHC or purely
LHC, some of one kind and some of the other.
18-16","464"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","493","(3) Surely you cannot alter the physical nature of his photons by changing
the kind of observation you make on your photons.
No matter what
measurements you make on yours, his must still be either RHC or LHC.
(4) Now suppose he changes his apparatus to split his photons into two linearly
polarized beams with a piece of calcite so that all of his photons go either
into an x-polarized beam or into a y-polarized beam. There is absolutely
no way, according to quantum mechanics, to tell into which beam any
particular RHC photon will go. There is a 50% probability it will go into
the x-beam and a 50% probability it will go into the y-beam. And the
same goes for a LHC photon.
(5) Since each photon is RHC or LHC—according to (2) and (3)—each one
must have a 50-50 chance of going into the x-beam or the y-beam and there
is no way to predict which way it will go.
(6) Yet the theory predicts that if you see your photon go through an x-polarizer
you can predict with certainty that his photon will go into his y-polarized
beam. This is in contradiction to (5) so there is a paradox.
Nature apparently doesn’t see the “paradox,” however, because experiment
shows that the prediction in (6) is, in fact, true. We have already discussed the
key to this “paradox” in our very ﬁrst lecture on quantum mechanical behavior
in Chapter 37, Vol. I.† In the argument above, steps (1), (2), (4), and (6) are
all correct, but (3), and its consequence (5), are wrong; they are not a true
description of nature. Argument (3) says that by your measurement (seeing
a RHC or a LHC photon) you can determine which of two alternative events
occurs for him (seeing a RHC or a LHC photon), and that even if you do not
make your measurement you can still say that his event will occur either by one
alternative or the other. But it was precisely the point of Chapter 37, Vol. I, to
point out right at the beginning that this is not so in Nature. Her way requires a
description in terms of interfering amplitudes, one amplitude for each alternative.
A measurement of which alternative actually occurs destroys the interference,
but if a measurement is not made you cannot still say that “one alternative or
the other is still occurring.”
If you could determine for each one of your photons whether it was RHC
and LHC, and also whether it was x-polarized (all for the same photon) there
would indeed be a paradox. But you cannot do that—it is an example of the
uncertainty principle.
† See also Chapter 1 of the present volume.
18-17","457"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","494","Do you still think there is a “paradox”? Make sure that it is, in fact, a
paradox about the behavior of Nature, by setting up an imaginary experiment
for which the theory of quantum mechanics would predict inconsistent results
via two diﬀerent arguments. Otherwise the “paradox” is only a conﬂict between
reality and your feeling of what reality “ought to be.”
Do you think that it is not a “paradox,” but that it is still very peculiar? On
that we can all agree. It is what makes physics fascinating.
18-4 Rotation matrix for any spin
By now you can see, we hope, how important the idea of the angular mo-
mentum is in understanding atomic processes. So far, we have considered only
systems with spins—or “total angular momentum”—of zero, one-half, or one.
There are, of course, atomic systems with higher angular momenta. For analyzing
such systems we would need to have tables of rotation amplitudes like those in
Section 17-6. That is, we would need the matrix of amplitudes for spin 3
2, 2, 5
2,
3, etc. Although we will not work out these tables in detail, we would like to
show you how it is done, so that you can do it if you ever need to.
As we have seen earlier, any system which has the spin or “total angular
momentum” j can exist in any one of (2j + 1) states for which the z-component
of angular momentum can have any one of the discrete values in the sequence
j, j −1, j −2, . . . , −(j −1), −j (all in units of ℏ). Calling the z-component
of angular momentum of any particular state mℏ, we can deﬁne a particular
angular momentum state by giving the numerical values of the two “angular
momentum quantum numbers” j and m. We can indicate such a state by the
state vector | j, m⟩. In the case of a spin one-half particle, the two states are then
| 1
2, 1
2⟩and | 1
2, −1
2⟩; or for a spin-one system, the states would be written in this
notation as | 1, +1⟩, | 1, 0⟩, | 1, −1⟩. A spin-zero particle has, of course, only the
one state | 0, 0⟩.
Now we want to know what happens when we project the general state | j, m⟩
into a representation with respect to a rotated set of axes. First, we know that j
is a number which characterizes the system, so it doesn’t change. If we rotate the
axes, all we do is get a mixture of the various m-values for the same j. In general,
there will be some amplitude that in the rotated frame the system will be in the
state | j, m′⟩, where m′ gives the new z-component of angular momentum. So
what we want are all the matrix elements ⟨j, m′ | R | j, m⟩for various rotations.
We already know what happens if we rotate by an angle φ about the z-axis. The
18-18","505"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","495","new state is just the old one multiplied by eimφ—it still has the same m-value.
We can write this by
Rz(φ) | j, m⟩= eimφ | j, m⟩.
(18.24)
Or, if you prefer,
⟨j, m′ | Rz(φ) | j, m⟩= δm,m′eimφ
(18.25)
(where δm,m′ is 1 if m′ = m, or zero otherwise).
For a rotation about any other axis there will be a mixing of the various m-
states. We could, of course, try to work out the matrix elements for an arbitrary
rotation described by the Euler angles β, α, and γ. But it is easier to remember
that the most general such rotation can be made up of the three rotations Rz(γ),
Ry(α), Rz(β); so if we know the matrix elements for a rotation about the y-axis,
we will have all we need.
How can we ﬁnd the rotation matrix for a rotation by the angle θ about the
y-axis for a particle of spin j? We can’t tell you how to do it in a basic way
(with what we have had). We did it for spin one-half by a complicated symmetry
argument. We then did it for spin one by taking the special case of a spin-one
system which was made up of two spin one-half particles. If you will go along
with us and accept the fact that in the general case the answers depend only
on the spin j, and are independent of how the inner guts of the object of spin j
are put together, we can extend the spin-one argument to an arbitrary spin. We
can, for example, cook up an artiﬁcial system of spin 3
2 out of three spin one-half
objects. We can even avoid complications by imagining that they are all distinct
particles—like a proton, an electron, and a muon. By transforming each spin
one-half object, we can see what happens to the whole system—remembering
that the three amplitudes are multiplied for the combined state. Let’s see how it
goes in this case.
Suppose we take the three spin one-half objects all with spins “up”; we can
indicate this state by | + + +⟩. If we look at this system in a frame rotated about
the z-axis by the angle φ, each plus stays a plus, but gets multiplied by eiφ/2.
We have three such factors, so
Rz(φ) | + + +⟩= ei(3φ/2) | + + +⟩.
(18.26)
Evidently the state | + + +⟩is just what we mean by the m = + 3
2 state, or the
state | 3
2, + 3
2⟩.
If we now rotate this system about the y-axis, each of the spin one-half objects
will have some amplitude to be plus or to be minus, so the system will now
18-19","461"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","496","be a mixture of the eight possible combinations | + + +⟩, | + + −⟩, | + −+⟩,
| −+ +⟩, | + −−⟩, | −+ −⟩, | −−+⟩, or | −−−⟩. It is clear, however, that
these can be broken up into four sets, each set corresponding to a particular
value of m. First, we have | + + +⟩, for which m = 3
2. Then there are the three
states | + + −⟩, | + −+⟩, and | −+ +⟩—each with two plusses and one minus.
Since each spin one-half object has the same chance of coming out minus under
the rotation, the amounts of each of these three combinations should be equal.
So let’s take the combination
1
√
3 {| + + −⟩+ | + −+⟩+ | −+ +⟩}
(18.27)
with the factor 1/
√
3 put in to normalize the state. If we rotate this state about
the z-axis, we get a factor eiφ/2 for each plus, and e−iφ/2 for each minus. Each
term in (18.27) is multiplied by eiφ/2, so there is the common factor eiφ/2. This
state satisﬁes our idea of an m = + 1
2 state; we can conclude that
1
√
3 {| + + −⟩+ | + −+⟩+ | −+ +⟩} = | 3
2, + 1
2⟩.
(18.28)
Similarly, we can write
1
√
3 {| + −−⟩+ | −+ −⟩+ | −−+⟩} = | 3
2, −1
2⟩,
(18.29)
which corresponds to a state with m = −1
2.
Notice that we take only the
symmetric combinations—we do not take any combinations with minus signs.
They would correspond to states of the same m but a diﬀerent j. (It’s just like the
spin-one case, where we found that (1/
√
2){| + −⟩+ | −+⟩} was the state | 1, 0⟩,
but the state (1/
√
2){| + −⟩−| −+⟩} was the state | 0, 0⟩.) Finally, we would
have that
| 3
2, −3
2⟩= | −−−⟩.
(18.30)
We summarize our four states in Table 18-1.
Now all we have to do is take each state and rotate it about the y-axis and
see how much of the other states it gives—using our known rotation matrix for
the spin one-half particles. We can proceed in exactly the same way we did for
the spin-one case in Section 12-6. (It just takes a little more algebra.) We will
follow directly the ideas of Chapter 12, so we won’t repeat all the explanations
18-20","419"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","497","Table 18-1
| + + +⟩
= | 3
2, + 3
2⟩
1
√
3
{| + + −⟩+ | + −+⟩+ | −+ +⟩} = | 3
2, + 1
2⟩
1
√
3
{| + −−⟩+ | −+ −⟩+ | −−+⟩} = | 3
2, −1
2⟩
| −−−⟩
= | 3
2, −3
2⟩
in detail. The states in the system S will be labelled | 3
2, + 3
2, S⟩= | + + +⟩,
| 3
2, + 1
2, S⟩= (1/
√
3){| + + −⟩+| + −+⟩+| −+ +⟩}, and so on. The T-system
will be one rotated about the y-axis of S by the angle θ. States in T will be
labelled | 3
2, + 3
2, T⟩, | 3
2, + 1
2, T⟩, and so on. Of course, | 3
2, + 3
2, T⟩is the same
as | +′ +′ +′⟩, the primes referring always to the T-system. Similarly, | 3
2, + 1
2, T⟩
will be equal to (1/
√
3){| +′ +′ −′⟩+| +′ −′ +′⟩+| −′ +′ +′⟩}, and so on. Each
| +′⟩state in the T-frame comes from both the | +⟩and | −⟩states in S via the
matrix elements of Table 12-4.
When we have three spin one-half particles, Eq. (12.47) gets replaced by
| + + +⟩= a3 | +′ +′ +′⟩+ a2b{| +′ +′ −′⟩+ | +′ −′ +′⟩+ | −′ +′ +′⟩}
+ ab2{| +′ −′ −′⟩+ | −′ +′ −′⟩+ | −′ −′ +′⟩}
+ b3 | −′ −′ −′⟩.
(18.31)
Using the transformation of Table 12-4, we get instead of (12.48) the equation
| 3
2, + 3
2, S⟩= a3 | 3
2, + 3
2, T⟩+
√
3 a2b | 3
2, + 1
2, T⟩
+
√
3 ab2 | 3
2, −1
2, T⟩+ b3 | 3
2, −3
2, T⟩.
(18.32)
This already gives us several of our matrix elements ⟨jT | iS⟩. To get the expression
for | 3
2, + 1
2, S⟩we begin with the transformation of a state with two “+” and
18-21","356"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","498","one “−” pieces. For instance,
| + + −⟩= a2c | +′ +′ +′⟩+ a2d | +′ +′ −′⟩+ abc | +′ −′ +′⟩
+ bac | −′ +′ +′⟩+ abd | +′ −′ −′⟩+ bad | −′ +′ −′⟩
+ b2c | −′ −′ +′⟩+ b2d | −′ −′ −′⟩.
(18.33)
Adding two similar expressions for | + −+⟩and | −+ +⟩and dividing by
√
3,
we ﬁnd
| 3
2, + 1
2, S⟩=
√
3 a2c | 3
2, + 3
2, T⟩
+ (a2d + 2abc) | 3
2, + 1
2, T⟩
+ (2bad + b2c) | 3
2, −1
2, T⟩
+
√
3 b2d | 3
2, −3
2, T⟩.
(18.34)
Continuing the process we ﬁnd all the elements ⟨jT | iS⟩of the transformation
matrix as given in Table 18-2. The ﬁrst column comes from Eq. (18.32); the
second from (18.34). The last two columns were worked out in the same way.
Table 18-2
Rotation matrix for a spin 3
2 particle
(The coeﬃcients a, b, c, and d are given in Table 12-4.)
⟨jT | iS⟩
| 3
2, + 3
2, S⟩
| 3
2, + 1
2, S⟩
| 3
2, −1
2, S⟩
| 3
2, −3
2, S⟩
⟨3
2, + 3
2, T |
a3
√
3 a2c
√
3 ac2
c3
⟨3
2, + 1
2, T |
√
3 a2b
a2d + 2abc
c2b + 2dac
√
3 c2d
⟨3
2, −1
2, T |
√
3 ab2
2bad + b2c
2cdb + d2a
√
3 cd2
⟨3
2, −3
2, T |
b3
√
3 b2d
√
3 bd2
d3
Now suppose the T-frame were rotated with respect to S by the angle θ about
their y-axes. Then a, b, c, and d have the values [see (12.54)] a = d = cos θ/2,
and c = −b = sin θ/2. Using these values in Table 18-2 we get the forms which
correspond to the second part of Table 17-2, but now for a spin 3
2 system.
18-22","345"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","499","The arguments we have just gone through are readily generalized to a system
of any spin j. The states | j, m⟩can be put together from 2j particles, each
of spin one-half. (There are j + m of them in the | +⟩state and j −m in the
| −⟩state.) Sums are taken over all the possible ways this can be done, and the
state is normalized by multiplying by a suitable constant. Those of you who are
mathematically inclined may be able to show that the following result comes
out†:
⟨j, m′ | Ry(θ) | j, m⟩= [(j + m)!(j −m)!(j + m′)!(j −m′)!]1/2
×
X
k
(−1)k+m−m′(cos θ/2)2j+m′−m−2k(sin θ/2)m−m′+2k
(m −m′ + k)!(j + m′ −k)!(j −m −k)!k!
,
(18.35)
where k is to go over all values which give terms ≥0 in all the factorials.
This is quite a messy formula, but with it you can check Table 17-2 for j = 1
and prepare tables of your own for larger j. Several special matrix elements are
of extra importance and have been given special names. For example the matrix
elements for m = m′ = 0 and integral j are known as the Legendre polynomials
and are called Pj(cos θ):
⟨j, 0 | Ry(θ) | j, 0⟩= Pj(cos θ).
(18.36)
The ﬁrst few of these polynomials are:
P0(cos θ) = 1,
(18.37)
P1(cos θ) = cos θ,
(18.38)
P2(cos θ) = 1
2(3 cos2 θ −1),
(18.39)
P3(cos θ) = 1
2(5 cos3 θ −3 cos θ).
(18.40)
18-5 Measuring a nuclear spin
We would like to show you one example of the application of the coeﬃcients
we have just described. It has to do with a recent, interesting experiment which
you will now be able to understand. Some physicists wanted to ﬁnd out the spin
of a certain excited state of the Ne20 nucleus. To do this, they bombarded a
† If you want details, they are given in an appendix to this chapter.
18-23","330"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","500","carbon target with a beam of accelerated carbon ions, and produced the desired
excited state of Ne20—called Ne20∗—in the reaction
C12 + C12 →Ne20∗+ α1,
where α1 is the α-particle, or He4. Several of the excited states of Ne20 produced
this way are unstable and disintegrate in the reaction
Ne20∗→O16 + α2.
So experimentally there are two α-particles which come out of the reaction. We
call them α1 and α2; since they come oﬀwith diﬀerent energies, they can be
distinguished from each other. Also, by picking a particular energy for α1 we
can pick out any particular excited state of the Ne20.
C12 BEAM
16 MeV
CARBON FOIL
30 µg/cm2
SILICON JUNCTION
DETECTORS
α1
α2
Fig. 18-9. Experimental arrangement for determining the spin of
certain states of Ne20.
The experiment was set up as shown in Fig. 18-9. A beam of 16-MeV carbon
ions was directed onto a thin foil of carbon. The ﬁrst α-particle was counted in
a silicon diﬀused junction detector marked α1—set to accept α-particles of the
proper energy moving in the forward direction (with respect to the incident C12
beam). The second α-particle was picked up in the counter α2 at the angle θ
with respect to α1. The counting rate of coincidence signals from α1 and α2 were
measured as a function of the angle θ.
The idea of the experiment is the following. First, you need to know that
the spins of C12, O16, and the α-particle are all zero. If we call the direction of
18-24","253"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","501","motion of the initial C12 the +z-direction, then we know that the Ne20∗must
have zero angular momentum about the z-axis. None of the other particles has
any spin; the C12 arrives along the z-axis and the α1 leaves along the z-axis so
they can’t have any angular momentum about it. So whatever the spin j of the
Ne20∗is, we know that it is in the state | j, 0⟩. Now what will happen when the
Ne20∗disintegrates into an O16 and the second α-particle? Well, the α-particle is
picked up in the counter α2 and to conserve momentum the O16 must go oﬀin the
opposite direction.† About the new axis through α2, there can be no component
of angular momentum. The ﬁnal state has zero angular momentum about the
new axis, so the Ne20∗can disintegrate this way only if it has some amplitude to
have m′ equal to zero, where m′ is the quantum number of the component of
angular momentum about the new axis. In fact, the probability of observing α2
at the angle θ is just the square of the amplitude (or matrix element)
⟨j, 0 | Ry(θ) | j, 0⟩.
(18.41)
To ﬁnd the spin of the Ne20∗state in question, the intensity of the second
α-particle was plotted as a function of angle and compared with the theoret-
ical curves for various values of j. As we said in the last section, the ampli-
tudes ⟨j, 0 | Ry(θ) | j, 0⟩are just the functions Pj(cos θ). So the possible angular
distributions are curves of [Pj(cos θ)]2. The experimental results are shown in
Fig. 18-10 for two of the excited states. You can see that the angular distribution
for the 5.80-MeV state ﬁts very well the curve for [P1(cos θ)]2, and so it must be
a spin-one state. The data for the 5.63-MeV state, on the other hand, are quite
diﬀerent; they ﬁt the curve [P3(cos θ)]2. The state has a spin of 3.
From this experiment we have been able to ﬁnd out the angular momentum
of two of the excited states of Ne20∗. This information can then be used for
trying to understand what the conﬁguration of protons and neutrons is inside
this nucleus—one more piece of information about the mysterious nuclear forces.
18-6 Composition of angular momentum
When we studied the hyperﬁne structure of the hydrogen atom in Chapter 12
we had to work out the internal states of a system composed of two particles—the
electron and the proton—each with a spin of one-half. We found that the four
† We can neglect the recoil given to the Ne20∗in the ﬁrst collision. Or better still, we can
calculate what it is and make a correction for it.
18-25","453"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","502","0.02
0.04
0.06
[COINCIDENCE/DIRECT]C.M. PER STERADIAN
20
40
60
80
100
120
140
160
CENTER-OF-MASS ANGLE IN DEGREES
0.36 ×
7
4π [P3(cos θ)]2
5.63 Mev STATE
J = 3
0.02
0.04
0.06
0.08
0.10
0.12
0.61 ×
3
4π [P1(cos θ)]2
5.80 Mev STATE
J = 1
Fig. 18-10. Experimental results for the angular distribution of the
α-particles from two excited states of Ne20 produced in the setup of
Fig. 18-9. [From J. A. Kuehner, Physical Review, Vol. 125, p. 1650,
1962.]
possible spin states of such a system could be put together into two groups—a
group with one energy that looked to the external world like a spin-one particle,
and one remaining state that behaved like a particle of zero spin. That is, putting
together two spin one-half particles we can form a system whose “total spin”
is one, or zero. In this section we want to discuss in more general terms the
spin states of a system which is made up of two particles of arbitrary spin. It is
another important problem about angular momentum in quantum mechanical
systems.
Let’s ﬁrst rewrite the results of Chapter 12 for the hydrogen atom in a form
that will be easier to extend to the more general case. We began with two
particles which we will now call particle a (the electron) and particle b (the
proton).
Particle a had the spin ja (=
1
2), and its z-component of angular
momentum ma could have one of several values (actually 2, namely ma = + 1
2
or ma = −1
2). Similarly, the spin state of particle b is described by its spin jb
and its z-component of angular momentum mb. Various combinations of the spin
18-26","288"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","503","states of the two particles could be formed. For instance, we could have particle a
with ma = 1
2 and particle b with mb = −1
2, to make a state | a, + 1
2; b, −1
2⟩. In
general, the combined states formed a system whose “system spin,” or “total spin,”
or “total angular momentum” J could be 1, or 0. And the system could have
a z-component of angular momentum M, which was +1, 0, or −1 when J = 1,
or 0 when J = 0. In this new language we can rewrite the formulas in (12.41)
and (12.42) as shown in Table 18-3.
Table 18-3
Composition of angular momenta for two
spin 1
2 particles (ja = 1
2, jb = 1
2)
| J = 1, M = +1⟩= | a, + 1
2; b, + 1
2⟩
| J = 1, M =
0⟩=
1
√
2
{| a, + 1
2; b, −1
2⟩+ | a, −1
2; b, + 1
2⟩}
| J = 1, M = −1⟩= | a, −1
2; b, −1
2⟩
| J = 0, M =
0⟩=
1
√
2
{| a, + 1
2; b, −1
2⟩−| a, −1
2; b, + 1
2⟩}
In the table the left-hand column describes the compound state in terms of
its total angular momentum J and the z-component M. The right-hand column
shows how these states are made up in terms of the m-values of the two particles
a and b.
We want now to generalize this result to states made up of two objects a
and b of arbitrary spins ja and jb. We start by considering an example for
which ja = 1
2 and jb = 1, namely, the deuterium atom in which particle a is
an electron (e) and particle b is the nucleus—a deuteron (d). We have then
that ja = je =
1
2.
The deuteron is formed of one proton and one neutron
in a state whose total spin is one, so jb = jd = 1. We want to discuss the
hyperﬁne states of deuterium—just the way we did for hydrogen. Since the
deuteron has three possible states mb = md = +1, 0, −1, and the electron has
two, ma = me = + 1
2, −1
2, there are six possible states as follows (using the
18-27","400"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","504","notation | e, me; d, md⟩:
| e, + 1
2; d, +1⟩,
| e, + 1
2; d, 0⟩; | e, −1
2; d, +1⟩,
| e, + 1
2; d, −1⟩; | e, −1
2; d, 0⟩,
| e, −1
2; d, −1⟩.
(18.42)
You will notice that we have grouped the states according to the values of the
sum of me and md—arranged in descending order.
Now we ask: What happens to these states if we project into a diﬀerent
coordinate system? If the new system is just rotated about the z-axis by the
angle φ, then the state | e, me; d, md⟩gets multiplied by
eimeφeimdφ = ei(me+md)φ.
(18.43)
(The state may be thought of as the product | e, me⟩| d, md⟩, and each state vector
contributes independently its own exponential factor.) The factor (18.43) is of the
form eiMφ, so the state | e, me; d, md⟩has a z-component of angular momentum
equal to
M = me + md.
(18.44)
The z-component of the total angular momentum is the sum of the z-components
of angular momentum of the parts.
In the list of (18.42), therefore, the state in the top line has M = + 3
2, the
two in the second line have M = + 1
2, the next two have M = −1
2, and the last
state has M = −3
2. We see immediately one possibility for the spin J of the
combined state (the total angular momentum) must be 3
2, and this will require
four states with M = + 3
2, + 1
2, −1
2, and −3
2.
There is only one candidate for M = + 3
2, so we know already that
| J = 3
2, M = + 3
2⟩= | e, + 1
2; d, +1⟩.
(18.45)
But what is the state | J = 3
2, M = + 1
2⟩? We have two candidates in the second
line of (18.42), and, in fact, any linear combination of them would also have M =
1
2. So, in general, we must expect to ﬁnd that
| J = 3
2, M = + 1
2⟩= α | e, + 1
2; d, 0⟩+ β | e, −1
2; d, +1⟩,
(18.46)
18-28","383"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","505","where α and β are two numbers. They are called the Clebsch-Gordan coeﬃcients.
Our next problem is to ﬁnd out what they are.
We can ﬁnd out easily if we just remember that the deuteron is made up of a
neutron and a proton, and write the deuteron states out more explicitly using
the rules of Table 18-3. If we do that, the states listed in (18.42) then look as
shown in Table 18-4.
Table 18-4
Angular momentum states of a deuterium atom
M = 3
2
| e, + 1
2; d, +1⟩= | e, + 1
2; n, + 1
2; p, + 1
2⟩
M = 1
2
| e, + 1
2; d, 0⟩=
1
√
2
{| e, + 1
2; n, + 1
2; p, −1
2⟩+ | e, + 1
2; n, −1
2; p, + 1
2⟩}
| e, −1
2; d, +1⟩= | e, −1
2; n, + 1
2; p, + 1
2⟩
M = −1
2
| e, + 1
2; d, −1⟩= | e, + 1
2; n, −1
2; p, −1
2⟩
| e, −1
2; d, 0⟩=
1
√
2
{| e, −1
2; n, + 1
2; p, −1
2⟩+ | e, −1
2; n, −1
2; p, + 1
2⟩}
M = −3
2
| e, −1
2; d, −1⟩= | e, −1
2; n, −1
2; p, −1
2⟩
We want to form the four states of J = 3
2, using the states in the table.
But we already know the answer, because in Table 18-1 we have states of
spin 3
2 formed from three spin one-half particles. The ﬁrst state in Table 18-1
has | J = 3
2, M = + 3
2⟩and it is | + + +⟩, which—in our present notation—is the
same as | e, + 1
2; n, + 1
2; p, + 1
2⟩, or the ﬁrst state in Table 18-4. But this state is
also the same as the ﬁrst in the list of (18.42), conﬁrming our statement in (18.45).
The second line of Table 18-1 says—changing to our present notation—that
18-29","359"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","506","| J = 3
2, M = + 1
2⟩=
1
√
3 {| e, + 1
2; n, + 1
2; p, −1
2⟩
+ | e, + 1
2; n, −1
2; p, + 1
2⟩+ | e, −1
2; n, + 1
2; p, + 1
2⟩}.
(18.47)
The right side can evidently be put together from the two entries in the second
line of Table 18-4 by taking
p
2/3 of the ﬁrst term with
p
1/3 of the second.
That is, Eq. (18.47) is equivalent to
| J = 3
2, M = + 1
2⟩=
p
2/3 | e, + 1
2; d, 0⟩+
p
1/3 | e, −1
2; d, +1⟩.
(18.48)
We have found our two Clebsch-Gordan coeﬃcients α and β in Eq. (18.46):
α =
p
2/3,
β =
p
1/3.
(18.49)
Following the same procedure we can ﬁnd that
| J = 3
2, M = −1
2⟩=
p
1/3 | e, + 1
2; d, −1⟩+
p
2/3 | e, −1
2; d, 0⟩.
(18.50)
And, also, of course,
| J = 3
2, M = −3
2⟩= | e, −1
2; d, −1⟩.
(18.51)
These are the rules for the composition of spin 1 and spin 1
2 to make a total J = 3
2.
We summarize (18.45), (18.48), (18.50), and (18.51) in Table 18-5.
Table 18-5
The J = 3
2 states of the deuterium atom
| J = 3
2, M = + 3
2⟩= | e, + 1
2; d, +1⟩
| J = 3
2, M = + 1
2⟩=
p
2/3 | e, + 1
2; d, 0⟩+
p
1/3 | e, −1
2; d, +1⟩
| J = 3
2, M = −1
2⟩=
p
1/3 | e, + 1
2; d, −1⟩+
p
2/3 | e, −1
2; d, 0⟩
| J = 3
2, M = −3
2⟩= | e, −1
2; d, −1⟩
We have, however, only four states here while the system we are considering
has six possible states. Of the two states in the second line of (18.42) we have
18-30","355"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","507","used only one linear combination to form | J = 3
2, M = + 1
2⟩. There is another
linear combination orthogonal to the one we have taken which also has M = + 1
2,
namely
p
1/3 | e, + 1
2; d, 0⟩−
p
2/3 | e, −1
2; d, +1⟩.
(18.52)
Similarly, the two states in the third line of (18.42) can be combined to give two
orthogonal states, each with M = −1
2. The one orthogonal to (18.52) is
p
2/3 | e, + 1
2; d, −1⟩−
p
1/3 | e, −1
2; d, 0⟩.
(18.53)
These are the two remaining states. They have M = me + md = ± 1
2; and must
be the two states corresponding to J = 1
2. So we have
| J = 1
2, M = + 1
2⟩=
p
1/3 | e, + 1
2; d, 0⟩−
p
2/3 | e, −1
2; d, +1⟩,
| J = 1
2, M = −1
2⟩=
p
2/3 | e, + 1
2; d, −1⟩−
p
1/3 | e, −1
2; d, 0⟩.
(18.54)
We can verify that these two states do indeed behave like the states of a spin
one-half object by writing out the deuterium parts in terms of the neutron and
proton states—using Table 18-4. The ﬁrst state in (18.52) is
p
1/6 {| e, + 1
2; n, + 1
2; p, −1
2⟩+ | e, + 1
2; n, −1
2; p, + 1
2⟩}
−
p
2/3 | e, −1
2; n, + 1
2; p, + 1
2⟩,
(18.55)
which can also be written
p
1/3
p
1/2 {| e, + 1
2; n, + 1
2; p, −1
2⟩−| e, −1
2; n, + 1
2; p, + 1
2⟩}
+
p
1/2{| e, + 1
2; n, −1
2; p, + 1
2⟩−| e, −1
2; n, + 1
2; p, + 1
2⟩}

.
(18.56)
Now look at the terms in the ﬁrst curly brackets, and think of the e and p taken
together. Together they form a spin-zero state (see the bottom line of Table 18-3),
and contribute no angular momentum. Only the neutron is left, so the whole of
the ﬁrst curly bracket of (18.56) behaves under rotations like a neutron, namely
as a state with J = 1
2, M = + 1
2. Following the same reasoning, we see that in the
second curly bracket of (18.56) the electron and neutron team up to produce zero
18-31","428"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","508","angular momentum, and only the proton contribution—with mp = 1
2—is left.
The terms behave like an object with J = 1
2, M = + 1
2. So the whole expression
of (18.56) transforms like | J = 1
2, M = + 1
2⟩as it should. The M = −1
2 state
which corresponds to (18.53) can be written down (by changing the proper + 1
2’s
to −1
2’s) to get
p
1/3
p
1/2 {| e, + 1
2; n, −1
2; p, −1
2⟩−| e, −1
2; n, −1
2; p, + 1
2⟩}
+
p
1/2{| e, + 1
2; n, −1
2; p, −1
2⟩−| e, −1
2; n, + 1
2; p, −1
2⟩}

.
(18.57)
You can easily check that this is equal to the second line of (18.54), as it should
be if the two terms of that pair are to be the two states of a spin one-half system.
So our results are conﬁrmed. A deuteron and an electron can exist in six spin
states, four of which act like the states of a spin 3
2 object (Table 18-5) and two
of which act like an object of spin one-half (18.54).
The results of Table 18-5 and of Eq. (18.54) were obtained by making use of
the fact that the deuteron is made up of a neutron and a proton. The truth of
the equations does not depend on that special circumstance. For any spin-one
object put together with any spin one-half object the composition laws (and the
coeﬃcients) are the same. The set of equations in Table 18-5 means that if the
coordinates are rotated about, say, the y-axis—so that the states of the spin
one-half particle and of the spin-one particle change according to Table 17-1 and
Table 17-2—the linear combinations on the right-hand side will change in the
proper way for a spin 3
2 object. Under the same rotation the states of (18.54)
will change as the states of a spin one-half object. The results depend only on
the rotation properties (that is, the spin states) of the two original particles but
not in any way on the origins of their angular momenta. We have only made
use of this fact to work out the formulas by choosing a special case in which
one of the component parts is itself made up of two spin one-half particles in a
symmetric state. We have put all our results together in Table 18-6, changing
the notation “e” and “d” to “a” and “b” to emphasize the generality of the
conclusions.
Suppose we have the general problem of ﬁnding the states which can be
formed when two objects of arbitrary spins are combined. Say one has ja (so its
z-component ma runs over the 2ja + 1 values from −ja to +ja) and the other
18-32","480"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","509","Table 18-6
Composition of a spin one-half particle (ja = 1
2) and a spin-one particle (jb = 1)
| J = 3
2, M = + 3
2⟩= | a, + 1
2; b, +1⟩
| J = 3
2, M = + 1
2⟩=
p
2/3 | a, + 1
2; b, 0⟩+
p
1/3 | a, −1
2; b, +1⟩
| J = 3
2, M = −1
2⟩=
p
1/3 | a, + 1
2; b, −1⟩+
p
2/3 | a, −1
2; b, 0⟩
| J = 3
2, M = −3
2⟩= | a, −1
2; b, −1⟩
| J = 1
2, M = + 1
2⟩=
p
1/3 | a, + 1
2; b, 0⟩−
p
2/3 | a, −1
2; b, +1⟩
| J = 1
2, M = −1
2⟩=
p
2/3 | a, + 1
2; b, −1⟩−
p
1/3 | a, −1
2; b, 0⟩
has jb (with z-component mb running over the values from −jb to +jb). The
combined states are | a, ma; b, mb⟩, and there are (2ja + 1)(2jb + 1) diﬀerent ones.
Now what states of total spin J can be found?
The total z-component of angular momentum M is equal to ma + mb, and
the states can all be listed according to M [as in (18.42)]. The largest M is
unique; it corresponds to ma = ja and mb = jb, and is, therefore, just ja + jb.
That means that the largest total spin J is also equal to the sum ja + jb:
J = (M)max = ja + jb.
For the ﬁrst M value smaller than (M)max, there are two states (either ma or mb
is one unit less than its maximum). They must contribute one state to the
set that goes with J = ja + jb, and the one left over will belong to a new set
with J = ja + jb −1. The next M-value—the third from the top of the list—can
be formed in three ways. (From ma = ja −2, mb = jb; from ma = ja −1,
mb = jb −1; and from ma = ja, mb = jb −2.) Two of these belong to groups
already started above; the third tells us that states of J = ja + jb −2 must also
be included. This argument continues until we reach a stage where in our list we
can no longer go one more step down in one of the m’s to make new states.
Let jb be the smaller of ja and jb (if they are equal take either one); then only
2jb values of J are required—going in integer steps from ja + jb down to ja −jb.
That is, when two objects of spin ja and jb are combined, the system can have a
18-33","478"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","510","total angular momentum J equal to any one of the values
J =















ja + jb
ja + jb −1
ja + jb −2
...
|ja −jb|.
(18.58)
(By writing |ja −jb| instead of ja−jb we can avoid the extra admonition that ja ≥
jb.)
For each of these J values there are the 2J + 1 states of diﬀerent M-values—
with M going from +J to −J. Each of these is formed from linear combinations
of the original states | a, ma; b, mb⟩with appropriate factors—the Clebsch-Gordan
coeﬃcients for each particular term. We can consider that these coeﬃcients give
the “amount” of the state | ja, ma; jb, mb⟩which appears in the state | J, M⟩. So
each of the Clebsch-Gordan coeﬃcients has, if you wish, six indices identifying its
position in the formulas like those of Tables 18-3 and 18-6. That is, calling these
coeﬃcients C(J, M; ja, ma; jb, mb), we could express the equality of the second
line of Table 18-6 by writing
C( 3
2, + 1
2; 1
2, + 1
2; 1, 0) =
p
2/3,
C( 3
2, + 1
2; 1
2, −1
2; 1, +1) =
p
1/3.
We will not calculate here the coeﬃcients for any other special cases.† You
can, however, ﬁnd tables in many books. You might wish to try another special
case for yourself. The next one to do would be the composition of two spin-one
particles. We give just the ﬁnal result in Table 18-7.
These laws of the composition of angular momenta are very important in
particle physics—where they have innumerable applications. Unfortunately, we
have no time to look at more examples here.
18-7 Added Note 1: Derivation of the rotation matrix‡
For those who would like to see the details, we work out here the general
rotation matrix for a system with spin (total angular momentum) j. It is really
† A large part of the work is done now that we have the general rotation matrix Eq. (18.35).
‡ The material of this appendix was originally included in the body of the lecture. We now
feel that it is unnecessary to include such a detailed treatment of the general case.
18-34","384"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","511","Table 18-7
Composition of two spin-one particles (ja = 1, jb = 1)
| J = 2, M = +2⟩= | a, +1; b, +1⟩
| J = 2, M = +1⟩=
1
√
2
| a, +1; b, 0⟩+ 1
√
2
| a, 0; b, +1⟩
| J = 2, M =
0⟩=
1
√
6
| a, +1; b, −1⟩+ 1
√
6
| a, −1; b, +1⟩+ 2
√
6
| a, 0; b, 0⟩
| J = 2, M = −1⟩=
1
√
2
| a, 0; b, −1⟩+ 1
√
2
| a, −1; b, 0⟩
| J = 2, M = −2⟩= | a, −1; b, −1⟩
| J = 1, M = +1⟩=
1
√
2
| a, +1; b, 0⟩−1
√
2
| a, 0; b, +1⟩
| J = 1, M =
0⟩=
1
√
2
| a, +1; b, −1⟩−1
√
2
| a, −1; b, +1⟩
| J = 1, M = −1⟩=
1
√
2
| a, 0; b, −1⟩−1
√
2
| a, −1; b, 0⟩
| J = 0, M =
0⟩=
1
√
3
{| a, +1; b, −1⟩+ | a, −1; b, +1⟩−| a, 0; b, 0⟩}
not very important to work out the general case; once you have the idea, you
can ﬁnd the general results in tables in many books. On the other hand, after
coming this far you might like to see that you can indeed understand even the
very complicated formulas of quantum mechanics, such as Eq. (18.35), that come
into the description of angular momentum.
We extend the arguments of Section 18-4 to a system with spin j, which we
consider to be made up of 2j spin one-half objects. The state with m = j would
be | + + + · · · +⟩(with 2j plus signs). For m = j −1, there will be 2j terms
like | + + · · · + + −⟩, | + + · · · + −+⟩, and so on. Let’s consider the general
case in which there are r plusses and s minuses—with r + s = 2j. Under a
rotation about the z-axis each of the r plusses will contribute e+iφ/2. The result
18-35","378"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","512","is a phase change of (r/2 −s/2)φ. You see that
m = r −s
2
.
(18.59)
Just as for j = 3
2, each state of deﬁnite m must be the linear combination with
plus signs of all the states with the same r and s—that is, states corresponding to
every possible arrangement which has r plusses and s minuses. We assume that
you can ﬁgure out that there are (r + s)!/r!s! such arrangements. To normalize
each state, we should divide the sum by the square root of this number. We can
write
(r + s)!
r!s!
−1/2
{| + + + · · · + +
|
{z
}
r
−−−· · · −−
|
{z
}
s
⟩
+ (all rearrangements of order)} = | j, m⟩
(18.60)
with
j = r + s
2
,
m = r −s
2
.
(18.61)
It will help our work if we now go to still another notation. Once we have
deﬁned the states by Eq. (18.60), the two numbers r and s deﬁne a state just as
well as j and m. It will help us keep track of things if we write
| j, m⟩= | r
s⟩,
(18.62)
where, using the equalities of (18.61)
r = j + m,
s = j −m.
Next, we would like to write Eq. (18.60) with a new special notation as
| j, m⟩= | r
s⟩=
(r + s)!
r!s!
+1/2
{| +⟩r| −⟩s}perm.
(18.63)
Note that we have changed the exponent of the factor in front to plus 1
2. We
do that because there are just N = (r + s)!/r!s! terms inside the curly brackets.
Comparing (18.63) with (18.60) it is clear that
{| +⟩r| −⟩s}perm
18-36","292"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","513","is just a shorthand way of writing
{| + + · · · −−⟩+ all rearrangements}
N
,
where N is the number of diﬀerent terms in the bracket. The reason that this
notation is convenient is that each time we make a rotation, all of the plus signs
contribute the same factor, so we get this factor to the rth power. Similarly, all
together the s minus terms contribute a factor to the sth power no matter what
the sequence of the terms is.
Now suppose we rotate our system by the angle θ about the y-axis. What we
want is Ry(θ) | r
s⟩. When Ry(θ) operates on each | +⟩it gives
Ry(θ) | +⟩= | +⟩C + | −⟩S,
(18.64)
where C = cos θ/2 and S = −sin θ/2. When Ry(θ) operates on each | −⟩it gives
Ry(θ) | −⟩= | −⟩C −| +⟩S.
So what we want is
Ry(θ) | r
s⟩=
(r + s)!
r!s!
1/2
Ry(θ) {| +⟩r| −⟩s}perm
=
(r + s)!
r!s!
1/2
{(Ry(θ) | +⟩)r(Ry(θ) | −⟩)s}perm
=
(r + s)!
r!s!
1/2
{(| +⟩C + | −⟩S)r(| −⟩C −| +⟩S)s}perm.
(18.65)
Now each binomial has to be expanded out to its appropriate power and the two
expressions multiplied together. There will be terms with | +⟩to all powers from
zero to (r + s). Let’s look at all of the terms which have | +⟩to the r′ power.
They will appear always multiplied with | −⟩to the s′ power, where s′ = 2j −r′.
Suppose we collect all such terms. For each permutation they will have some
numerical coeﬃcient involving the factors of the binomial expansion as well as
the factors C and S. Suppose we call that factor Ar′. Then Eq. (18.65) will look
like
Ry(θ) | r
s⟩=
r+s
X
r′=0
{Ar′ | +⟩r′| −⟩s′}perm.
(18.66)
18-37","310"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","514","Now let’s say that we divide Ar′ by the factor [(r′ + s′)!/r′!s′!]1/2 and call the
quotient Br′. Equation (18.66) is then equivalent to
Ry(θ) | r
s⟩=
r+s
X
r′=0
Br′
(r′ + s′)!
r′!s′!
1/2
{| +⟩r′| −⟩s′}perm.
(18.67)
(We could just say that this equation deﬁnes Br′ by the requirement that (18.67)
gives the same expression that appears in (18.65).)
With this deﬁnition of Br′ the remaining factors on the right-hand side of
Eq. (18.67) are just the states | r′
s′⟩. So we have that
Ry(θ) | r
s⟩=
r+s
X
r′=0
Br′ | r′
s′⟩,
(18.68)
with s′ always equal to r + s −r′. This means, of course, that the coeﬃcients Br′
are just the matrix elements we want, namely
⟨r′
s′ | Ry(θ) | r
s⟩= Br′.
(18.69)
Now we just have to push through the algebra to ﬁnd the various Br′. Com-
paring (18.65) with (18.67)—and remembering that r′ + s′ = r + s—we see that
Br′ is just the coeﬃcient of ar′bs′ in the following expression:
r′!s′!
r!s!
1/2
(aC + bS)r(bC −aS)s.
(18.70)
It is now only a dirty job to make the expansions by the binomial theorem, and
collect the terms with the given power of a and b. If you work it all out, you ﬁnd
that the coeﬃcient of ar′bs′ in (18.70) is
r′!s′!
r!s!
1/2 X
k
(−1)kSr−r′+2kCs+r′−2k ·
r!
(r −r′ + k)!(r′ −k)! ·
s!
(s −k)!k!.
(18.71)
The sum is to be taken over all integers k which give terms of zero or greater in
the factorials. This expression is then the matrix element we wanted.
Finally, we can return to our original notation in terms of j, m, and m′ using
r = j + m,
r′ = j + m′,
s = j −m,
s′ = j −m′.
Making these substitutions, we get Eq. (18.35) in Section 18-4.
18-38","319"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","515","18-8 Added Note 2: Conservation of parity in photon emission
In Section 18-1 of this chapter we considered the emission of light by an atom
that goes from an excited state of spin 1 to a ground state of spin 0. If the excited
state has its spin up (m = +1), it can emit a RHC photon along the +z-axis or
a LHC photon along the −z-axis. Let’s call these two states of the photon | Rup⟩
and | Ldn⟩. Neither of these states has a deﬁnite parity. Letting ˆP be the parity
operator, ˆP | Rup⟩= | Ldn⟩and ˆP | Ldn⟩= | Rup⟩.
What about our earlier proof that an atom in a state of deﬁnite energy must
have a deﬁnite parity, and our statement that parity is conserved in atomic
processes? Shouldn’t the ﬁnal state in this problem (the state after the emission
of a photon) have a deﬁnite parity? It does if we consider the complete ﬁnal state
which contains amplitudes for the emission of photons into all sorts of angles. In
Section 18-1 we chose to consider only a part of the complete ﬁnal state.
If we wish we can look only at ﬁnal states that do have a deﬁnite parity.
For example, consider a ﬁnal state | ψF ⟩which has some amplitude α to be a
RHC photon going along +z and some amplitude β to be a LHC photon going
along −z. We can write
| ψF ⟩= α | Rup⟩+ β | Ldn⟩.
(18.72)
The parity operation on this state gives
ˆP | ψF ⟩= α | Ldn⟩+ β | Rup⟩.
(18.73)
This state will be ± | ψF ⟩if β = α or if β = −α. So a ﬁnal state of even parity is
| ψ+
F ⟩= α{| Rup⟩+ | Ldn⟩},
(18.74)
and a state of odd parity is
| ψ−
F ⟩= α{| Rup⟩−| Ldn⟩}.
(18.75)
Next, we wish to consider the decay of an excited state of odd parity to a
ground state of even parity. If parity is to be conserved, the ﬁnal state of the
photon must have odd parity. It must be the state in (18.75). If the amplitude
to ﬁnd | Rup⟩is α, the amplitude to ﬁnd | Ldn⟩is −α.
Now notice what happens when we perform a rotation of 180◦about the
y-axis. The initial excited state of the atom becomes an m = −1 state (with no
change in sign, according to Table 17-2). And the rotation of the ﬁnal state gives
Ry(180◦) | ψ−
F ⟩= α{| Rdn⟩−| Lup⟩}.
(18.76)
18-39","430"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","516","Comparing this equation with (18.75), you see that for the assumed parity of the
ﬁnal state, the amplitude to get a LHC photon along +z from the m = −1 initial
state is the negative of the amplitude to get a RHC photon from the m = +1 initial
state. This agrees with the result we found in Section 18-1.
18-40","61"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","517","19
The Hydrogen Atom and
The Periodic Table
19-1 Schrödinger’s equation for the hydrogen atom
The most dramatic success in the history of the quantum mechanics was
the understanding of the details of the spectra of some simple atoms and the
understanding of the periodicities which are found in the table of chemical
elements. In this chapter we will at last bring our quantum mechanics to the
point of this important achievement, speciﬁcally to an understanding of the
spectrum of the hydrogen atom. We will at the same time arrive at a qualitative
explanation of the mysterious properties of the chemical elements. We will do
this by studying in detail the behavior of the electron in a hydrogen atom—for
the ﬁrst time making a detailed calculation of a distribution-in-space according
to the ideas we developed in Chapter 16.
For a complete description of the hydrogen atom we should describe the
motions of both the proton and the electron. It is possible to do this in quantum
mechanics in a way that is analogous to the classical idea of describing the motion
of each particle relative to the center of gravity, but we will not do so. We will
just discuss an approximation in which we consider the proton to be very heavy,
so we can think of it as ﬁxed at the center of the atom.
We will make another approximation by forgetting that the electron has a spin
and should be described by relativistic laws of mechanics. Some small corrections
to our treatment will be required since we will be using the nonrelativistic
Schrödinger equation and will disregard magnetic eﬀects. Small magnetic eﬀects
occur because from the electron’s point-of-view the proton is a circulating charge
which produces a magnetic ﬁeld. In this ﬁeld the electron will have a diﬀerent
energy with its spin up than with it down. The energy of the atom will be shifted
a little bit from what we will calculate. We will ignore this small energy shift.
Also we will imagine that the electron is just like a gyroscope moving around in
19-1","348"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","518","space always keeping the same direction of spin. Since we will be considering
a free atom in space the total angular momentum will be conserved. In our
approximation we will assume that the angular momentum of the electron spin
stays constant, so all the rest of the angular momentum of the atom—what
is usually called “orbital” angular momentum—will also be conserved. To an
excellent approximation the electron moves in the hydrogen atom like a particle
without spin—the angular momentum of the motion is a constant.
With these approximations the amplitude to ﬁnd the electron at diﬀerent
places in space can be represented by a function of position in space and time.
We let ψ(x, y, z, t) be the amplitude to ﬁnd the electron somewhere at the time t.
According to the quantum mechanics the rate of change of this amplitude with
time is given by the Hamiltonian operator working on the same function. From
Chapter 16,
iℏ∂ψ
∂t = ˆHψ,
(19.1)
with
ˆH = −ℏ2
2m ∇2 + V (r).
(19.2)
Here, m is the electron mass, and V (r) is the potential energy of the electron in
the electrostatic ﬁeld of the proton. Taking V = 0 at large distances from the
proton we can write†
V = −e2
r .
The wave function ψ must then satisfy the equation
iℏ∂ψ
∂t = −ℏ2
2m ∇2ψ −e2
r ψ.
(19.3)
We want to look for deﬁnite energy states, so we try to ﬁnd solutions which
have the form
ψ(r, t) = e−(i/ℏ)Etψ(r).
(19.4)
The function ψ(r) must then be a solution of
−ℏ2
2m ∇2ψ =

E + e2
r

ψ,
(19.5)
where E is some constant—the energy of the atom.
† As usual, e2 = q2
e/4πϵ0.
19-2","293"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","519","x
y
z
O
P
r
φ
θ
Fig. 19-1. The spherical polar coordinates r, θ, φ of the point P.
Since the potential energy term depends only on the radius, it turns out to
be much more convenient to solve this equation in polar coordinates rather than
rectangular coordinates. The Laplacian is deﬁned in rectangular coordinates by
∇2 = ∂2
∂x2 + ∂2
∂y2 + ∂2
∂z2 .
We want to use instead the coordinates r, θ, φ shown in Fig. 19-1.
These
coordinates are related to x, y, z by
x = r sin θ cos φ;
y = r sin θ sin φ;
z = r cos θ.
It’s a rather tedious mess to work through the algebra, but you can eventually
show that for any function f(r) = f(r, θ, φ),
∇2f(r, θ, φ) = 1
r
∂2
∂r2 (rf) + 1
r2

1
sin θ
∂
∂θ

sin θ ∂f
∂θ

+
1
sin2 θ
∂2f
∂φ2

.
(19.6)
19-3","169"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","520","So in terms of the polar coordinates, the equation which is to be satisﬁed
by ψ(r, θ, φ) is
1
r
∂2
∂r2 (rψ) + 1
r2

1
sin θ
∂
∂θ

sin θ ∂ψ
∂θ

+
1
sin2 θ
∂2ψ
∂φ2

= −2m
ℏ2

E + e2
r

ψ.
(19.7)
19-2 Spherically symmetric solutions
Let’s ﬁrst try to ﬁnd some very simple function that satisﬁes the horrible
equation in (19.7). Although the wave function ψ will, in general, depend on
the angles θ and φ as well as on the radius r, we can see whether there might
be a special situation in which ψ does not depend on the angles. For a wave
function that doesn’t depend on the angles, none of the amplitudes will change
in any way if you rotate the coordinate system. That means that all of the
components of the angular momentum are zero. Such a ψ must correspond to
a state whose total angular momentum is zero. (Actually, it is only the orbital
angular momentum which is zero because we still have the spin of the electron,
but we are ignoring that part.) A state with zero orbital angular momentum is
called by a special name. It is called an “s-state”—you can remember “s for
spherically symmetric.”†
Now if ψ, is not going to depend on θ and φ then the entire Laplacian contains
only the ﬁrst term and Eq. (19.7) becomes much simpler:
1
r
d2
dr2 (rψ) = −2m
ℏ2

E + e2
r

ψ.
(19.8)
Before you start to work on solving an equation like this, it’s a good idea to get
rid of all excess constants like e2, m, and ℏ, by making some scale changes. Then
the algebra will be easier. If we make the following substitutions:
r = ℏ2
me2 ρ,
(19.9)
and
E = me4
2ℏ2 ϵ,
(19.10)
† Since these special names are part of the common vocabulary of atomic physics, you will
just have to learn them. We will help out by putting them together in a short “dictionary”
later in the chapter.
19-4","358"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","521","then Eq. (19.8) becomes (after multiplying through by ρ)
d2(ρψ)
dρ2
= −

ϵ + 2
ρ

ρψ.
(19.11)
These scale changes mean that we are measuring the distance r and energy E
as multiples of “natural” atomic units. That is, ρ = r/rB, where rB = ℏ2/me2,
is called the “Bohr radius” and is about 0.528 angstroms. Similarly, ϵ = E/ER,
with ER = me4/2ℏ2. This energy is called the “Rydberg” and is about 13.6 elec-
tron volts.
Since the product ρψ appears on both sides, it is convenient to work with it
rather than with ψ itself. Letting
ρψ = f,
(19.12)
we have the more simple-looking equation
d2f
dρ2 = −

ϵ + 2
ρ

f.
(19.13)
Now we have to ﬁnd some function f which satisﬁes Eq. (19.13)—in other
words, we just have to solve a diﬀerential equation. Unfortunately, there is no
very useful, general method for solving any given diﬀerential equation. You just
have to ﬁddle around. Our equation is not easy, but people have found that it
can be solved by the following procedure. First, you replace f, which is some
function of ρ, by a product of two functions
f(ρ) = e−αρg(ρ).
(19.14)
This just means that you are factoring e−αρ out of f(ρ). You can certainly
do that for any f(ρ) at all. This just shifts our problem to ﬁnding the right
function g(ρ).
Sticking (19.14) into (19.13), we get the following equation for g:
d2g
dρ2 −2α dg
dρ +
2
ρ + ϵ + α2

g = 0.
(19.15)
Since we are free to choose α, let’s make
α2 = −ϵ,
(19.16)
19-5","278"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","522","and get
d2g
dρ2 −2α dg
dρ + 2
ρ g = 0.
(19.17)
You may think we are no better oﬀthan we were at Eq. (19.13), but the
happy thing about our new equation is that it can be solved easily in terms of a
power series in ρ. (It is possible, in principle, to solve (19.13) that way too, but
it is much harder.) We are saying that Eq. (19.17) can be satisﬁed by some g(ρ)
which can be written as a series,
g(ρ) =
∞
X
k=1
akρk,
(19.18)
in which the ak are constant coeﬃcients. Now all we have to do is ﬁnd a suitable
inﬁnite set of coeﬃcients! Let’s check to see that such a solution will work. The
ﬁrst derivative of this g(ρ) is
dg
dρ =
∞
X
k=1
akkρk−1,
and the second derivative is
d2g
dρ2 =
∞
X
k=1
akk(k −1)ρk−2.
Using these expressions in (19.17) we have
∞
X
k=1
k(k −1)akρk−2 −
∞
X
k=1
2αkakρk−1 +
∞
X
k=1
2akρk−1 = 0.
(19.19)
It’s not obvious that we have succeeded; but we forge onward. It will all look
better if we replace the ﬁrst sum by an equivalent. Since the ﬁrst term of the
sum is zero, we can replace each k by k + 1 without changing anything in the
inﬁnite series; with this change the ﬁrst sum can equally well be written as
∞
X
k=1
(k + 1)kak+1ρk−1.
19-6","244"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","523","Now we can put all the sums together to get
∞
X
k=1
[(k + 1)kak+1 −2αkak + 2ak]ρk−1 = 0.
(19.20)
This power series must vanish for all possible values of ρ. It can do that only
if the coeﬃcient of each power of ρ is separately zero. We will have a solution for
the hydrogen atom if we can ﬁnd a set ak for which
(k + 1)kak+1 −2(αk −1)ak = 0
(19.21)
for all k ≥1. That is certainly easy to arrange. Pick any a1 you like. Then
generate all of the other coeﬃcients from
ak+1 = 2(αk −1)
k(k + 1) ak.
(19.22)
With this you will get a2, a3, a4, and so on, and each pair will certainly sat-
isfy (19.21). We get a series for g(ρ) which satisﬁes (19.17). With it we can make
a ψ, that satisﬁes Schrödinger’s equation. Notice that the solutions depend on
the assumed energy (through α), but for each value of ϵ, there is a corresponding
series.
We have a solution, but what does it represent physically? We can get an
idea by seeing what happens far from the proton—for large values of ρ. Out
there, the high-order terms of the series are the most important, so we should
look at what happens for large k. When k ≫1, Eq. (19.22) is approximately the
same as
ak+1 = 2α
k ak,
which means that
ak+1 ≈(2α)k
k!
.
(19.23)
But these are just the coeﬃcients of the series for e+2αρ. The function of g is a
rapidly increasing exponential. Even coupled with e−αρ to produce f(ρ)—see
Eq. (19.14)—it still gives a solution for f(ρ) which goes like eαρ for large ρ.
We have found a mathematical solution but not a physical one. It represents a
situation in which the electron is least likely to be near the proton! It is always
19-7","313"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","524","more likely to be found at a very large radius ρ. A wave function for a bound
electron must go to zero for large ρ.
We have to think whether there is some way to beat the game, and there is.
Observe! If it just happened by luck that α were equal to 1/n, where n is any
positive integer, then Eq. (19.22) would make an+1 = 0. All higher terms would
also be zero. We wouldn’t have an inﬁnite series but a ﬁnite polynomial. Any
polynomial increases more slowly than eαρ, so the term e−αρ will eventually beat
it down, and the function f will go to zero for large ρ. The only bound-state
solutions are those for which α = 1/n, with n = 1, 2, 3, 4, and so on.
Looking back to Eq. (19.16), we see that the bound-state solutions to the
spherically symmetric wave equation can exist only when
−ϵ = 1, 1
4, 1
9, 1
16, . . . , 1
n2 , . . .
The allowed energies are just these fractions times the Rydberg, ER = me4/2ℏ2,
or the energy of the nth energy level is
En = −ER
1
n2 .
(19.24)
There is, incidentally, nothing mysterious about negative numbers for the energy.
The energies are negative because when we chose to write V = −e2/r, we picked
our zero point as the energy of an electron located far from the proton. When it
is close to the proton, its energy is less, so somewhat below zero. The energy is
lowest (most negative) for n = 1, and increases toward zero with increasing n.
Before the discovery of quantum mechanics, it was known from experimental
studies of the spectrum of hydrogen that the energy levels could be described by
Eq. (19.24), where ER was found from the observations to be about 13.6 electron
volts. Bohr then devised a model which gave the same equation and predicted
that ER should be me4/2ℏ2. But it was the ﬁrst great success of the Schrödinger
theory that it could reproduce this result from a basic equation of motion for the
electron.
Now that we have solved our ﬁrst atom, let’s look at the nature of the solution
we got. Pulling all the pieces together, each solution looks like this:
ψn = fn(ρ)
ρ
= e−ρ/n
ρ
gn(ρ),
(19.25)
19-8","395"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","525","where
gn(ρ) =
n
X
k=1
akρk
(19.26)
and
ak+1 = 2(k/n −1)
k(k + 1)
ak.
(19.27)
So long as we are mainly interested in the relative probabilities of ﬁnding the
electron at various places we can pick any number we wish for a1. We may as
well set a1 = 1. (People often choose a1 so that the wave function is “normalized,”
that is, so that the integrated probability of ﬁnding the electron anywhere in the
atom is equal to 1. We have no need to do that just now.)
For the lowest energy state, n = 1, and
ψ1(ρ) = e−ρ.
(19.28)
For a hydrogen atom in its ground (lowest-energy) state, the amplitude to ﬁnd
the electron at any point drops oﬀexponentially with the distance from the
proton. It is most likely to be found right at the proton, and the characteristic
spreading distance is about one unit in ρ, or about one Bohr radius, rB.
Putting n = 2 gives the next higher level. The wave function for this state
will have two terms. It is
ψ2(ρ) =

1 −ρ
2

e−ρ/2.
(19.29)
The wave function for the next level is
ψ3(ρ) =

1 −2ρ
3 + 2
27 ρ2

e−ρ/3.
(19.30)
The wave functions for these ﬁrst three levels are plotted in Fig. 19-2. You
can see the general trend. All of the wave functions approach zero rapidly for
large ρ after oscillating a few times. In fact, the number of “bumps” is just equal
to n—or, if you prefer, the number of zero-crossings of ψn is n −1.
19-3 States with an angular dependence
In the states described by the ψn(r) we have found that the probability
amplitude for ﬁnding the electron is spherically symmetric—depending only on r,
19-9","299"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","526","r
ψ
n = 1
n = 2
n = 3
Fig. 19-2. The wave functions for the ﬁrst three l = 0 states of the
hydrogen atom. (The scales are chosen so that the total probabilities
are equal.)
the distance for the proton. Such states have zero orbital angular momentum.
We should now inquire about states which may have some angular dependences.
We could, if we wished, just investigate the strictly mathematical problem of
ﬁnding the functions of r, θ, and φ which satisfy the diﬀerential equation (19.7)—
putting in the additional physical conditions that the only acceptable functions
are ones which go to zero for large r. You will ﬁnd this done in many books. We
are going to take a short cut by using the knowledge we already have about how
amplitudes depend on angles in space.
The hydrogen atom in any particular state is a particle with a certain “spin” j—
the quantum number of the total angular momentum. Part of this spin comes
from the electron’s intrinsic spin, and part from the electron’s motion. Since
each of these two components acts independently (to an excellent approximation)
we will again ignore the spin part and think only about the “orbital” angular
momentum. This orbital motion behaves, however, just like a spin. For example,
if the orbital quantum number is l, the z-component of angular momentum can
be l, l −1, l −2, . . . , −l. (We are, as usual, measuring in units of ℏ.) Also,
all the rotation matrices and other properties we have worked out still apply.
(From now on we will really ignore the electron’s spin; when we speak of “angular
momentum” we will mean only the orbital part.)
19-10","288"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","527","Since the potential V in which the electron moves depends only on r and
not on θ or φ, the Hamiltonian is symmetric under all rotations. It follows that
the angular momentum and all its components are conserved. (This is true for
motion in any “central ﬁeld”—one which depends only on r—so is not a special
feature of the Coulomb e2/r potential.)
Now let’s think of some possible state of the electron; its internal angular
structure will be characterized by the quantum number l. Depending on the
“orientation” of the total angular momentum with respect to the z-axis, the z-
component of angular momentum will be m, which is one of the 2l+1 possibilities
between +l and −l. Let’s say m = 1. With what amplitude will the electron be
found on the z-axis at some distance r? Zero. An electron on the z-axis cannot
have any orbital angular momentum around that axis. Alright, suppose m is
zero, then there can be some nonzero amplitude to ﬁnd the electron at each
distance from the proton. We’ll call this amplitude Fl(r). It is the amplitude to
ﬁnd the electron at the distance r up along the z-axis, when the atom is in the
state | l, 0⟩, by which we mean orbital spin l and z-component m = 0.
If we know Fl(r) everything is known. For any state | l, m⟩, we know the
amplitude ψl,m(r) to ﬁnd the electron anywhere in the atom. How? Watch.
Suppose we have the atom in the state | l, m⟩, what is the amplitude to ﬁnd the
electron at the angle θ, φ and the distance r from the origin? Put a new z-axis,
say z′, at that angle (see Fig. 19-3), and ask what is the amplitude that the
electron will be at the distance r along the new axis z′? We know that it cannot
be found along z′ unless its z′-component of angular momentum, say m′, is zero.
When m′ is zero, however, the amplitude to ﬁnd the electron along z′ is Fl(r).
Therefore, the result is the product of two factors. The ﬁrst is the amplitude
that an atom in the state | l, m⟩along the z-axis will be in the state | l, m′ = 0⟩
with respect to the z′-axis. Multiply that amplitude by Fl(r) and you have the
amplitude ψl,m(r) to ﬁnd the electron at (r, θ, φ) with respect to the original axes.
Let’s write it out. We have worked out earlier the transformation matrices
for rotations. To go from the frame x, y, z to the frame x′, y′, z′ of Fig. 19-3, we
can rotate ﬁrst around the z-axis by the angle φ, and then rotate about the new
y-axis (y′) by the angle θ. This combined rotation is the product
Ry(θ)Rz(φ).
The amplitude to ﬁnd the state l, m′ = 0 after the rotation is
⟨l, 0 | Ry(θ)Rz(φ) | l, m⟩.
(19.31)
19-11","492"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","528","x
y
z
x′
y ′
z′
(r, θ, φ)
r
θ
φ
φ
Fig. 19-3. The point (r, θ, φ) is on the z′-axis of the x′, y ′, z ′ coordi-
nate frame.
Our result, then, is
ψl,m(r) = ⟨l, 0 | Ry(θ)Rz(φ) | l, m⟩Fl(r).
(19.32)
The orbital motion can have only integral values of l. (If the electron can
be found anywhere at r ̸= 0, there is some amplitude to have m = 0 in that
direction. And m = 0 states exist only for integral spins.) The rotation ma-
trices for l = 1 are given in Table 17-2. For larger l you can use the general
formulas we worked out in Chapter 18.
The matrices for Rz(φ) and Ry(θ)
appear separately, but you know how to combine them. For the general case
you would start with the state | l, m⟩and operate with Rz(φ) to get the new
state Rz(φ) | l, m⟩(which is just eimφ | l, m⟩). Then you operate on this state
with Ry(θ) to get the state Ry(θ)Rz(φ) | l, m⟩. Multiplying by ⟨l, 0 | gives the
matrix element (19.31).
19-12","193"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","529","The matrix elements of the rotation operation are functions of θ and φ. The
particular functions which appear in (19.31) also show up in many kinds of
problems which involve waves in spherical geometries and so has been given
a special name. Not everyone uses the same convention; but one of the most
common ones is
⟨l, 0 | Ry(θ)Rz(φ) | l, m⟩≡a Yl,m(θ, φ).
(19.33)
The functions Yl,m(θ, φ) are called the spherical harmonics, and a is just a
numerical factor which depends on the deﬁnition chosen for Yl,m. For the usual
deﬁnition,
a =
r
4π
2l + 1.
(19.34)
With this notation, the hydrogen wave functions can be written
ψl,m(r) = a Yl,m(θ, φ)Fl(r).
(19.35)
The angle functions Yl,m(θ, φ) are important not only in many quantum-
mechanical problems, but also in many areas of classical physics in which the
∇2 operator appears, such as electromagnetism. As another example of their use
in quantum mechanics, consider the disintegration of an excited state of Ne20
(such as we discussed in the last chapter) which decays by emitting an α-particle
and going into O16:
Ne20∗→O16 + He4.
Suppose that the excited state has some spin l (necessarily an integer) and that
the z-component of angular momentum is m. We might now ask the following:
given l and m, what is the amplitude that we will ﬁnd the α-particle going oﬀin
a direction which makes the angle θ with respect to the z-axis and the angle φ
with respect to the xz-plane—as shown in Fig. 19-4.
To solve this problem we make, ﬁrst, the following observation. A decay in
which the α-particle goes straight up along z must come from a state with m = 0.
This is so because both O16 and the α-particle have spin zero, and because their
motion cannot have any angular momentum about the z-axis. Let’s call this
amplitude a (per unit solid angle). Then, to ﬁnd the amplitude for a decay at the
arbitrary angle of Fig. 19-4, all we need to know is what amplitude the given initial
state has zero angular momentum about the decay direction. The amplitude
for the decay at θ and φ is then a times the amplitude that a state | l, m⟩with
respect to the z-axis will be in the state | l, 0⟩with respect to z′—the decay
19-13","392"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","530","x
y
z
Ne20∗
| l, m⟩
x
y
z
O16
α
φ
θ
Fig. 19-4. The decay of an excited state of Ne20.
direction. This latter amplitude is just what we have written in (19.31). The
probability to see the α-particle at θ, φ is
P(θ, φ) = a2|⟨l, 0 | Ry(θ)Rz(φ) | l, m⟩|2.
As an example, consider an initial state with l = 1 and various values of m.
From Table 17-2 we know the necessary amplitudes. They are
⟨1, 0 | Ry(θ)Rz(φ) | 1, +1⟩= −1
√
2 sin θeiφ,
⟨1, 0 | Ry(θ)Rz(φ) | 1, 0⟩= cos θ,
⟨1, 0 | Ry(θ)Rz(φ) | 1, −1⟩=
1
√
2 sin θe−iφ.
(19.36)
These are the three possible angular distribution amplitudes—depending on the
m-value of the initial nucleus.
Amplitudes such as the ones in (19.36) appear so often and are suﬃciently
important that they are given several names. If the angular distribution amplitude
is proportional to any one of the three functions or any linear combination of
them, we say, “The system has an orbital angular momentum of one.” Or we may
say, “The Ne20∗emits a p-wave α-particle.” Or we say, “The α-particle is emitted
in an l = 1 state.” Because there are so many ways of saying the same thing it is
useful to have a dictionary. If you are going to understand what other physicists
19-14","232"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","531","are talking about, you will just have to memorize the language. In Table 19-1 we
give a dictionary of orbital angular momentum.
Table 19-1
Dictionary of orbital angular momentum
(l = j = an integer)
Orbital
z-
component,
m
Name
angular
Angular dependence
Number of
Orbital
momentum,
of amplitudes
states
parity
l
0
0
1
s
1
+









+1
−1
√
2
sin θeiφ









1
0
cos θ
p
3
−
−1
1
√
2
sin θe−iφ



























+2
√
6
4 sin2 θe2iφ



























+1
√
6
2 sin θ cos θeiφ
2
0
1
2(3 cos2 θ −1)
d
5
+
−1
−
√
6
2 sin θ cos θe−iφ
−2
√
6
4 sin2 θe−2iφ
3
)
(
⟨l, 0 | Ry(θ)Rz(φ) | l, m⟩)
f
4
= Yl,m(θ, φ)
g
2l + 1
(−1)l
5
= P m
l (cos θ)eimφ
h
...
...
If the orbital angular momentum is zero, then there is no change when
you rotate the coordinate system and there is no variation with angle—the
19-15","244"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","532","“dependence” on angle is as a constant, say 1. This is also called an “s-state”, and
there is only one such state—as far as angular dependence is concerned. If the
orbital angular momentum is 1, then the amplitude of the angular variation may
be any one of the three functions given—depending on the value of m—or it may
be a linear combination. These are called “p-states,” and there are three of them.
If the orbital angular momentum is 2 then there are the ﬁve functions shown. Any
linear combination is called an “l = 2,” or a “d-wave” amplitude. Now you can
immediately guess what the next letter is—what should come after s, p, d? Well,
of course, f, g, h, and so on down the alphabet! The letters don’t mean anything.
(They did once mean something—they meant “sharp” lines, “principal” lines,
“diﬀuse” lines and “fundamental” lines of the optical spectra of atoms. But those
were in the days when people did not know where the lines came from. After
f there were no special names, so we now just continue with g, h, and so on.)
The angular functions in the table go by several names—and are sometimes
deﬁned with slightly diﬀerent conventions about the numerical factors that appear
out in front. Sometimes they are called “spherical harmonics,” and written
as Yl,m(θ, φ). Sometimes they are written P m
l (cos θ)eimφ, and if m = 0, simply
as Pl(cos θ). The functions Pl(cos θ) are called the “Legendre polynomials” in
cos θ, and the functions P m
l (cos θ) are called the “associated Legendre functions.”
You will ﬁnd tables of these functions in many books.
Notice, incidentally, that all the functions for a given l have the property that
they have the same parity—for odd l they change sign under an inversion and
for even l they don’t change. So we can write that the parity of a state of orbital
angular momentum l is (−1)l.
As we have seen, these angular distributions may refer to a nuclear disinte-
gration or some other process, or to the distribution of the amplitude to ﬁnd
an electron at some place in the hydrogen atom. For instance, if an electron is
in a p-state (l = 1) the amplitude to ﬁnd it can depend on the angle in many
possible ways—but all are linear combinations of the three functions for l = 1 in
Table 19-1. Let’s take the case cos θ. That’s interesting. That means that the
amplitude is positive, say, in the upper part (θ < π/2), is negative in the lower
part (θ > π/2), and is zero when θ is 90◦. Squaring this amplitude we see that
the probability of ﬁnding the electron varies with θ as shown in Fig. 19-5—and
is independent of φ. This angular distribution is responsible for the fact that
in molecular binding the attraction of an electron in an l = 1 state for another
atom depends on direction—it is the origin of the directed valences of chemical
attraction.
19-16","509"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","533","z
θ
PROBABILITY
Fig. 19-5. A polar graph of cos2 θ, which is the relative probability of
ﬁnding an electron at various angles from the z-axis (for a given r) in
an atomic state with l = 1 and m = 0.
19-4 The general solution for hydrogen
In Eq. (19.35) we have written the wave functions for the hydrogen atom as
ψl,m(r) = a Yl,m(θ, φ)Fl(r).
(19.37)
These wave functions must be solutions of the diﬀerential equation (19.7). Let’s
see what that means. Put (19.37) into (19.7); you get
Yl,m
r
∂2
∂r2 (rFl) +
Fl
r2 sin θ
∂
∂θ

sin θ ∂Yl,m
∂θ

+
Fl
r2 sin2 θ
∂2Yl,m
∂φ2
= −2m
ℏ2

E + e2
r

Yl,mFl.
(19.38)
19-17","127"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","534","Now multiply through by r2/Fl and rearrange terms. The result is
1
sin θ
∂
∂θ

sin θ ∂Yl,m
∂θ

+
1
sin2 θ
∂2Yl,m
∂φ2
= −
r2
Fl
1
r
d2
dr2 (rFl) + 2m
ℏ2

E + e2
r

Fl

Yl,m.
(19.39)
The left-hand side of this equation depends on θ and φ, but not on r. No matter
what value we choose for r, the left side doesn’t change. This must also be true
for the right-hand side. Although the quantity in the square brackets has r’s all
over the place, the whole quantity cannot depend on r, otherwise we wouldn’t
have an equation good for all r. As you can see, the bracket also does not depend
on θ or φ. It must be some constant. Its value may well depend on the l-value of
the state we are studying, since the function Fl must be the one appropriate to
that state; we’ll call the constant Kl. Equation (19.39) is therefore equivalent to
two equations:
1
sin θ
∂
∂θ

sin θ ∂Yl,m
∂θ

+
1
sin2 θ
∂2Yl,m
∂φ2
= −KlYl,m,
(19.40)
1
r
d2
dr2 (rFl) + 2m
ℏ2

E + e2
r

Fl = Kl
Fl
r2 .
(19.41)
Now look at what we’ve done. For any state described by l and m, we know
the functions Yl,m; we can use Eq. (19.40) to determine the constant Kl. Putting
Kl into Eq. (19.41) we have a diﬀerential equation for the function Fl(r). If we
can solve that equation for Fl(r), we have all of the pieces to put into (19.37) to
give ψ(r).
What is Kl? First, notice that it must be the same for all m (which go with
a particular l), so we can pick any m we want for Yl,m and plug it into (19.40)
to solve for Kl. Perhaps the easiest one to use is Yl,l. From Eq. (18.24),
Rz(φ) | l, l⟩= eilφ | l, l⟩.
(19.42)
The matrix element for Ry(θ) is also quite simple:
⟨l, 0 | Ry(θ) | l, l⟩= b (sin θ)l,
(19.43)
19-18","360"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","535","where b is some number.† Combining the two, we obtain
Yl,l ∝eilφ sinl θ.
(19.44)
Putting this function into (19.40) gives
Kl = l(l + 1).
(19.45)
Now that we have determined Kl, Eq. (19.41) tells us about the radial
function Fl(r). It is, of course, just the Schrödinger equation with the angular
part replaced by its equivalent KlFl/r2. Let’s rewrite (19.41) in the form we had
in Eq. (19.8), as follows:
1
r
d2
dr2 (rFl) = −2m
ℏ2

E + e2
r −l(l + 1)ℏ2
2mr2

Fl.
(19.46)
A mysterious term has been added to the potential energy. Although we got this
term by some mathematical shenanigan, it has a simple physical origin. We can
give you an idea about where it comes from in terms of a semi-classical argument.
Then perhaps you will not ﬁnd it quite so mysterious.
Think of a classical particle moving around some center of force. The total
energy is conserved and is the sum of the potential and kinetic energies
U = V (r) + 1
2mv2 = constant.
In general, v can be resolved into a radial component vr and a tangential
component r ˙θ; then
v2 = v2
r + (r ˙θ)2.
Now the angular momentum mr2 ˙θ is also conserved; say it is equal to L. We can
then write
mr2 ˙θ = L,
or
r ˙θ = L
mr,
† You can with some work show that this comes out of Eq. (18.35), but it is also easy to
work out from ﬁrst principles following the ideas of Section 18-4. A state | l, l⟩can be made out
of 2l spin one-half particles all with spins up; while the state | l, 0⟩would have l up and l down.
Under the rotation the amplitude that an up-spin remains up is cos θ/2, and that an up-spin
goes down is −sin θ/2. We are asking for the amplitude that l up-spins stay up, while the
other l up-spins go down. The amplitude for that is (−cos θ/2 sin θ/2)l which is proportional
to sinl θ.
19-19","349"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","536","and the energy is
U = 1
2mv2
r + V (r) +
L2
2mr2 .
If there were no angular momentum we would have just the ﬁrst two terms. Adding
the angular momentum L does to the energy just what adding a term L2/2mr2 to
the potential energy would do. But this is almost exactly the extra term in (19.46).
The only diﬀerence is that l(l + 1)ℏ2 appears for the angular momentum instead
of l2ℏ2 as we might expect. But we have seen before (for example, Volume II,
Section 34-7)† that this is just the substitution that is usually required to make
a quasi-classical argument agree with a correct quantum-mechanical calculation.
We can, then, understand the new term as a “pseudo-potential” which gives
the “centrifugal force” term that appears in the equations of radial motion for a
rotating system. (See the discussion of “pseudo-forces” in Volume I, Section 12-5.)
We are now ready to solve Eq. (19.46) for Fl(r). It is very much like Eq. (19.8),
so the same technique will work again. Everything goes as before until you get
to Eq. (19.19) which will have the additional term
−l(l + 1)
∞
X
k=1
akρk−2.
(19.47)
This term can also be written as
−l(l + 1)
a1
ρ +
∞
X
k=1
ak+1ρk−1

.
(19.48)
(We have taken out the ﬁrst term and then shifted the running index k down
by 1.) Instead of Eq. (19.20) we have
∞
X
k=1
[{k(k + 1) −l(l + 1)}ak+1 −2(αk −1)ak]ρk−1
−l(l + 1)a1
ρ
= 0.
(19.49)
There is only one term in ρ−1, so it must be zero. The coeﬃcient a1 must be zero
(unless l = 0 and we have our previous solution). Each of the other terms is made
† See Appendix to this volume.
19-20","302"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","537","zero by having the square bracket come out zero for every k. This condition
replaces Eq. (19.22) by
ak+1 =
2(αk −1)
k(k + 1) −l(l + 1) ak.
(19.50)
This is the only signiﬁcant change from the spherically symmetric case.
As before the series must terminate if we are to have solutions which can
represent bound electrons. The series will end at k = n if αn = 1. We get
again the same condition on α, that it must be equal to 1/n, where n is some
positive integer. However, Eq. (19.50) also gives a new restriction. The index k
cannot be equal to l, the denominator becomes zero and al+1 is inﬁnite. That
is, since a1 = 0, Eq. (19.50) implies that all successive ak are zero until we get
to al+1, which can be nonzero. This means that k must start at l + 1 and end
at n.
Our ﬁnal result is that for any l there are many possible solutions which we
can call Fn,l where n ≥l + 1. Each solution has the energy
En = −me4
2ℏ2
 1
n2

.
(19.51)
The wave function for the state of this energy with the angular quantum numbers
l and m is
ψn,l,m = a Yl,m(θ, φ)Fn,l(ρ),
(19.52)
with
ρFn,l(ρ) = e−αρ
n
X
k=l+1
akρk.
(19.53)
The coeﬃcients ak are obtained from (19.50). We have, ﬁnally, a complete
description of the states of a hydrogen atom.
19-5 The hydrogen wave functions
Let’s review what we have discovered. The states which satisfy Schrödinger’s
equation for an electron in a Coulomb ﬁeld are characterized by three quantum
numbers n, l, m, all integers. The angular distribution of the electron amplitude
can have only certain forms which we call Yl,m. They are labeled by l, the
19-21","302"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","538","quantum number of total angular momentum, and m, the “magnetic” quantum
number, which can range from −l to +l. For each angular conﬁguration, various
possible radial distributions Fn,l(r) of the electron amplitude are possible; they
are labeled by the principal quantum number n—which can range from l +1 to ∞.
The energy of the state depends only on n, and increases with increasing n.
The lowest energy, or ground, state is an s-state.
It has l = 0, n = 1,
and m = 0. It is a “nondegenerate” state—there is only one with this energy, and
its wave function is spherically symmetric. The amplitude to ﬁnd the electron is
a maximum at the center, and falls oﬀmonotonically with increasing distance
from the center. We can visualize the electron amplitude as a blob as shown in
Fig. 19-6(a).
There are other s-states with higher energies, for n = 2, 3, 4, . . . For each
energy there is only one version (m = 0), and they are all spherically symmetric.
These states have amplitudes which alternate in sign one or more times with
increasing r. There are n −1 spherical nodal surfaces—the places where ψ goes
through zero. The 2s-state (l = 0, n = 2), for example, will look as sketched
in Fig. 19-6(b). (The dark areas indicate regions where the amplitude is large,
and the plus and minus signs indicate the relative phases of the amplitude.) The
energy levels of the s-states are shown in the ﬁrst column of Fig. 19-7.
Then there are the p-states—with l = 1. For each n, which must be 2 or
greater, there are three states of the same energy, one each for m = +1, m = 0,
and m = −1. The energy levels are as shown in Fig. 19-7. The angular depend-
ences of these states are given in Table 19-1. For instance, for m = 0, if the
amplitude is positive for θ near zero, it will be negative for θ near 180◦. There is
a nodal plane coincident with the xy-plane. For n > 2 there are also spherical
nodes. The n = 2, m = 0 amplitude is sketched in Fig. 19-6(c), and the n = 3,
m = 0 wave function is sketched in Fig. 19-6(d).
You might think that since m represents a kind of “orientation” in space,
there should be similar distributions with the peaks of amplitude along the x-axis
or along the y-axis. Are these perhaps the m = +1 and m = −1 states? No.
But since we have three states with equal energies, any linear combinations of
the three will also be stationary states of the same energy. It turns out that the
“x”-state—which corresponds to the “z”-state, or m = 0 state, of Fig. 19-6(c)—is
a linear combination of the m = +1 and m = −1 states. The corresponding
“y”-state is another combination. Speciﬁcally, we mean that
“z” = | 1, 0⟩,
19-22","496"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","539","(a)
x
z
1s; m = 0
+
(b)
x
z
2s; m = 0
+
−
(c)
x
z
2p; m = 0
+
−
(d)
x
z
3p; m = 0
−
+
−
+
(e)
x
z
3d; m = 0
+
−
−
+
(f)
x
z
4d; m = 0
−
+
+
−
−
+
+
−
Fig. 19-6. Rough sketches showing the general nature of some of
the hydrogen wave functions.
The shaded regions show where the
amplitudes are large. The plus and minus signs show the relative sign of
the amplitude in each region.
19-23","103"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","540","√
E
0
−13.6 eV
ℓ= 0
1
2
3
s
p
d
f
n
1
2
3
4
5
6
8
10
1s
2s
3s
4s
2p
3p
4p
3d
4d
4f
And so on
Fig. 19-7. The energy level diagram for hydrogen.
“x” = −| 1, +1⟩−| 1, −1⟩
√
2
,
“y” = −| 1, +1⟩+ | 1, −1⟩
i
√
2
.
These states all look the same when referred to their particular axes.
The d-states (l = 2) have ﬁve possible values of m for each energy, the lowest
energy has n = 3. The levels go as shown in Fig. 19-7. The angular dependences
get more complicated. For instance the m = 0 states have two conical nodes, so
19-24","126"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","541","the wave function reverses phase from +, to −, to + as you go around from the
north pole to the south pole. The rough form of the amplitude is sketched in
(e) and (f) of Fig. 19-6 for the m = 0 states with n = 3 and n = 4. Again, the
larger n’s have spherical nodes.
We will not try to describe any more of the possible states. You will ﬁnd
the hydrogen wave functions described in more detail in many books. Two good
references are L. Pauling and E. B. Wilson, Introduction to Quantum Mechanics,
McGraw-Hill (1935); and R. B. Leighton, Principles of Modern Physics, McGraw-
Hill (1959). You will ﬁnd in them graphs of some of the functions and pictorial
representations of many states.
We would like to mention one particular feature of the wave functions for
higher l: for l > 0 the amplitudes are zero at the center. That is not surprising,
since it’s hard for an electron to have angular momentum when its radius arm
is very small. For this reason, the higher the l, the more the amplitudes are
“pushed away” from the center. If you look at the way the radial functions Fn,l(r)
vary for small r, you ﬁnd from (19.53) that
Fn,l(r) ≈rl.
Such a dependence on r means that for larger l’s you have to go farther from r = 0
before you get an appreciable amplitude. This behavior is, incidentally, deter-
mined by the centrifugal force term in the radial equation, so the same thing
will apply for any potential that varies slower than 1/r2 for small r—which most
atomic potentials do.
19-6 The periodic table
We would like now to apply the theory of the hydrogen atom in an approximate
way to get some understanding of the chemist’s periodic table of the elements.
For an element with atomic number Z there are Z electrons held together by
the electric attraction of the nucleus but with mutual repulsion of the electrons.
To get an exact solution we would have to solve Schrödinger’s equation for
Z electrons in a Coulomb ﬁeld. For helium the equation is
−ℏ
i
∂ψ
∂t = −ℏ2
2m (∇2
1ψ + ∇2
2ψ) +

−2e2
r1
−2e2
r2
+ e2
r12

ψ,
where ∇2
1 is a Laplacian which operates on r1, the coordinate of one electron;
∇2
2 operates on r2; and r12 = |r1 −r2|. (We are again neglecting the spin of the
19-25","415"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","542","electrons.) To ﬁnd the stationary states and energy levels we would have to ﬁnd
solutions of the form
ψ = f(r1, r2)e−(i/ℏ)Et.
The geometrical dependence is contained in f, which is a function of six variables—
the simultaneous positions of the two electrons. No one has found an analytic
solution, although solutions for the lowest energy states have been obtained by
numerical methods.
With 3, 4, or 5 electrons it is hopeless to try to obtain exact solutions, and it
is going too far to say that quantum mechanics has given a precise understanding
of the periodic table. It is possible, however, even with a sloppy approximation—
and some ﬁxing—to understand, at least qualitatively, many chemical properties
which show up in the periodic table.
The chemical properties of atoms are determined primarily by their lowest
energy states. We can use the following approximate theory to ﬁnd these states
and their energies. First, we neglect the electron spin, except that we adopt the
exclusion principle and say that any particular electronic state can be occupied
by only one electron. This means that any particular orbital conﬁguration can
have up to two electrons—one with spin up, the other with spin down. Next
we disregard the details of the interactions between the electrons in our ﬁrst
approximation, and say that each electron moves in a central ﬁeld which is the
combined ﬁeld of the nucleus and all the other electrons. For neon, which has
10 electrons, we say that one electron sees an average potential due to the nucleus
plus the other nine electrons. We imagine then that in the Schrödinger equation
for each electron we put a V (r) which is a 1/r ﬁeld modiﬁed by a spherically
symmetric charge density coming from the other electrons.
In this model each electron acts like an independent particle. The angular
dependence of its wave function will be just the same as the ones we had for the
hydrogen atom. There will be s-states, p-states, and so on; and they will have
the various possible m-values. Since V (r) no longer goes as 1/r, the radial part
of the wave functions will be somewhat diﬀerent, but it will be qualitatively the
same, so we will have the same radial quantum numbers, n. The energies of the
states will also be somewhat diﬀerent.
H
With these ideas, let’s see what we get. The ground state of hydrogen has
l = m = 0 and n = 1; we say the electron conﬁguration is 1s. The energy is
19-26","422"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","543","−13.6 eV. This means that it takes 13.6 electron volts to pull the electron oﬀ
the atom. We call this the “ionization energy”, WI. A large ionization energy
means that it is harder to pull the electron oﬀand, in general, that the material
is chemically less active.
He
Now take helium. Both electrons can be in the same lowest state (one spin up
and the other spin down). In this lowest state the electron moves in a potential
which is for small r like a Coulomb ﬁeld for z = 2 and for large r like a Coulomb
ﬁeld for z = 1. The result is a “hydrogen-like” 1s state with a somewhat lower
energy. Both electrons occupy identical 1s states (l = 0, m = 0). The observed
ionization energy (to remove one electron) is 24.6 electron volts. Since the
1s “shell” is now ﬁlled—we allow only two electrons—there is practically no
tendency for an electron to be attracted from another atom. Helium is chemically
inert.
Li
The lithium nucleus has a charge of 3. The electron states will again be
hydrogen-like, and the three electrons will occupy the lowest three energy levels.
Two will go into 1s states and the third will go into an n = 2 state. But with
l = 0 or l = 1? In hydrogen these states have the same energy, but in other
atoms they don’t, for the following reason. Remember that a 2s state has some
amplitude to be near the nucleus while the 2p state does not. That means that
a 2s electron will feel some of the triple electric charge of the Li nucleus, but
that a 2p electron will stay out where the ﬁeld looks like the Coulomb ﬁeld of a
single charge. The extra attraction lowers the energy of the 2s state relative to
the 2p state. The energy levels will be roughly as shown in Fig. 19-8—which you
should compare with the corresponding diagram for hydrogen in Fig. 19-7. So
the lithium atom will have two electrons in 1s states and one in a 2s. Since the
2s electron has a higher energy than a 1s electron it is relatively easily removed.
The ionization energy of lithium is only 5.4 electron volts, and it is quite active
chemically.
So you can see the patterns which develop; we have given in Table 19-2 a
list of the ﬁrst 36 elements, showing the states occupied by the electrons in the
ground state of each atom. The Table gives the ionization energy for the most
loosely bound electron, and the number of electrons occupying each “shell”—by
which we mean states with the same n. Since the diﬀerent l-states have diﬀerent
19-27","452"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","544","√
E
0
s
p
d
f
n
6
3
2
1
1s
2s
2p
3s
3p
3d
4s
4p
4d
4f
Fig. 19-8. Schematic energy level diagram for an atomic electron with
other electrons present. (The scale is not the same as Fig. 19-7.)
energies, each l-value corresponds to a sub-shell of 2(2l + 1) possible states (of
diﬀerent m and electron spin). These all have the same energy—except for some
very small eﬀects we are neglecting.
19-28","79"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","545","Table 19-2
The electron conﬁgurations of the ﬁrst 36 elements
Z
Element
WI(eV)
Electron Conﬁguration
1s
2s
2p
3s
3p
3d
4s
4p
4d
4f
1
H
hydrogen
13.6
1
2
He helium
24.6
2
3
Li
lithium
5.4
1
4
Be beryllium
9.3
2
5
B
boron
8.3
2
1
6
C
carbon
11.3
FILLED
2
2
Number of electrons
7
N
nitrogen
14.5
(2)
2
3
in each state
8
O
oxygen
13.6
2
4
9
F
ﬂuorine
17.4
2
5
10
Ne neon
21.6
2
6
11
Na sodium
5.1
1
12
Mg magnesium
7.6
2
13
Al aluminum
6.0
2
1
14
Si
silicon
8.1
—FILLED—
2
2
15
P
phosphorus
10.5
2
3
16
S
sulfur
10.4
(2)
(8)
2
4
17
Cl
chlorine
13.0
2
5
18
Ar argon
15.8
2
6
19
K
potassium
4.3
1
20
Ca calcium
6.1
2
21
Sc
scandium
6.5
1
2
22
Ti
titanium
6.8
2
2
23
V
vanadium
6.7
——FILLED——
3
2
24
Cr chromium
6.8
5
1
25
Mn manganese
7.4
(2)
(8)
(8)
5
2
26
Fe
iron
7.9
6
2
27
Co cobalt
7.9
7
2
28
Ni nickel
7.6
8
2
29
Cu copper
7.7
10
1
30
Zn zinc
9.4
10
2
31
Ga gallium
6.0
2
1
32
Ge germanium
7.9
——FILLED——
2
2
33
As arsenic
9.8
2
3
34
Se
selenium
9.7
(2)
(8)
(18)
2
4
35
Br bromine
11.8
2
5
36
Kr krypton
14.0
2
6
19-29","253"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","546","Be
Beryllium is like lithium except that it has two electrons in the 2s state as
well as two in the ﬁlled 1s shell.
B to Ne
Boron has 5 electrons. The ﬁfth must go into a 2p state. There are 2 × 3 = 6
diﬀerent 2p states, so we can keep adding electrons until we get to a total of 8.
This takes us to neon. As we add these electrons we are also increasing Z, so the
whole electron distribution gets pulled in closer and closer to the nucleus and the
energy of the 2p states goes down. By the time we get to neon the ionization
energy is up to 21.6 electron volts. Neon does not easily give up an electron.
Also there are no more low-energy slots to be ﬁlled, so it won’t try to grab an
extra electron. Neon is chemically inert. Fluorine, on the other hand, does have
an empty position where an electron can drop into a state of low energy, so it is
quite active in chemical reactions.
Na to Ar
With sodium the eleventh electron must start a new shell—going into a
3s state. The energy level of this state is much higher; the ionization energy
jumps down; and sodium is an active chemical. From sodium to argon the s and
p states with n = 3 are occupied in exactly the same sequence as for lithium
to neon. Angular conﬁgurations of the electrons in the outer unﬁlled shell have
the same sequence, and the progression of ionization energies is quite similar.
You can see why the chemical properties repeat with increasing atomic number.
Magnesium acts chemically much like beryllium, silicon like carbon, and chlorine
like ﬂuorine. Argon is inert like neon.
You may have noticed that there is a slight peculiarity in the sequence of
ionization energies between lithium and neon, and a similar one between sodium
and argon. The last electron is bound to the oxygen atom somewhat less than we
might expect. And sulfur is similar. Why should that be? We can understand it
if we put in just a little bit of the eﬀects of the interactions between individual
electrons. Think of what happens when we put the ﬁrst 2p electron onto the
boron atom. It has six possibilities—three possible p-states, each with two spins.
Imagine that the electron goes with spin up into the m = 0 state, which we have
also called the “z” state because it hugs the z-axis. Now what will happen in
carbon? There are now two 2p electrons. If one of them goes into the “z” state,
19-30","438"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","547","where will the second one go? It will have lower energy if it stays away from the
ﬁrst electron, which it can do by going into, say, the “x” state of the 2p shell. (This
state is, remember, just a linear combination of the m = +1 and m = −1 states.)
Next, when we go to nitrogen, the three 2p electrons will have the lowest energy
of mutual repulsion if they go one each into the “x,” “y,” and “z” conﬁgurations.
For oxygen, however, the jig is up. The fourth electron must go into one of the
ﬁlled states—with opposite spin. It is strongly repelled by the electron already
in that state, so its energy will not be as low as it might otherwise be, and it is
more easily removed. That explains the break in the sequence of binding energies
which appears between nitrogen and oxygen, and between phosphorus and sulfur.
K to Zn
After argon, you would, at ﬁrst, think that the new electrons would start to
ﬁll up the 3d states. But they don’t. As we described earlier—and illustrated in
Fig. 19-8—the higher angular momentum states get pushed up in energy. By the
time we get to the 3d states they are pushed to an energy a little bit above the
energy of the 4s state. So in potassium the last electron goes into the 4s state.
After this shell is ﬁlled (with two electrons) at calcium, the 3d states begin to be
ﬁlled for scandium, titanium, and vanadium.
The energies of the 3d and 4s states are so close together that small eﬀects
can shift the balance either way. By the time we get to put four electrons into
the 3d states, their repulsion raises the energy of the 4s state just enough that its
energy is slightly above the 3d energy, so one electron shifts over. For chromium
we don’t get a 4, 2 combination as we would have expected, but instead a 5,
1 combination. The new electron added to get manganese ﬁlls up the 4s shell
again, and the states of the 3d shell are then occupied one by one until we reach
copper.
Since the outermost shell of manganese, iron, cobalt, and nickel have the
same conﬁgurations, however, they all tend to have similar chemical properties.
(This eﬀect is much more pronounced in the rare-earth elements which all have
the same outer shell but a progressively ﬁlling inner shell which has much less
inﬂuence on their chemical properties.)
In copper an electron is robbed from the 4s shell, ﬁnally completing the
3d shell. The energy of the 10, 1 combination is, however, so close to the 9, 2
conﬁguration for copper that just the presence of another atom nearby can shift
the balance. For this reason the two last electrons of copper are nearly equivalent,
and copper can have a valence of either 1 or 2. (It sometimes acts as though its
19-31","492"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","548","electrons were in the 9, 2 combination.) Similar things happen at other places
and account for the fact that other metals, such as iron, combine chemically with
either of two valences. By zinc, both the 3d and 4s shells are ﬁlled once and for
all.
Ga to Kr
From gallium to krypton the sequence proceeds normally again, ﬁlling the
4p shell. The outer shells, the energies, and the chemical properties repeat the
pattern of boron to neon and aluminum to argon.
Krypton, like argon and neon, is known as “noble” gas. All three are chemically
“inert.” This means only that, having ﬁlled shells of relatively low energy, there
are few situations in which there is an energy advantage for them to join in
a simple combination with other elements. Having a ﬁlled shell is not enough.
Beryllium and magnesium have ﬁlled s-shells, but the energy of these shells is
too high to lead to stability. Similarly, one would have expected another “noble”
element at nickel, if the energy of the 3d shell had been lower (or the 4s higher).
On the other hand, krypton is not completely inert; it will form a weakly-bound
compound with chlorine.
Since our sample has turned up most of the main features of the periodic
table, we stop our examination at element number 36—there are still seventy or
so more!
We would like to bring up only one more point—that we not only can
understand the valences to some extent but also can say something about the
directional properties of the chemical bonds. Take an atom like oxygen which has
four 2p electrons. The ﬁrst three go into “x,” “y,” and “z” states and the fourth
will double one of these states, leaving two—say “x” and “y”—vacant. Consider
then what happens in H2O. Each of the two hydrogens are willing to share an
electron with the oxygen, helping the oxygen to ﬁll a shell. These electrons will
tend to go into the “x” and “y” vacancies. So the water molecule should have
the two hydrogen atoms making a right angle with respect to the center of the
oxygen. The angle is actually 105◦. We can even understand why the angle
is larger than 90◦. In sharing their electrons the hydrogens end up with a net
positive charge. The electric repulsion “strains” the wave functions and pushes
the angle out to 105◦. The same situation occurs in H2S. But because the sulfur
atom is larger, the two hydrogen atoms are farther apart, there is less repulsion,
and the angle is only pushed out to about 93◦. Selenium is even larger, so in
H2Se the angle is very nearly 90◦.
19-32","444"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","549","We can use the same arguments to understand the geometry of ammonia,
H3N. Nitrogen has room for three more 2p electrons, one each for the “x,” “y,”
and “z” type states. The three hydrogens should join on at right angles to each
other. The angles come out a little larger than 90◦—again from the electric
repulsion—but at least we see why the molecule of H3N is not ﬂat. The angles
in phosphene, H3P, are close to 90◦, and in H3As are still closer. We assumed
that NH3 was not ﬂat when we described it as a two-state system. And the
nonﬂatness is what makes the ammonia maser possible. Now we see that also
that shape can be understood from our quantum mechanics.
The Schrödinger equation has been one of the great triumphs of physics. By
providing the key to the underlying machinery of atomic structure it has given
an explanation for atomic spectra, for chemistry, and for the nature of matter.
19-33","162"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","550","20
Operators
20-1 Operations and operators
All the things we have done so far in quantum mechanics could be handled
with ordinary algebra, although we did from time to time show you some special
ways of writing quantum-mechanical quantities and equations. We would like
now to talk some more about some interesting and useful mathematical ways
of describing quantum-mechanical things. There are many ways of approaching
the subject of quantum mechanics, and most books use a diﬀerent approach
from the one we have taken. As you go on to read other books you might not
see right away the connections of what you will ﬁnd in them to what we have
been doing. Although we will also be able to get a few useful results, the main
purpose of this chapter is to tell you about some of the diﬀerent ways of writing
the same physics. Knowing them you should be able to understand better what
other people are saying. When people were ﬁrst working out classical mechanics
they always wrote all the equations in terms of x-, y-, and z-components. Then
someone came along and pointed out that all of the writing could be made much
simpler by inventing the vector notation. It’s true that when you come down
to ﬁguring something out you often have to convert the vectors back to their
components. But it’s generally much easier to see what’s going on when you
work with vectors and also easier to do many of the calculations. In quantum
mechanics we were able to write many things in a simpler way by using the idea
of the “state vector.” The state vector | ψ⟩has, of course, nothing to do with
geometric vectors in three dimensions but is an abstract symbol that stands for a
physical state, identiﬁed by the “label,” or “name,” ψ. The idea is useful because
the laws of quantum mechanics can be written as algebraic equations in terms of
these symbols. For instance, our fundamental law that any state can be made up
from a linear combination of base states is written as
| ψ⟩=
X
i
Ci | i⟩,
(20.1)
20-1","357"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","551","where the Ci are a set of ordinary (complex) numbers—the amplitudes Ci =
⟨i | ψ⟩—while | 1⟩, | 2⟩, | 3⟩, and so on, stand for the base states in some base, or
representation.
If you take some physical state and do something to it—like rotating it, or
like waiting for the time ∆t—you get a diﬀerent state. We say, “performing an
operation on a state produces a new state.” We can express the same idea by an
equation:
| φ⟩= ˆA | ψ⟩.
(20.2)
An operation on a state produces another state. The operator ˆA stands for some
particular operation. When this operation is performed on any state, say | ψ⟩, it
produces some other state | φ⟩.
What does Eq. (20.2) mean? We deﬁne it this way. If you multiply the
equation by ⟨i | and expand | ψ⟩according to Eq. (20.1), you get
⟨i | φ⟩=
X
j
⟨i | ˆA | j⟩⟨j | ψ⟩.
(20.3)
(The states | j⟩are from the same set as | i⟩.) This is now just an algebraic
equation. The numbers ⟨i | φ⟩give the amount of each base state you will ﬁnd
in | φ⟩, and it is given in terms of a linear superposition of the amplitudes ⟨j | ψ⟩
that you ﬁnd | ψ⟩in each base state. The numbers ⟨i | ˆA | j⟩are just the coeﬃcients
which tell how much of ⟨j | ψ⟩goes into each sum. The operator ˆA is described
numerically by the set of numbers, or “matrix,”
Aij ≡⟨i | ˆA | j⟩.
(20.4)
So Eq. (20.2) is a high-class way of writing Eq. (20.3). Actually it is a little
more than that; something more is implied. In Eq. (20.2) we do not make any
reference to a set of base states. Equation (20.3) is an image of Eq. (20.2) in
terms of some set of base states. But, as you know, you may use any set you
wish. And this idea is implied in Eq. (20.2). The operator way of writing avoids
making any particular choice. Of course, when you want to get deﬁnite you
have to choose some set. When you make your choice, you use Eq. (20.3). So
the operator equation (20.2) is a more abstract way of writing the algebraic
equation (20.3). It’s similar to the diﬀerence between writing
c = a × b
20-2","395"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","552","instead of
cx = aybz −azby,
cy = azbx −axbz,
cz = axby −aybx.
The ﬁrst way is much handier. When you want results, however, you will eventually
have to give the components with respect to some set of axes. Similarly, if you
want to be able to say what you really mean by ˆA, you will have to be ready to
give the matrix Aij in terms of some set of base states. So long as you have in
mind some set | i⟩, Eq. (20.2) means just the same as Eq. (20.3). (You should
remember also that once you know a matrix for one particular set of base states
you can always calculate the corresponding matrix that goes with any other base.
You can transform the matrix from one “representation” to another.)
The operator equation in (20.2) also allows a new way of thinking. If we
imagine some operator ˆA, we can use it with any state | ψ⟩to create a new
state ˆA | ψ⟩. Sometimes a “state” we get this way may be very peculiar—it may
not represent any physical situation we are likely to encounter in nature. (For
instance, we may get a state that is not normalized to represent one electron.)
In other words, we may at times get “states” that are mathematically artiﬁcial.
Such artiﬁcial “states” may still be useful, perhaps as the mid-point of some
calculation.
We have already shown you many examples of quantum-mechanical operators.
We have had the rotation operator ˆRy(θ) which takes a state | ψ⟩and produces a
new state, which is the old state as seen in a rotated coordinate system. We have
had the parity (or inversion) operator ˆP, which makes a new state by reversing
all coordinates. We have had the operators ˆσx, ˆσy, and ˆσz for spin one-half
particles.
The operator ˆJz was deﬁned in Chapter 17 in terms of the rotation operator
for a small angle ϵ.
ˆRz(ϵ) = 1 + i
ℏϵ ˆJz.
(20.5)
This just means, of course, that
ˆRz(ϵ) | ψ⟩= | ψ⟩+ i
ℏϵ ˆJz | ψ⟩.
(20.6)
In this example, ˆJz | ψ⟩is ℏ/iϵ times the state you get if you rotate | ψ⟩by the
20-3","367"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","553","small angle ϵ and then subtract the original state. It represents a “state” which
is the diﬀerence of two states.
One more example. We had an operator ˆpx—called the momentum operator
(x-component) deﬁned in an equation like (20.6). If ˆDx(L) is the operator which
displaces a state along x by the distance L, then ˆpx is deﬁned by
ˆDx(δ) = 1 + i
ℏδˆpx,
(20.7)
where δ is a small displacement. Displacing the state | ψ⟩along x by a small
distance δ gives a new state | ψ′⟩. We are saying that this new state is the old
state plus a small new piece
i
ℏδˆpx | ψ⟩.
The operators we are talking about work on a state vector like | ψ⟩, which
is an abstract description of a physical situation. They are quite diﬀerent from
algebraic operators which work on mathematical functions. For instance, d/dx is
an “operator” that works on f(x) by changing it to a new function f ′(x) = df/dx.
Another example is the algebraic operator ∇2. You can see why the same word is
used in both cases, but you should keep in mind that the two kinds of operators
are diﬀerent. A quantum-mechanical operator ˆA does not work on an algebraic
function, but on a state vector like | ψ⟩. Both kinds of operators are used in
quantum mechanics and often in similar kinds of equations, as you will see a little
later. When you are ﬁrst learning the subject it is well to keep the distinction
always in mind. Later on, when you are more familiar with the subject, you
will ﬁnd that it is less important to keep any sharp distinction between the two
kinds of operators. You will, indeed, ﬁnd that most books generally use the same
notation for both!
We’ll go on now and look at some useful things you can do with operators.
But ﬁrst, one special remark. Suppose we have an operator ˆA whose matrix in
some base is Aij ≡⟨i | ˆA | j⟩. The amplitude that the state ˆA | ψ⟩is also in some
other state | φ⟩is ⟨φ | ˆA | ψ⟩. Is there some meaning to the complex conjugate of
this amplitude? You should be able to show that
⟨φ | ˆA | ψ⟩∗= ⟨ψ | ˆA† | φ⟩,
(20.8)
where ˆA† (read “A dagger”) is an operator whose matrix elements are
A†
ij = (Aji)∗.
(20.9)
20-4","404"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","554","To get the i, j element of A† you go to the j, i element of ˆA (the indexes are
reversed) and take its complex conjugate. The amplitude that the state ˆA† | φ⟩
is in | ψ⟩is the complex conjugate of the amplitude that ˆA | ψ⟩is in | φ⟩. The
operator ˆA† is called the “Hermitian adjoint” of ˆA. Many important operators of
quantum mechanics have the special property that when you take the Hermitian
adjoint, you get the same operator back. If ˆB is such an operator, then
ˆB† = ˆB,
and it is called a “self-adjoint” or “Hermitian,” operator.
20-2 Average energies
So far we have reminded you mainly of what you already know. Now we
would like to discuss a new question. How would you ﬁnd the average energy of a
system—say, an atom? If an atom is in a particular state of deﬁnite energy and
you measure the energy, you will ﬁnd a certain energy E. If you keep repeating
the measurement on each one of a whole series of atoms which are all selected
to be in the same state, all the measurements will give E, and the “average” of
your measurements will, of course, be just E.
Now, however, what happens if you make the measurement on some state | ψ⟩
which is not a stationary state? Since the system does not have a deﬁnite energy,
one measurement would give one energy, the same measurement on another atom
in the same state would give a diﬀerent energy, and so on. What would you get
for the average of a whole series of energy measurements?
We can answer the question by projecting the state | ψ⟩onto the set of states
of deﬁnite energy. To remind you that this is a special base set, we’ll call the
states | ηi⟩. Each of the states | ηi⟩has a deﬁnite energy Ei. In this representation,
| ψ⟩=
X
i
Ci | ηi⟩.
(20.10)
When you make an energy measurement and get some number Ei, you have
found that the system was in the state ηi. But you may get a diﬀerent number
for each measurement. Sometimes you will get E1, sometimes E2, sometimes E3,
and so on. The probability that you observe the energy E1 is just the probability
of ﬁnding the system in the state | η1⟩, which is, of course, just the absolute
20-5","400"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","555","square of the amplitude C1 = ⟨η1 | ψ⟩. The probability of ﬁnding each of the
possible energies Ei is
Pi = |Ci|2.
(20.11)
How are these probabilities related to the mean value of a whole sequence of
energy measurements? Let’s imagine that we get a series of measurements like
this: E1, E7, E11, E9, E1, E10, E7, E2, E3, E9, E6, E4, and so on. We continue
for, say, a thousand measurements. When we are ﬁnished we add all the energies
and divide by one thousand. That’s what we mean by the average. There’s also
a short-cut to adding all the numbers. You can count up how many times you
get E1, say that is N1, and then count up the number of times you get E2, call
that N2, and so on. The sum of all the energies is certainly just
N1E1 + N2E2 + N3E3 + · · · =
X
i
NiEi.
The average energy is this sum divided by the total number of measurements
which is just the sum of all the Ni’s, which we can call N;
Eav =
P
i NiEi
N
.
(20.12)
We are almost there. What we mean by the probability of something hap-
pening is just the number of times we expect it to happen divided by the total
number of tries. The ratio Ni/N should—for large N—be very near to Pi, the
probability of ﬁnding the state | ηi⟩, although it will not be exactly Pi because
of the statistical ﬂuctuations. Let’s write the predicted (or “expected”) average
energy as ⟨E⟩av; then we can say that
⟨E⟩av =
X
i
PiEi.
(20.13)
The same arguments apply for any measurement. The average value of a measured
quantity A should be equal to
⟨A⟩av =
X
i
PiAi,
where Ai are the various possible values of the observed quantity, and Pi is the
probability of getting that value.
20-6","319"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","556","Let’s go back to our quantum-mechanical state | ψ⟩. It’s average energy is
⟨E⟩av =
X
i
|Ci|2Ei =
X
i
C∗
i CiEi.
(20.14)
Now watch this trickery! First, we write the sum as
X
i
⟨ψ | ηi⟩Ei⟨ηi | ψ⟩.
(20.15)
Next we treat the left-hand ⟨ψ | as a common “factor.” We can take this factor
out of the sum, and write it as
⟨ψ |
X
i
| ηi⟩Ei⟨ηi | ψ⟩

.
This expression has the form
⟨ψ | φ⟩,
where | φ⟩is some “cooked-up” state deﬁned by
| φ⟩=
X
i
| ηi⟩Ei⟨ηi | ψ⟩.
(20.16)
It is, in other words, the state you get if you take each base state | ηi⟩in the
amount Ei⟨ηi | ψ⟩.
Now remember what we mean by the states | ηi⟩. They are supposed to be
the stationary states—by which we mean that for each one,
ˆH | ηi⟩= Ei | ηi⟩.
Since Ei is just a number, the right-hand side is the same as | ηi⟩Ei, and the sum
in Eq. (20.16) is the same as
X
i
ˆH | ηi⟩⟨ηi | ψ⟩.
Now i appears only in the famous combination that contracts to unity, so
X
i
ˆH | ηi⟩⟨ηi | ψ⟩= ˆH
X
i
| ηi⟩⟨ηi | ψ⟩= ˆH | ψ⟩.
20-7","218"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","557","Magic! Equation (20.16) is the same as
| φ⟩= ˆH | ψ⟩.
(20.17)
The average energy of the state | ψ⟩can be written very prettily as
⟨E⟩av = ⟨ψ | ˆH | ψ⟩.
(20.18)
To get the average energy you operate on | ψ⟩with ˆH, and then multiply by ⟨ψ |.
A simple result.
Our new formula for the average energy is not only pretty. It is also useful,
because now we don’t need to say anything about any particular set of base
states. We don’t even have to know all of the possible energy levels. When we go
to calculate, we’ll need to describe our state in terms of some set of base states,
but if we know the Hamiltonian matrix Hij for that set we can get the average
energy. Equation (20.18) says that for any set of base states | i⟩, the average
energy can be calculated from
⟨E⟩av =
X
ij
⟨ψ | i⟩⟨i | ˆH | j⟩⟨j | ψ⟩,
(20.19)
where the amplitudes ⟨i | ˆH | j⟩are just the elements of the matrix Hij.
Let’s check this result for the special case that the states | i⟩are the deﬁnite
energy states. For them, ˆH | j⟩= Ej | j⟩, so ⟨i | ˆH | j⟩= Ej δij and
⟨E⟩av =
X
ij
⟨ψ | i⟩Ejδij⟨j | ψ⟩=
X
i
Ei⟨ψ | i⟩⟨i | ψ⟩,
which is right.
Equation (20.19) can, incidentally, be extended to other physical measure-
ments which you can express as an operator. For instance, ˆLz is the operator of
the z-component of the angular momentum L. The average of the z-component
for the state | ψ⟩is
⟨Lz⟩av = ⟨ψ | ˆLz | ψ⟩.
One way to prove it is to think of some situation in which the energy is proportional
to the angular momentum. Then all the arguments go through in the same way.
20-8","314"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","558","In summary, if a physical observable A is related to a suitable quantum-
mechanical operator ˆA, the average value of A for the state | ψ⟩is given by
⟨A⟩av = ⟨ψ | ˆA | ψ⟩.
(20.20)
By this we mean that
⟨A⟩av = ⟨ψ | φ⟩,
(20.21)
with
| φ⟩= ˆA | ψ⟩.
(20.22)
20-3 The average energy of an atom
Suppose we want the average energy of an atom in a state described by a wave
function ψ(r); How do we ﬁnd it? Let’s ﬁrst think of a one-dimensional situation
with a state | ψ⟩deﬁned by the amplitude ⟨x | ψ⟩= ψ(x). We are asking for the
special case of Eq. (20.19) applied to the coordinate representation. Following
our usual procedure, we replace the states | i⟩and | j⟩by | x⟩and | x′⟩, and change
the sums to integrals. We get
⟨E⟩av =
Z Z
⟨ψ | x⟩⟨x | ˆH | x′⟩⟨x′ | ψ⟩dx dx′.
(20.23)
This integral can, if we wish, be written in the following way:
Z
⟨ψ | x⟩⟨x | φ⟩dx,
(20.24)
with
⟨x | φ⟩=
Z
⟨x | ˆH | x′⟩⟨x′ | ψ⟩dx′.
(20.25)
The integral over x′ in (20.25) is the same one we had in Chapter 16—see
Eq. (16.50) and Eq. (16.52)—and is equal to
−ℏ2
2m
d2
dx2 ψ(x) + V (x)ψ(x).
We can therefore write
⟨x | φ⟩=

−ℏ2
2m
d2
dx2 + V (x)

ψ(x).
(20.26)
20-9","239"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","559","Remember that ⟨ψ | x⟩= ⟨x | ψ⟩∗= ψ∗(x); using this equality, the average
energy in Eq. (20.23) can be written as
⟨E⟩av =
Z
ψ∗(x)

−ℏ2
2m
d2
dx2 + V (x)

ψ(x) dx.
(20.27)
Given a wave function ψ(x), you can get the average energy by doing this integral.
You can begin to see how we can go back and forth from the state-vector ideas
to the wave-function ideas.
The quantity in the braces of Eq. (20.27) is an algebraic operator.† We will
write it as ˆH
ˆH = −ℏ2
2m
d2
dx2 + V (x).
With this notation Eq. (20.23) becomes
⟨E⟩av =
Z
ψ∗(x) ˆHψ(x) dx.
(20.28)
The algebraic operator ˆH deﬁned here is, of course, not identical to the
quantum-mechanical operator ˆH. The new operator works on a function of
position ψ(x) = ⟨x | ψ⟩to give a new function of x, φ(x) = ⟨x | φ⟩; while ˆH
operates on a state vector | ψ⟩to give another state vector | φ⟩, without implying
the coordinate representation or any particular representation at all. Nor is ˆH
strictly the same as ˆH even in the coordinate representation.
If we choose
to work in the coordinate representation, we would interpret ˆH in terms of
a matrix ⟨x | ˆH | x′⟩which depends somehow on the two “indices” x and x′;
that is, we expect—according to Eq. (20.25)—that ⟨x | φ⟩is related to all the
amplitudes ⟨x | ψ⟩by an integration. On the other hand, we ﬁnd that ˆH is a
diﬀerential operator. We have already worked out in Section 16-5 the connection
between ⟨x | ˆH | x′⟩and the algebraic operator ˆH.
We should make one qualiﬁcation on our results. We have been assuming
that the amplitude ψ(x) = ⟨x | ψ⟩is normalized. By this we mean that the scale
has been chosen so that
Z
|ψ(x)|2 dx = 1;
† The “operator” V (x) means “multiply by V (x).”
20-10","326"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","560","so the probability of ﬁnding the electron somewhere is unity. If you should choose
to work with a ψ(x) which is not normalized you should write
⟨E⟩av =
R
ψ∗(x) ˆHψ(x) dx
R
ψ∗(x)ψ(x) dx .
(20.29)
It’s the same thing.
Notice the similarity in form between Eq. (20.28) and Eq. (20.18). These
two ways of writing the same result appear often when you work with the x-
representation. You can go from the ﬁrst form to the second with any ˆA which
is a local operator, where a local operator is one which in the integral
Z
⟨x | ˆA | x′⟩⟨x′ | ψ⟩dx′
can be written as ˆAψ(x), where ˆA is a diﬀerential algebraic operator. There are,
however, operators for which this is not true. For them you must work with the
basic equations in (20.21) and (20.22).
You can easily extend the derivation to three dimensions. The result is that†
⟨E⟩av =
Z
ψ∗(r) ˆHψ(r) dV,
(20.30)
with
ˆH = −ℏ2
2m ∇2 + V (r),
(20.31)
and with the understanding that
Z
|ψ|2 dV = 1.
(20.32)
The same equations can be extended to systems with several electrons in a fairly
obvious way, but we won’t bother to write down the results.
With Eq. (20.30) we can calculate the average energy of an atomic state even
without knowing its energy levels. All we need is the wave function. It’s an
important law. We’ll tell you about one interesting application. Suppose you
† We write dV ol for the element of volume. It is, of course, just dx dy dz, and the integral
goes from −∞to +∞in all three coordinates.
20-11","274"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","561","want to know the ground-state energy of some system—say the helium atom, but
it’s too hard to solve Schrödinger’s equation for the wave function, because there
are too many variables. Suppose, however, that you take a guess at the wave
function—pick any function you like—and calculate the average energy. That is,
you use Eq. (20.29)—generalized to three dimensions-to ﬁnd what the average
energy would be if the atom were really in the state described by this wave
function. This energy will certainly be higher than the ground-state energy which
is the lowest possible energy the atom can have.† Now pick another function and
calculate its average energy. If it is lower than your ﬁrst choice you are getting
closer to the true ground-state energy. If you keep on trying all sorts of artiﬁcial
states you will be able to get lower and lower energies, which come closer and
closer to the ground-state energy. If you are clever, you will try some functions
which have a few adjustable parameters. When you calculate the energy it will
be expressed in terms of these parameters. By varying the parameters to give
the lowest possible energy, you are trying out a whole class of functions at once.
Eventually you will ﬁnd that it is harder and harder to get lower energies and
you will begin to be convinced that you are fairly close to the lowest possible
energy. The helium atom has been solved in just this way—not by solving a
diﬀerential equation, but by making up a special function with a lot of adjustable
parameters which are eventually chosen to give the lowest possible value for the
average energy.
20-4 The position operator
What is the average value of the position of an electron in an atom? For any
particular state | ψ⟩what is the average value of the coordinate x? We’ll work in
one dimension and let you extend the ideas to three dimensions or to systems
with more than one particle: We have a state described by ψ(x), and we keep
measuring x over and over again. What is the average? It is
Z
xP(x) dx,
where P(x) dx is the probability of ﬁnding the electron in a little element dx at x.
Suppose the probability density P(x) varies with x as shown in Fig. 20-1. The
† You can also look at it this way. Any function (that is, state) you choose can be written as a
linear combination of the base states which are deﬁnite energy states. Since in this combination
there is a mixture of higher energy states in with the lowest energy state, the average energy
will be higher than the ground-state energy.
20-12","447"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","562","x
P (x)
Fig. 20-1. A curve of probability density representing a localized particle.
electron is most likely to be found near the peak of the curve. The average value
of x is also somewhere near the peak. It is, in fact, just the center of gravity of
the area under the curve.
We have seen earlier that P(x) is just |ψ(x)|2 = ψ∗(x)ψ(x), so we can write
the average of x as
⟨x⟩av =
Z
ψ∗(x)xψ(x) dx.
(20.33)
Our equation for ⟨x⟩av has the same form as Eq. (20.28). For the average
energy, the energy operator ˆH appears between the two ψ’s, for the average
position there is just x. (If you wish you can consider x to be the algebraic
operator “multiply by x.”) We can carry the parallelism still further, expressing
the average position in a form which corresponds to Eq. (20.18). Suppose we
just write
⟨x⟩av = ⟨ψ | α⟩
(20.34)
with
| α⟩= ˆx | ψ⟩,
(20.35)
and then see if we can ﬁnd the operator ˆx which generates the state | α⟩, which
will make Eq. (20.34) agree with Eq. (20.33). That is, we must ﬁnd a | α⟩, so
that
⟨ψ | α⟩= ⟨x⟩av =
Z
⟨ψ | x⟩x⟨x | ψ⟩dx.
(20.36)
20-13","210"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","563","First, let’s expand ⟨ψ | α⟩in the x-representation. It is
⟨ψ | α⟩=
Z
⟨ψ | x⟩⟨x | α⟩dx.
(20.37)
Now compare the integrals in the last two equations.
You see that in the
x-representation
⟨x | α⟩= x⟨x | ψ⟩.
(20.38)
Operating on | ψ⟩with ˆx to get | α⟩is equivalent to multiplying ψ(x) = ⟨x | ψ⟩by x
to get α(x) = ⟨x | α⟩. We have a deﬁnition of ˆx in the coordinate representation.†
[We have not bothered to try to get the x-representation of the matrix of the
operator ˆx. If you are ambitious you can try to show that
⟨x | ˆx | x′⟩= x δ(x −x′).
(20.39)
You can then work out the amusing result that
ˆx | x⟩= x | x⟩.
(20.40)
The operator ˆx has the interesting property that when it works on the base
states | x⟩it is equivalent to multiplying by x.]
Do you want to know the average value of x2? It is
⟨x2⟩av =
Z
ψ∗(x)x2ψ(x) dx.
(20.41)
Or, if you prefer you can write
⟨x2⟩av = ⟨ψ | α′⟩
with
| α′⟩= ˆx2 | ψ⟩.
(20.42)
By ˆx2 we mean ˆxˆx—the two operators are used one after the other. With the
second form you can calculate ⟨x2⟩av, using any representation (base-states) you
wish. If you want the average of xn, or of any polynomial in x, you can see how
to get it.
† Equation (20.38) does not mean that | α⟩= x | ψ⟩. You cannot “factor out” the ⟨x |,
because the multiplier x in front of ⟨x | ψ⟩is a number which is diﬀerent for each state ⟨x |. It
is the value of the coordinate of the electron in the state | x⟩. See Eq. (20.40).
20-14","294"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","564","20-5 The momentum operator
Now we would like to calculate the mean momentum of an electron—again,
we’ll stick to one dimension. Let P(p) dp be the probability that a measurement
will give a momentum between p and p + dp. Then
⟨p⟩av =
Z
p P(p) dp.
(20.43)
Now we let ⟨p | ψ⟩be the amplitude that the state | ψ⟩is in a deﬁnite momentum
state | p⟩. This is the same amplitude we called ⟨mom p | ψ⟩in Section 16-3 and
is a function of p just as ⟨x | ψ⟩is a function of x. There we chose to normalize
the amplitude so that
P(p) =
1
2πℏ|⟨p | ψ⟩|2.
(20.44)
We have, then,
⟨p⟩av =
Z
⟨ψ | p⟩p⟨p | ψ⟩dp
2πℏ.
(20.45)
The form is quite similar to what we had for ⟨x⟩av.
If we want, we can play exactly the same game we did with ⟨x⟩av. First, we
can write the integral above as
Z
⟨ψ | p⟩⟨p | β⟩dp
2πℏ.
(20.46)
You should now recognize this equation as just the expanded form of the ampli-
tude ⟨ψ | β⟩—expanded in terms of the base states of deﬁnite momentum. From
Eq. (20.45) the state | β⟩is deﬁned in the momentum representation by
⟨p | β⟩= p⟨p | ψ⟩
(20.47)
That is, we can now write
⟨p⟩av = ⟨ψ | β⟩
(20.48)
with
| β⟩= ˆp | ψ⟩,
(20.49)
where the operator ˆp is deﬁned in terms of the p-representation by Eq. (20.47).
20-15","247"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","565","[Again, you can if you wish show that the matrix form of ˆp is
⟨p | ˆp | p′⟩= p δ(p −p′),
(20.50)
and that
ˆp | p⟩= p | p⟩.
(20.51)
It works out the same as for x.]
Now comes an interesting question. We can write ⟨p⟩av, as we have done in Eqs.
(20.45) and (20.48), and we know the meaning of the operator ˆp in the momentum
representation. But how should we interpret ˆp in the coordinate representation?
That is what we will need to know if we have some wave function ψ(x), and
we want to compute its average momentum. Let’s make clear what we mean.
If we start by saying that ⟨p⟩av is given by Eq. (20.48), we can expand that
equation in terms of the p-representation to get back to Eq. (20.46). If we are
given the p-description of the state—namely the amplitude ⟨p | ψ⟩, which is an
algebraic function of the momentum p—we can get ⟨p | β⟩from Eq. (20.47) and
proceed to evaluate the integral. The question now is: What do we do if we
are given a description of the state in the x-representation, namely the wave
function ψ(x) = ⟨x | ψ⟩?
Well, let’s start by expanding Eq. (20.48) in the x-representation. It is
⟨p⟩av =
Z
⟨ψ | x⟩⟨x | β⟩dx.
(20.52)
Now, however, we need to know what the state | β⟩is in the x-representation.
If we can ﬁnd it, we can carry out the integral. So our problem is to ﬁnd the
function β(x) = ⟨x | β⟩.
We can ﬁnd it in the following way. In Section 16-3 we saw how ⟨p | β⟩was
related to ⟨x | β⟩. According to Eq. (16.24),
⟨p | β⟩=
Z
e−ipx/ℏ⟨x | β⟩dx.
(20.53)
If we know ⟨p | β⟩we can solve this equation for ⟨x | β⟩. What we want, of course,
is to express the result somehow in terms of ψ(x) = ⟨x | ψ⟩, which we are assuming
to be known. Suppose we start with Eq. (20.47) and again use Eq. (16.24) to
write
⟨p | β⟩= p⟨p | ψ⟩= p
Z
e−ipx/ℏψ(x) dx.
(20.54)
20-16","361"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","566","Since the integral is over x we can put the p inside the integral and write
⟨p | β⟩=
Z
e−ipx/ℏpψ(x) dx.
(20.55)
Compare this with (20.53). You would say that ⟨x | β⟩is equal to pψ(x). No, No!
The wave function ⟨x | β⟩= β(x) can depend only on x—not on p. That’s the
whole problem.
However, some ingenious fellow discovered that the integral in (20.55) could be
integrated by parts. The derivative of e−ipx/ℏwith respect to x is (−i/ℏ)pe−ipx/ℏ,
so the integral in (20.55) is equivalent to
−ℏ
i
Z
d
dx (e−ipx/ℏ)ψ(x) dx.
If we integrate by parts, it becomes
−ℏ
i

e−ipx/ℏψ(x)
+∞
−∞+ ℏ
i
Z
e−ipx/ℏdψ
dx dx.
So long as we are considering bound states, so that ψ(x) goes to zero at x = ±∞,
the bracket is zero and we have
⟨p | β⟩= ℏ
i
Z
e−ipx/ℏdψ
dx dx.
(20.56)
Now compare this result with Eq. (20.53). You see that
⟨x | β⟩= ℏ
i
d
dx ψ(x).
(20.57)
We have the necessary piece to be able to complete Eq. (20.52). The answer is
⟨p⟩av =
Z
ψ∗(x) ℏ
i
d
dx ψ(x) dx.
(20.58)
We have found how Eq. (20.48) looks in the coordinate representation.
Now you should begin to see an interesting pattern developing. When we
asked for the average energy of the state | ψ⟩we said it was
⟨E⟩av = ⟨ψ | φ⟩,
with
| φ⟩= ˆH | ψ⟩.
20-17","243"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","567","The same thing is written in the coordinate world as
⟨E⟩av =
Z
ψ∗(x)φ(x) dx
with
φ(x) = ˆHψ(x).
Here ˆH is an algebraic operator which works on a function of x. When we asked
about the average value of x, we found that it could also be written
⟨x⟩av = ⟨ψ | α⟩,
with
| α⟩= ˆx | ψ⟩.
In the coordinate world the corresponding equations are
⟨x⟩av =
Z
ψ∗(x)α(x) dx,
with
α(x) = xψ(x).
When we asked about the average value of p, we wrote
⟨p⟩av = ⟨ψ | β⟩,
with
| β⟩= ˆp | ψ⟩.
In the coordinate world the equivalent equations were
⟨p⟩av =
Z
ψ∗(x)β(x) dx,
with
β(x) = ℏ
i
d
dx ψ(x).
In each of our three examples we start with the state | ψ⟩and produce another
(hypothetical) state by a quantum-mechanical operator. In the coordinate repre-
sentation we generate the corresponding wave function by operating on the wave
function ψ(x) with an algebraic operator. There are the following one-to-one
correspondences (for one-dimensional problems):
ˆH →ˆH = −ℏ2
2m
d2
dx2 + V (x),
ˆx →x,
ˆpx →ˆPx = ℏ
i
∂
∂x.
(20.59)
In this list, we have introduced the symbol ˆPx for the algebraic operator (ℏ/i)∂/∂x:
ˆPx = ℏ
i
∂
∂x,
(20.60)
20-18","214"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","568","and we have inserted the x subscript on ˆP to remind you that we have been
working only with the x-component of momentum.
You can easily extend the results to three dimensions. For the other compo-
nents of the momentum,
ˆpy →ˆPy = ℏ
i
∂
∂y ,
ˆpz →ˆPz = ℏ
i
∂
∂z .
If you want, you can even think of an operator of the vector momentum and
write
ˆp →ˆP = ℏ
i

ex
∂
∂x + ey
∂
∂y + ez
∂
∂z

,
where ex, ey, and ez are the unit vectors in the three directions. It looks even
more elegant if we write
ˆp →ˆP = ℏ
i ∇.
(20.61)
Our general result is that for at least some quantum-mechanical operators,
there are corresponding algebraic operators in the coordinate representation. We
summarize our results so far—extended to three dimensions—in Table 20-1. For
each operator we have the two equivalent forms:†
| φ⟩= ˆA | ψ⟩
(20.62)
or
φ(r) = ˆAψ(r).
(20.63)
We will now give a few illustrations of the use of these ideas. The ﬁrst one is
just to point out the relation between ˆP and ˆH. If we use ˆPx twice, we get
ˆPxˆPx = −ℏ2 ∂2
∂x2 .
This means that we can write the equality
ˆH =
1
2m {ˆPxˆPx + ˆPy ˆPy + ˆPz ˆPz} + V (r).
† In many books the same symbol is used for ˆ
A and ˆA, because they both stand for the
same physics, and because it is convenient not to have to write diﬀerent kinds of letters. You
can usually tell which one is intended by the context.
20-19","280"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","569","Table 20-1
Physical Quantity
Operator
Coordinate Form
Energy
ˆH
ˆH = −ℏ2
2m ∇2 + V (r)
Position
ˆx
x
ˆy
y
ˆz
z
Momentum
ˆpx
ˆPx = ℏ
i
∂
∂x
ˆpy
ˆPy = ℏ
i
∂
∂y
ˆpz
ˆPz = ℏ
i
∂
∂z
Or, using the vector notation,
ˆH =
1
2m
ˆP · ˆP + V (r).
(20.64)
(In an algebraic operator, any term without the operator symbol (ˆ) means just
a straight multiplication.) This equation is nice because it’s easy to remember if
you haven’t forgotten your classical physics. Everyone knows that the energy is
(nonrelativistically) just the kinetic energy p2/2m plus the potential energy, and
ˆH is the operator of the total energy.
This result has impressed people so much that they try to teach students all
about classical physics before quantum mechanics. (We think diﬀerently!) But
such parallels are often misleading. For one thing, when you have operators, the
order of various factors is important; but that is not true for the factors in a
classical equation.
In Chapter 17 we deﬁned an operator ˆpx in terms of the displacement opera-
tor ˆDx by [see Eq. (17.27)]
| ψ′⟩= ˆDx(δ) | ψ⟩=

1 + i
ℏˆpxδ

| ψ⟩.
(20.65)
20-20","209"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","570","where δ is a small displacement. We should show you that this is equivalent to
our new deﬁnition. According to what we have just worked out, this equation
should mean the same as
ψ′(x) = ψ(x) + ∂ψ
∂x δ.
But the right-hand side is just the Taylor expansion of ψ(x+δ), which is certainly
what you get if you displace the state to the left by δ (or shift the coordinates to
the right by the same amount). Our two deﬁnitions of ˆp agree!
Let’s use this fact to show something else. Suppose we have a bunch of
particles which we label 1, 2, 3, . . . , in some complicated system. (To keep things
simple we’ll stick to one dimension.) The wave function describing the state is a
function of all the coordinates x1, x2, x3, . . . We can write it as ψ(x1, x2, x3, . . . ).
Now displace the system (to the left) by δ. The new wave function
ψ′(x1, x2, x3, . . . ) = ψ(x1 + δ, x2 + δ, x3 + δ, . . . )
can be written as
ψ′(x1, x2, x3, . . . ) = ψ(x1, x2, x3, . . . )
+

δ ∂ψ
∂x1
+ δ ∂ψ
∂x2
+ δ ∂ψ
∂x3
+ · · ·

.
(20.66)
According to Eq. (20.65) the operator of the momentum of the state | ψ⟩(let’s
call it the total momentum) is equal to
ˆPtotal = ℏ
i
 ∂
∂x1
+
∂
∂x2
+
∂
∂x3
+ · · ·

.
But this is just the same as
ˆPtotal = ˆPx1 + ˆPx2 + ˆPx3 + · · · .
(20.67)
The operators of momentum obey the rule that the total momentum is the sum
of the momenta of all the parts. Everything holds together nicely, and many of
the things we have been saying are consistent with each other.
20-21","327"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","571","20-6 Angular momentum
Let’s for fun look at another operation—the operation of orbital angular
momentum. In Chapter 17 we deﬁned an operator ˆJz in terms of ˆRz(φ), the
operator of a rotation by the angle φ about the z-axis. We consider here a
system described simply by a single wave function ψ(r), which is a function of
coordinates only, and does not take into account the fact that the electron may
have its spin either up or down. That is, we want for the moment to disregard
intrinsic angular momentum and think about only the orbital part. To keep the
distinction clear, we’ll call the orbital operator ˆLz, and deﬁne it in terms of the
operator of a rotation by an inﬁnitesimal angle ϵ by
ˆRz(ϵ) | ψ⟩=

1 + i
ℏϵ ˆLz

| ψ⟩.
(Remember, this deﬁnition applies only to a state | ψ⟩which has no internal
spin variables, but depends only on the coordinates r = x, y, z.) If we look at
the state | ψ⟩in a new coordinate system, rotated about the z-axis by the small
angle ϵ, we see a new state
| ψ′⟩= ˆRz(ϵ) | ψ⟩.
If we choose to describe the state | ψ⟩in the coordinate representation—that
is, by its wave function ψ(r), we would expect to be able to write
ψ′(r) =

1 + i
ℏϵ ˆLz

ψ(r).
(20.68)
What is ˆLz? Well, a point P at x and y in the new coordinate system (really
x′ and y′, but we will drop the primes) was formerly at x −ϵy and y + ϵx, as
you can see from Fig. 20-2. Since the amplitude for the electron to be at P isn’t
changed by the rotation of the coordinates we can write
ψ′(x, y, z) = ψ(x −ϵy, y + ϵx, z) = ψ(x, y, z) −ϵy ∂ψ
∂x + ϵx ∂ψ
∂y
(remembering that ϵ is a small angle). This means that
ˆLz = ℏ
i

x ∂
∂y −y ∂
∂x

.
(20.69)
20-22","340"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","572","x
y
x′
y ′
P
ϵ
ϵ
Fig. 20-2. Rotation of the axes around the z-axis by the small angle ϵ.
That’s our answer. But notice. It is equivalent to
ˆLz = xˆPy −yˆPx.
(20.70)
Returning to our quantum-mechanical operators, we can write
ˆLz = xˆpy −yˆpx.
(20.71)
This formula is easy to remember because it looks like the familiar formula of
classical mechanics; it is the z-component of
L = r × p.
(20.72)
One of the fun parts of this operator business is that many classical equations
get carried over into a quantum-mechanical form. Which ones don’t? There had
better be some that don’t come out right, because if everything did, then there
would be nothing diﬀerent about quantum mechanics. There would be no new
physics. Here is one equation which is diﬀerent. In classical physics
xpx −pxx = 0.
What is it in quantum mechanics?
ˆxˆpx −ˆpxˆx =?
20-23","153"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","573","Let’s work it out in the x-representation. So that we’ll know what we are doing
we put in some wave function ψ(x). We have
xˆPxψ(x) −ˆPxxψ(x),
or
x ℏ
i
∂
∂x ψ(x) −ℏ
i
∂
∂x xψ(x).
Remember now that the derivatives operate on everything to the right. We get
x ℏ
i
∂ψ
∂x −ℏ
i ψ(x) −ℏ
i x ∂ψ
∂x = −ℏ
i ψ(x).
(20.73)
The answer is not zero. The whole operation is equivalent simply to multiplication
by −ℏ/i:
ˆxˆpx −ˆpxˆx = −ℏ
i .
(20.74)
If Planck’s constant were zero, the classical and quantum results would be the
same, and there would be no quantum mechanics to learn!
Incidentally, if any two operators ˆA and ˆB, when taken together like this:
ˆA ˆB −ˆB ˆA,
do not give zero, we say that “the operators do not commute.” And an equation
such as (20.74) is called a “commutation rule.” You can see that the commutation
rule for px and y is
ˆpxˆy −ˆyˆpx = 0.
There is another very important commutation rule that has to do with angular
momenta. It is
ˆLx ˆLy −ˆLy ˆLx = iℏˆLz.
(20.75)
You can get some practice with ˆx and ˆp operators by proving it for yourself.
It is interesting to notice that operators which do not commute can also
occur in classical physics. We have already seen this when we have talked about
rotation in space. If you rotate something, such as a book, by 90◦around x
and then 90◦around y, you get something diﬀerent from rotating ﬁrst by 90◦
around y and then by 90◦around x. It is, in fact, just this property of space
that is responsible for Eq. (20.75).
20-24","285"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","574","20-7 The change of averages with time
Now we want to show you something else. How do averages change with time?
Suppose for the moment that we have an operator ˆA, which does not itself have
time in it in any obvious way. We mean an operator like ˆx or ˆp. (We exclude
things like, say, the operator of some external potential that was being varied
with time, such as V (x, t).) Now suppose we calculate ⟨A⟩av, in some state | ψ⟩,
which is
⟨A⟩av = ⟨ψ | ˆA | ψ⟩.
(20.76)
How will ⟨A⟩av, depend on time? Why should it? One reason might be that the
operator itself depended explicitly on time—for instance, if it had to do with a
time-varying potential like V (x, t). But even if the operator does not depend on t,
say, for example, the operator ˆA = ˆx, the corresponding average may depend on
time. Certainly the average position of a particle could be moving. How does
such a motion come out of Eq. (20.76) if ˆA has no time dependence? Well, the
state | ψ⟩might be changing with time. For nonstationary states we have often
shown a time dependence explicitly by writing a state as | ψ(t)⟩. We want to
show that the rate of change of ⟨A⟩av, is given by a new operator we will call ˆ˙A.
Remember that ˆA is an operator, so that putting a dot over the A does not here
mean taking the time derivative, but is just a way of writing a new operator ˆ˙A
which is deﬁned by
d
dt ⟨A⟩av = ⟨ψ | ˆ˙A | ψ⟩.
(20.77)
Our problem is to ﬁnd the operator ˆ˙A.
First, we know that the rate of change of a state is given by the Hamiltonian.
Speciﬁcally,
iℏd
dt | ψ(t)⟩= ˆH | ψ(t)⟩.
(20.78)
This is just the abstract way of writing our original deﬁnition of the Hamiltonian:
iℏdCi
dt =
X
j
HijCj.
(20.79)
If we take the complex conjugate of Eq. (20.78), it is equivalent to
−iℏd
dt ⟨ψ(t) | = ⟨ψ(t) | ˆH.
(20.80)
20-25","351"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","575","Next, see what happens if we take the derivatives with respect to t of Eq. (20.76).
Since each ψ depends on t, we have
d
dt ⟨A⟩av =
 d
dt ⟨ψ |

ˆA | ψ⟩+ ⟨ψ | ˆA
 d
dt | ψ⟩

.
(20.81)
Finally, using the two equations in (20.78) and (20.80) to replace the derivatives,
we get
d
dt ⟨A⟩av = i
ℏ{⟨ψ | ˆH ˆA | ψ⟩−⟨ψ | ˆA ˆH | ψ⟩}.
This equation is the same as
d
dt ⟨A⟩av = i
ℏ⟨ψ | ˆH ˆA −ˆA ˆH | ψ⟩.
Comparing this equation with Eq. (20.77), you see that
ˆ˙A = i
ℏ( ˆH ˆA −ˆA ˆH).
(20.82)
That is our interesting proposition, and it is true for any operator ˆA.
Incidentally, if the operator ˆA should itself be time dependent, we would have
had
ˆ˙A = i
ℏ( ˆH ˆA −ˆA ˆH) + ∂ˆA
∂t .
(20.83)
Let us try out Eq. (20.82) on some example to see whether it really makes
sense. For instance, what operator corresponds to ˆ˙x? We say it should be
ˆ˙x = i
ℏ( ˆHˆx −ˆx ˆH).
(20.84)
What is this? One way to ﬁnd out is to work it through in the coordinate
representation using the algebraic operator for ˆH. In this representation the
commutator is
ˆHx −x ˆH =

−ℏ2
2m
d2
dx2 + V (x)

x −x

−ℏ2
2m
d2
dx2 + V (x)

.
If you operate with this or any wave function ψ(x) and work out all of the
derivatives where you can, you end up after a little work with
−ℏ2
m
dψ
dx .
20-26","280"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","576","But this is just the same as
−i ℏ
m
ˆPxψ,
so we ﬁnd that
ˆHˆx −ˆx ˆH = −i ℏ
m ˆpx
(20.85)
or that
ˆ˙x = ˆpx
m .
(20.86)
A pretty result. It means that if the mean value of x is changing with time the
drift of the center of gravity is the same as the mean momentum divided by m.
Exactly like classical mechanics.
Another example. What is the rate of change of the average momentum of a
state? Same game. Its operator is
ˆ˙p = i
ℏ( ˆH ˆp −ˆp ˆH).
(20.87)
Again you can work it out in the x representation. Remember that ˆp becomes d/dx,
and this means that you will be taking the derivative of the potential energy V
(in the ˆH)—but only in the second term. It turns out that it is the only term
which does not cancel, and you ﬁnd that
ˆHˆP −ˆP ˆH = iℏdV
dx
or that
ˆ˙p = −dV
dx .
(20.88)
Again the classical result. The right-hand side is the force, so we have derived
Newton’s law! But remember—these are the laws for the operators which give
the average quantities. They do not describe what goes on in detail inside an
atom.
Quantum mechanics has the essential diﬀerence that ˆpˆx is not equal to ˆxˆp.
They diﬀer by a little bit—by the small number iℏ. But the whole wondrous
complications of interference, waves, and all, result from the little fact that
ˆxˆp −ˆpˆx is not quite zero.
The history of this idea is also interesting. Within a period of a few months in
1926, Heisenberg and Schrödinger independently found correct laws to describe
20-27","280"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","577","atomic mechanics. Schrödinger invented his wave function ψ(x) and found his
equation. Heisenberg, on the other hand, found that nature could be described by
classical equations, except that xp−px should be equal to iℏ, which he could make
happen by deﬁning them in terms of special kinds of matrices. In our language
he was using the energy-representation, with its matrices. Both Heisenberg’s
matrix algebra and Schrödinger’s diﬀerential equation explained the hydrogen
atom. A few months later Schrödinger was able to show that the two theories
were equivalent—as we have seen here. But the two diﬀerent mathematical forms
of quantum mechanics were discovered independently.
20-28","104"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","578","21
The Schrödinger Equation in a Classical
Context: A Seminar on Superconductivity
21-1 Schrödinger’s equation in a magnetic ﬁeld
This lecture is only for entertainment. I would like to give the lecture in
a somewhat diﬀerent style—just to see how it works out. It’s not a part of
the course—in the sense that it is not supposed to be a last minute eﬀort to
teach you something new. But, rather, I imagine that I’m giving a seminar or
research report on the subject to a more advanced audience, to people who have
already been educated in quantum mechanics. The main diﬀerence between a
seminar and a regular lecture is that the seminar speaker does not carry out all
the steps, or all the algebra. He says: “If you do such and such, this is what
comes out,” instead of showing all of the details. So in this lecture I’ll describe
the ideas all the way along but just give you the results of the computations. You
should realize that you’re not supposed to understand everything immediately,
but believe (more or less) that things would come out if you went through the
steps.
All that aside, this is a subject I want to talk about. It is recent and modern
and would be a perfectly legitimate talk to give at a research seminar. My subject
is the Schrödinger equation in a classical setting—the case of superconductivity.
Ordinarily, the wave function which appears in the Schrödinger equation
applies to only one or two particles. And the wave function itself is not something
that has a classical meaning—unlike the electric ﬁeld, or the vector potential,
or things of that kind. The wave function for a single particle is a “ﬁeld”—in
the sense that it is a function of position—but it does not generally have a
classical signiﬁcance. Nevertheless, there are some situations in which a quantum
mechanical wave function does have classical signiﬁcance, and they are the ones
I would like to take up. The peculiar quantum mechanical behavior of matter
on a small scale doesn’t usually make itself felt on a large scale except in the
21-1","355"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","579","standard way that it produces Newton’s laws—the laws of the so-called classical
mechanics. But there are certain situations in which the peculiarities of quantum
mechanics can come out in a special way on a large scale.
At low temperatures, when the energy of a system has been reduced very,
very low, instead of a large number of states being involved, only a very, very
small number of states near the ground state are involved. Under those circum-
stances the quantum mechanical character of that ground state can appear on a
macroscopic scale. It is the purpose of this lecture to show a connection between
quantum mechanics and large-scale eﬀects—not the usual discussion of the way
that quantum mechanics reproduces Newtonian mechanics on the average, but a
special situation in which quantum mechanics will produce its own characteristic
eﬀects on a large or “macroscopic” scale.
I will begin by reminding you of some of the properties of the Schrödinger
equation.† I want to describe the behavior of a particle in a magnetic ﬁeld
using the Schrödinger equation, because the superconductive phenomena are
involved with magnetic ﬁelds. An external magnetic ﬁeld is described by a vector
potential, and the problem is: what are the laws of quantum mechanics in a
vector potential? The principle that describes the behavior of quantum mechanics
in a vector potential is very simple. The amplitude that a particle goes from one
place to another along a certain route when there’s a ﬁeld present is the same
as the amplitude that it would go along the same route when there’s no ﬁeld,
multiplied by the exponential of the line integral of the vector potential, times
the electric charge divided by Planck’s constant1 (see Fig. 21-1):
⟨b | a⟩in A = ⟨b | a⟩A=0 · exp
iq
ℏ
Z b
a
A · ds

.
(21.1)
It is a basic statement of quantum mechanics.
Now without the vector potential the Schrödinger equation of a charged
particle (nonrelativistic, no spin) is
−ℏ
i
∂ψ
∂t = ˆHψ =
1
2m
ℏ
i ∇

·
ℏ
i ∇

ψ + qφψ,
(21.2)
† I’m not really reminding you, because I haven’t shown you some of these equations before;
but remember the spirit of this seminar.
1 Volume II, Section 15-5.
21-2","383"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","580","a
b
Γ
Fig. 21-1. The amplitude to go from a to b along the path Γ is
proportional to exp

(iq/ℏ)
R b
a A · ds

.
where φ is the electric potential so that qφ is the potential energy.† Equation (21.1)
is equivalent to the statement that in a magnetic ﬁeld the gradients in the
Hamiltonian are replaced in each case by the gradient minus qA, so that Eq. (21.2)
becomes
−ℏ
i
∂ψ
∂t = ˆHψ =
1
2m
ℏ
i ∇−qA

·
ℏ
i ∇−qA

ψ + qφψ,
(21.3)
This is the Schrödinger equation for a particle with charge q moving in an
electromagnetic ﬁeld A, φ (nonrelativistic, no spin).
To show that this is true I’d like to illustrate by a simple example in which
instead of having a continuous situation we have a line of atoms along the x-axis
with the spacing b and we have an amplitude −K for an electron to jump from
one atom to another when there is no ﬁeld.‡ Now according to Eq. (21.1) if
there’s a vector potential in the x-direction Ax(x, t), the amplitude to jump will
be altered from what it was before by a factor exp[(iq/ℏ) Axb], the exponent
being iq/ℏtimes the vector potential integrated from one atom to the next.
For simplicity we will write (q/ℏ)Ax ≡f(x), since Ax, will, in general, depend
on x. If the amplitude to ﬁnd the electron at the atom “n” located at x is
called C(x) ≡Cn, then the rate of change of that amplitude is given by the
following equation:
−ℏ
i
∂
∂t C(x) = E0C(x) −Ke−ibf(x+b/2)C(x + b)
−Ke+ibf(x−b/2)C(x −b).
(21.4)
† Not to be confused with our earlier use of φ for a state label!
‡ K is the same quantity that was called A in the problem of a linear lattice with no magnetic
ﬁeld. See Chapter 13.
21-3","321"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","581","There are three pieces. First, there’s some energy E0 if the electron is located
at x. As usual, that gives the term E0C(x). Next, there is the term −KC(x + b),
which is the amplitude for the electron to have jumped backwards one step from
atom “n + 1,” located at x + b. However, in doing so in a vector potential, the
phase of the amplitude must be shifted according to the rule in Eq. (21.1). If Ax
is not changing appreciably in one atomic spacing, the integral can be written as
just the value of Ax at the midpoint, times the spacing b. So (iq/ℏ) times the
integral is just ibf(x + b/2). Since the electron is jumping backwards, I showed
this phase shift with a minus sign. That gives the second piece. In the same
manner there’s a certain amplitude to have jumped from the other side, but this
time we need the vector potential at a distance (b/2) on the other side of x, times
the distance b. That gives the third piece. The sum gives the equation for the
amplitude to be at x in a vector potential.
Now we know that if the function C(x) is smooth enough (we take the long
wavelength limit), and if we let the atoms get closer together, Eq. (21.4) will
approach the behavior of an electron in free space. So the next step is to expand
the right-hand side of (21.4) in powers of b, assuming b is very small.
For
example, if b is zero the right-hand side is just (E0 −2K)C(x), so in the zeroth
approximation the energy is E0 −2K. Next comes the terms in b. But because
the two exponentials have opposite signs, only even powers of b remain. So if
you make a Taylor expansion of C(x), of f(x), and of the exponentials, and then
collect the terms in b2, you get
−ℏ
i
∂C(x)
∂t
= E0C(x) −2KC(x)
−Kb2{C′′(x) −2if(x)C′(x) −if ′(x)C(x) −f 2(x)C(x)}. (21.5)
(The “primes” mean diﬀerentiation with respect to x.)
Now this horrible combination of things looks quite complicated. But mathe-
matically it’s exactly the same as
−ℏ
i
∂C(x)
∂t
= (E0 −2K)C(x) −Kb2
 ∂
∂x −if(x)
 ∂
∂x −if(x)

C(x). (21.6)
The second bracket operating on C(x) gives C′(x) plus if(x)C(x). The ﬁrst
bracket operating on these two terms gives the C′′ term and terms in the ﬁrst
derivative of f(x) and the ﬁrst derivative of C(x). Now remember that the
21-4","418"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","582","solutions for zero magnetic ﬁeld2 represent a particle with an eﬀective mass meﬀ
given by
Kb2 =
ℏ2
2meﬀ
.
If you then set E0 = −2K, and put back f(x) = (q/ℏ)Ax, you can easily check
that Eq. (21.6) is the same as the ﬁrst part of Eq. (21.3). (The origin of the
potential energy term is well known, so I haven’t bothered to include it in this
discussion.) The proposition of Eq. (21.1) that the vector potential changes all the
amplitudes by the exponential factor is the same as the rule that the momentum
operator, (ℏ/i)∇gets replaced by
ℏ
i ∇−qA,
as you see in the Schrödinger equation of (21.3).
21-2 The equation of continuity for probabilities
Now I turn to a second point. An important part of the Schrödinger equation
for a single particle is the idea that the probability to ﬁnd the particle at a position
is given by the absolute square of the wave function. It is also characteristic of
the quantum mechanics that probability is conserved in a local sense. When the
probability of ﬁnding the electron somewhere decreases, while the probability of
the electron being elsewhere increases (keeping the total probability unchanged),
something must be going on in between. In other words, the electron has a
continuity in the sense that if the probability decreases at one place and builds
up at another place, there must be some kind of ﬂow between. If you put a
wall, for example, in the way, it will have an inﬂuence and the probabilities will
not be the same. So the conservation of probability alone is not the complete
statement of the conservation law, just as the conservation of energy alone is
not as deep and important as the local conservation of energy.3 If energy is
disappearing, there must be a ﬂow of energy to correspond. In the same way, we
would like to ﬁnd a “current” of probability such that if there is any change in
the probability density (the probability of being found in a unit volume), it can
be considered as coming from an inﬂow or an outﬂow due to some current. This
2 Section 13-3.
3 Volume II, Section 27-1.
21-5","368"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","583","current would be a vector which could be interpreted this way—the x-component
would be the net probability per second and per unit area that a particle passes
in the x-direction across a plane parallel to the yz-plane. Passage toward +x
is considered a positive ﬂow, and passage in the opposite direction, a negative
ﬂow.
Is there such a current? Well, you know that the probability density P(r, t)
is given in terms of the wave function by
P(r, t) = ψ∗(r, t)ψ(r, t).
(21.7)
I am asking: Is there a current J such that
∂P
∂t = −∇· J?
(21.8)
If I take the time derivative of Eq. (21.7), I get two terms:
∂P
∂t = ψ∗∂ψ
∂t + ψ ∂ψ∗
∂t .
(21.9)
Now use the Schrödinger equation—Eq. (21.3)—for ∂ψ/∂t; and take the complex
conjugate of it to get ∂ψ∗/∂t—each i gets its sign reversed. You get
∂P
∂t = −i
ℏ

ψ∗1
2m
ℏ
i ∇−qA

·
ℏ
i ∇−qA

ψ + qφψ∗ψ
−ψ 1
2m

−ℏ
i ∇−qA

·

−ℏ
i ∇−qA

ψ∗−qφψψ∗

.
(21.10)
The potential terms and a lot of other stuﬀcancel out. And it turns out that
what is left can indeed be written as a perfect divergence. The whole equation is
equivalent to
∂P
∂t = −∇·
 1
2m ψ∗
ℏ
i ∇−qA

ψ + 1
2m ψ

−ℏ
i ∇−qA

ψ∗

.
(21.11)
It is really not as complicated as it seems. It is a symmetrical combination of ψ∗
times a certain operation on ψ, plus ψ times the complex conjugate operation
on ψ∗. It is some quantity plus its own complex conjugate, so the whole thing is
21-6","290"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","584","real—as it ought to be. The operation can be remembered this way: it is just
the momentum operator ˆP minus qA. I could write the current in Eq. (21.8) as
J = 1
2

ψ∗
 ˆP −qA
m

ψ + ψ
 ˆP −qA
m
∗
ψ∗

.
(21.12)
There is then a current J which completes Eq. (21.8).
Equation (21.11) shows that the probability is conserved locally. If a particle
disappears from one region it cannot appear in another without something going
on in between. Imagine that the ﬁrst region is surrounded by a closed surface
far enough out that there is zero probability to ﬁnd the electron at the surface.
The total probability to ﬁnd the electron somewhere inside the surface is the
volume integral of P. But according to Gauss’s theorem the volume integral of
the divergence J is equal to the surface integral of J. If ψ is zero at the surface,
Eq. (21.12) says that J is zero, so the total probability to ﬁnd the particle inside
can’t change. Only if some of the probability approaches the boundary can
some of it leak out. We can say that it only gets out by moving through the
surface—and that is local conservation.
21-3 Two kinds of momentum
The equation for the current is rather interesting, and sometimes causes a
certain amount of worry. You would think the current would be something like
the density of particles times the velocity. The density should be something
like ψψ∗, which is o.k. And each term in Eq. (21.12) looks like the typical form
for the average-value of the operator
ˆP −qA
m
,
(21.13)
so maybe we should think of it as the velocity of ﬂow. It looks as though we
have two suggestions for relations of velocity to momentum, because we would
also think that momentum divided by mass, ˆP/m, should be a velocity. The two
possibilities diﬀer by the vector potential.
It happens that these two possibilities were also discovered in classical physics,
when it was found that momentum could be deﬁned in two ways.4 One of them is
4 See, for example, J. D. Jackson, Classical Electrodynamics, John Wiley and Sons, Inc.,
New York (1962), p. 408.
21-7","376"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","585","called “kinematic momentum,” but for absolute clarity I will in this lecture call
it the “mv-momentum.” This is the momentum obtained by multiplying mass
by velocity. The other is a more mathematical, more abstract momentum, some
times called the “dynamical momentum,” which I’ll call “p-momentum.” The two
possibilities are
mv-momentum = mv,
(21.14)
p-momentum = mv + qA.
(21.15)
It turns out that in quantum mechanics with magnetic ﬁelds it is the p-momentum
which is connected to the gradient operator ˆP, so it follows that (21.13) is the
operator of a velocity.
I’d like to make a brief digression to show you what this is all about—why
there must be something like Eq. (21.15) in the quantum mechanics. The wave
function changes with time according to the Schrödinger equation in Eq. (21.3).
If I would suddenly change the vector potential, the wave function wouldn’t
change at the ﬁrst instant; only its rate of change changes. Now think of what
would happen in the following circumstance. Suppose I have a long solenoid, in
which I can produce a ﬂux of magnetic ﬁeld (B-ﬁeld), as shown in Fig. 21-2. And
there is a charged particle sitting nearby. Suppose this ﬂux nearly instantaneously
builds up from zero to something. I start with zero vector potential and then I
turn on a vector potential. That means that I produce suddenly a circumferential
vector potential A. You’ll remember that the line integral of A around a loop is
the same as the ﬂux of B through the loop.5 Now what happens if I suddenly
turn on a vector potential? According to the quantum mechanical equation the
sudden change of A does not make a sudden change of ψ; the wave function is
still the same. So the gradient is also unchanged.
But remember what happens electrically when I suddenly turn on a ﬂux.
During the short time that the ﬂux is rising, there’s an electric ﬁeld generated
whose line integral is the rate of change of the ﬂux with time:
E = −∂A
∂t .
(21.16)
That electric ﬁeld is enormous if the ﬂux is changing rapidly, and it gives a
force on the particle. The force is the charge times the electric ﬁeld, and so
5 Volume II, Chapter 14, Section 14-1.
21-8","379"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","586","I
I
B
B
E
q
Fig. 21-2. The electric ﬁeld outside a solenoid with an increasing
current.
during the build up of the ﬂux the particle obtains a total impulse (that is, a
change in mv) equal to −qA. In other words, if you suddenly turn on a vector
potential at a charge, this charge immediately picks up an mv-momentum equal
to −qA. But there is something that isn’t changed immediately and that’s the
diﬀerence between mv and −qA. And so the sum p = mv + qA is something
which is not changed when you make a sudden change in the vector potential.
This quantity p is what we have called the p-momentum and is of importance in
classical mechanics in the theory of dynamics, but it also has a direct signiﬁcance
in quantum mechanics. It depends on the character of the wave function, and it
is the one to be identiﬁed with the operator
ˆP = ℏ
i ∇.
21-9","163"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","587","21-4 The meaning of the wave function
When Schrödinger ﬁrst discovered his equation he discovered the conservation
law of Eq. (21.8) as a consequence of his equation. But he imagined incorrectly
that P was the electric charge density of the electron and that J was the electric
current density, so he thought that the electrons interacted with the electromag-
netic ﬁeld through these charges and currents. When he solved his equations
for the hydrogen atom and calculated ψ, he wasn’t calculating the probability
of anything—there were no amplitudes at that time—the interpretation was
completely diﬀerent. The atomic nucleus was stationary but there were currents
moving around; the charges P and currents J would generate electromagnetic
ﬁelds and the thing would radiate light. He soon found on doing a number of
problems that it didn’t work out quite right. It was at this point that Born
made an essential contribution to our ideas regarding quantum mechanics. It
was Born who correctly (as far as we know) interpreted the ψ of the Schrödinger
equation in terms of a probability amplitude—that very diﬃcult idea that the
square of the amplitude is not the charge density but is only the probability per
unit volume of ﬁnding an electron there, and that when you do ﬁnd the electron
some place the entire charge is there. That whole idea is due to Born.
The wave function ψ(r) for an electron in an atom does not, then, describe a
smeared-out electron with a smooth charge density. The electron is either here,
or there, or somewhere else, but wherever it is, it is a point charge. On the other
hand, think of a situation in which there are an enormous number of particles in
exactly the same state, a very large number of them with exactly the same wave
function. Then what? One of them is here and one of them is there, and the
probability of ﬁnding any one of them at a given place is proportional to ψψ∗.
But since there are so many particles, if I look in any volume dx dy dz I will
generally ﬁnd a number close to ψψ∗dx dy dz. So in a situation in which ψ is
the wave function for each of an enormous number of particles which are all in
the same state, ψψ∗can be interpreted as the density of particles. If, under
these circumstances, each particle carries the same charge q, we can, in fact, go
further and interpret ψ∗ψ as the density of electricity. Normally, ψψ∗is given
the dimensions of a probability density, then ψ should be multiplied by q to give
the dimensions of a charge density. For our present purposes we can put this
constant factor into ψ, and take ψψ∗itself as the electric charge density. With
this understanding, J (the current of probability I have calculated) becomes
directly the electric current density.
21-10","480"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","588","So in the situation in which we can have very many particles in exactly the
same state, there is possible a new physical interpretation of the wave functions.
The charge density and the electric current can be calculated directly from the
wave functions and the wave functions take on a physical meaning which extends
into classical, macroscopic situations.
Something similar can happen with neutral particles. When we have the
wave function of a single photon, it is the amplitude to ﬁnd a photon somewhere.
Although we haven’t ever written it down there is an equation for the photon
wave function analogous to the Schrödinger equation for the electron. The photon
equation is just the same as Maxwell’s equations for the electromagnetic ﬁeld,
and the wave function is the same as the vector potential A. The wave function
turns out to be just the vector potential. The quantum physics is the same thing
as the classical physics because photons are noninteracting Bose particles and
many of them can be in the same state—as you know, they like to be in the
same state. The moment that you have billions in the same state (that is, in the
same electromagnetic wave), you can measure the wave function, which is the
vector potential, directly. Of course, it worked historically the other way. The
ﬁrst observations were on situations with many photons in the same state, and
so we were able to discover the correct equation for a single photon by observing
directly with our hands on a macroscopic level the nature of wave function.
Now the trouble with the electron is that you cannot put more than one in
the same state. Therefore, it was long believed that the wave function of the
Schrödinger equation would never have a macroscopic representation analogous
to the macroscopic representation of the amplitude for photons. On the other
hand, it is now realized that the phenomena of superconductivity presents us
with just this situation.
21-5 Superconductivity
As you know, very many metals become superconducting below a certain
temperature6—the temperature is diﬀerent for diﬀerent metals. When you reduce
the temperature suﬃciently the metals conduct electricity without any resistance.
This phenomenon has been observed for a very large number of metals but not
for all, and the theory of this phenomenon has caused a great deal of diﬃculty. It
6 First discovered by Kamerlingh-Onnes in 1911; H. Kamerlingh-Onnes, Comm. Phys. Lab.,
Univ. Leyden, Nos. 119, 120, 122 (1911). You will ﬁnd a nice up-to-date discussion of the
subject in E. A. Lynton, Superconductivity, John Wiley and Sons, Inc., New York, 1962.
21-11","433"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","589","took a very long time to understand what was going on inside of superconductors,
and I will only describe enough of it for our present purposes. It turns out that
due to the interactions of the electrons with the vibrations of the atoms in the
lattice, there is a small net eﬀective attraction between the electrons. The result
is that the electrons form together, if I may speak very qualitatively and crudely,
bound pairs.
Now you know that a single electron is a Fermi particle. But a bound pair
would act as a Bose particle, because if I exchange both electrons in a pair I
change the sign of the wave function twice, and that means that I don’t change
anything. A pair is a Bose particle.
The energy of pairing—that is, the net attraction—is very, very weak. Only a
tiny temperature is needed to throw the electrons apart by thermal agitation, and
convert them back to “normal” electrons. But when you make the temperature
suﬃciently low that they have to do their very best to get into the absolutely
lowest state; then they do collect in pairs.
I don’t wish you to imagine that the pairs are really held together very
closely like a point particle. As a matter of fact, one of the great diﬃculties of
understanding this phenomena originally was that that is not the way things
are. The two electrons which form the pair are really spread over a considerable
distance; and the mean distance between pairs is relatively smaller than the size
of a single pair. Several pairs are occupying the same space at the same time.
Both the reason why electrons in a metal form pairs and an estimate of the
energy given up in forming a pair have been a triumph of recent times. This
fundamental point in the theory of superconductivity was ﬁrst explained in the
theory of Bardeen, Cooper, and Schrieﬀer,7 but that is not the subject of this
seminar. We will accept, however, the idea that the electrons do, in some manner
or other, work in pairs, that we can think of these pairs as behaving more or less
like particles, and that we can therefore talk about the wave function for a “pair.”
Now the Schrödinger equation for the pair will be more or less like Eq. (21.3).
There will be one diﬀerence in that the charge q will be twice the charge of
an electron. Also, we don’t know the inertia—or eﬀective mass—for the pair
in the crystal lattice, so we don’t know what number to put in for m. Nor
should we think that if we go to very high frequencies (or short wavelengths),
this is exactly the right form, because the kinetic energy that corresponds to very
rapidly varying wave functions may be so great as to break up the pairs. At ﬁnite
7 J. Bardeen, L. N. Cooper, and J. R. Schrieﬀer, Phys. Rev. 108, 1175 (1957).
21-12","493"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","590","temperatures there are always a few pairs which are broken up according to the
usual Boltzmann theory. The probability that a pair is broken is proportional
to exp(−Epair/kT). The electrons that are not bound in pairs are called “normal”
electrons and will move around in the crystal in the ordinary way. I will, however,
consider only the situation at essentially zero temperature—or, in any case, I will
disregard the complications produced by those electrons which are not in pairs.
Since electron pairs are bosons, when there are a lot of them in a given state
there is an especially large amplitude for other pairs to go to the same state. So
nearly all of the pairs will be locked down at the lowest energy in exactly the
same state—it won’t be easy to get one of them into another state. There’s more
amplitude to go into the same state than into an unoccupied state by the famous
factor √n, where n −1 is the occupancy of the lowest state. So we would expect
all the pairs to be moving in the same state.
What then will our theory look like? I’ll call ψ the wave function of a pair
in the lowest energy state. However, since ψψ∗is going to be proportional to
the charge density ρ, I can just as well write ψ as the square root of the charge
density times some phase factor:
ψ(r) = ρ1/2(r)eiθ(r),
(21.17)
where ρ and θ are real functions of r. (Any complex function can, of course, be
written this way.) It’s clear what we mean when we talk about the charge density,
but what is the physical meaning of the phase θ of the wave function? Well, let’s
see what happens if we substitute ψ(r) into Eq. (21.12), and express the current
density in terms of these new variables ρ and θ. It’s just a change of variables
and I won’t go through all the algebra, but it comes out
J = ℏ
m

∇θ −q
ℏA

ρ.
(21.18)
Since both the current density and the charge density have a direct physical
meaning for the superconducting electron gas, both ρ and θ are real things. The
phase is just as observable as ρ; it is a piece of the current density J. The absolute
phase is not observable, but if the gradient of the phase is known everywhere,
the phase is known except for a constant. You can deﬁne the phase at one point,
and then the phase everywhere is determined.
Incidentally, the equation for the current can be analyzed a little nicer, when
you think that the current density J is in fact the charge density times the
21-13","450"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","591","velocity of motion of the ﬂuid of electrons, or ρv. Equation (21.18) is then
equivalent to
mv = ℏ∇θ −qA.
(21.19)
Notice that there are two pieces in the mv-momentum; one is a contribution
from the vector potential, and the other, a contribution from the behavior of the
wave function. In other words, the quantity ℏ∇θ is just what we have called the
p-momentum.
21-6 The Meissner eﬀect
Now we can describe some of the phenomena of superconductivity. First,
there is no electrical resistance. There’s no resistance because all the electrons
are collectively in the same state. In the ordinary ﬂow of current you knock one
electron or the other out of the regular ﬂow, gradually deteriorating the general
momentum. But here to get one electron away from what all the others are doing
is very hard because of the tendency of all Bose particles to go in the same state.
A current once started, just keeps on going forever.
It’s also easy to understand that if you have a piece of metal in the supercon-
ducting state and turn on a magnetic ﬁeld which isn’t too strong (we won’t go
into the details of how strong), the magnetic ﬁeld can’t penetrate the metal. If, as
you build up the magnetic ﬁeld, any of it were to build up inside the metal, there
would be a rate of change of ﬂux which would produce an electric ﬁeld, and an
electric ﬁeld would immediately generate a current which, by Lenz’s law, would
oppose the ﬂux. Since all the electrons will move together, an inﬁnitesimal electric
ﬁeld will generate enough current to oppose completely any applied magnetic ﬁeld.
So if you turn the ﬁeld on after you’ve cooled a metal to the superconducting
state, it will be excluded.
Even more interesting is a related phenomenon discovered experimentally by
Meissner.8 If you have a piece of the metal at a high temperature (so that it
is a normal conductor) and establish a magnetic ﬁeld through it, and then you
lower the temperature below the critical temperature (where the metal becomes
a superconductor), the ﬁeld is expelled. In other words, it starts up its own
current—and in just the right amount to push the ﬁeld out.
We can see the reason for that in the equations, and I’d like to explain how.
Suppose that we take a piece of superconducting material which is in one lump.
8 W. Meissner and R. Ochsenfeld, Naturwiss. 21, 787 (1933).
21-14","413"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","592","Then in a steady situation of any kind the divergence of the current must be
zero because there’s no place for it to go. It is convenient to choose to make the
divergence of A equal to zero. (I should explain why choosing this convention
doesn’t mean any loss of generality, but I don’t want to take the time.) Taking
the divergence of Eq. (21.18), then gives that the Laplacian of θ is equal to zero.
One moment. What about the variation of ρ? I forgot to mention an important
point. There is a background of positive charge in this metal due to the atomic
ions of the lattice. If the charge density ρ is uniform there is no net charge and
no electric ﬁeld. If there would be any accumulation of electrons in one region the
charge wouldn’t be neutralized and there would be a terriﬁc repulsion pushing the
electrons apart.† So in ordinary circumstances the charge density of the electrons
in the superconductor is almost perfectly uniform—I can take ρ as a constant.
Now the only way that ∇2θ can be zero everywhere inside the lump of metal is
for θ to be a constant. And that means that there is no contribution to J from
p-momentum. Equation (21.18) then says that the current is proportional to ρ
times A. So everywhere in a lump of superconducting material the current is
necessarily proportional to the vector potential:
J = −ρ q
m A.
(21.20)
Since ρ and q have the same (negative) sign, and since ρ is a constant, I can
set −ρq/m = −(some positive constant); then
J = −(some positive constant)A.
(21.21)
This equation was originally proposed by London and London9 to explain the
experimental observations of superconductivity—long before the quantum me-
chanical origin of the eﬀect was understood.
Now we can use Eq. (21.20) in the equations of electromagnetism to solve for
the ﬁelds. The vector potential is related to the current density by
∇2A = −1
ϵ0c2 J.
(21.22)
† Actually if the electric ﬁeld were too strong, pairs would be broken up and the “normal”
electrons created would move in to help neutralize any excess of positive charge. Still, it takes
energy to make these normal electrons, so the main point is that a nearly uniform density ρ is
highly favored energetically.
9 F. London and H. London, Proc. Roy. Soc. (London) A149, 71 (1935); Physica 2, 341
(1935).
21-15","407"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","593","If I use Eq. (21.21) for J, I have
∇2A = λ2A,
(21.23)
where λ2 is just a new constant;
λ2 = ρ
q
ϵ0mc2 .
(21.24)
We can now try to solve this equation for A and see what happens in detail.
For example, in one dimension Eq. (21.23) has exponential solutions of the form
e−λx and e+λx. These solutions mean that the vector potential must decrease
exponentially as you go from the surface into the material. (It can’t increase
because there would be a blow up.) If the piece of metal is very large compared
to 1/λ, the ﬁeld only penetrates to a thin layer at the surface—a layer about 1/λ
in thickness. The entire remainder of the interior is free of ﬁeld, as sketched in
Fig. 21-3. This is the explanation of the Meissner eﬀect.
How big is the distance λ? Well, remember that r0, the “electromagnetic
radius” of the electron (2.8 × 10−13 cm), is given by
mc2 =
q2
e
4πϵ0r0
.
Also, remember that q in Eq. (21.24) is twice the charge of an electron, so
q2
ϵ0mc2 = 8πr0
qe
.
Writing ρ as qeN, where N is the number of electrons per cubic centimeter, we
have
λ2 = 8πNr0.
(21.25)
For a metal such as lead there are about 3 × 1022 atoms per cm3, so if each one
contributed only one conduction electron, 1/λ would be about 2×10−6 cm. That
gives you the order of magnitude.
21-7 Flux quantization
The London equation (21.21) was proposed to account for the observed facts
of superconductivity including the Meissner eﬀect. In recent times, however,
21-16","271"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","594","(a)
B
(b)
r
B
Fig. 21-3. (a) A superconducting cylinder in a magnetic ﬁeld; (b) the
magnetic ﬁeld B as a function of r.
21-17","26"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","595","there have been some even more dramatic predictions. One prediction made by
London was so peculiar that nobody paid much attention to it until recently. I
will now discuss it. This time instead of taking a single lump, suppose we take
a ring whose thickness is large compared to 1/λ, and try to see what would
happen if we started with a magnetic ﬁeld through the ring, then cooled it to
the superconducting state, and afterward removed the original source of B. The
sequence of events is sketched in Fig. 21-4. In the normal state there will be a
(a)
B
(b)
B
(c)
B
Fig. 21-4. A ring in a magnetic ﬁeld: (a) in the normal state; (b) in
the superconducting state; (c) after the external ﬁeld is removed.
21-18","131"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","596","ﬁeld in the body of the ring as sketched in part (a) of the ﬁgure. When the ring
is made superconducting, the ﬁeld is forced outside of the material (as we have
just seen). There will then be some ﬂux through the hole of the ring as sketched
in part (b). If the external ﬁeld is now removed, the lines of ﬁeld going through
the hole are “trapped” as shown in part (c). The ﬂux Φ through the center
can’t decrease because ∂Φ/∂t must be equal to the line integral of E around
the ring, which is zero in a superconductor. As the external ﬁeld is removed a
super current starts ﬂowing around the ring to keep the ﬂux through the ring
a constant. (It’s the old eddy-current idea, only with zero resistance.) These
currents will, however, all ﬂow near the surface (down to a depth 1/λ), as can
be shown by the same kind of analysis that I made for the solid block. These
currents can keep the magnetic ﬁeld out of the body of the ring, and produce
the permanently trapped magnetic ﬁeld as well.
Now, however, there is an essential diﬀerence, and our equations predict a
surprising eﬀect. The argument I made above that θ must be a constant in a solid
block does not apply for a ring, as you can see from the following arguments.
Well inside the body of the ring the current density J is zero; so Eq. (21.18)
gives
ℏ∇θ = qA.
(21.26)
Now consider what we get if we take the line integral of A around a curve Γ,
which goes around the ring near the center of its cross-section so that it never
gets near the surface, as drawn in Fig. 21-5. From Eq. (21.26),
ℏ
I
∇θ · ds = q
I
A · ds.
(21.27)
Now you know that the line integral of A around any loop is equal to the ﬂux
Γ
Fig. 21-5. The curve Γ inside a superconducting ring.
21-19","335"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","597","of B through the loop
I
A · ds = Φ.
Equation (21.27) then becomes
I
∇θ · ds = q
ℏΦ.
(21.28)
The line integral of a gradient from one point to another (say from point 1 to
point 2) is the diﬀerence of the values of the function at the two points. Namely,
Z 2
1
∇θ · ds = θ2 −θ1.
If we let the two end points 1 and 2 come together to make a closed loop you
might at ﬁrst think that θ2 would equal θ1, so that the integral in Eq. (21.28)
would be zero. That would be true for a closed loop in a simply-connected piece
of superconductor, but it is not necessarily true for a ring-shaped piece. The
only physical requirement we can make is that there can be only one value of the
wave function for each point. Whatever θ does as you go around the ring, when
you get back to the starting point the θ you get must give the same value for the
wave function
ψ = √ρeiθ.
This will happen if θ changes by 2πn, where n is any integer. So if we make one
complete turn around the ring the left-hand side of Eq. (21.27) must be ℏ· 2πn.
Using Eq. (21.28), I get that
2πnℏ= qΦ.
(21.29)
The trapped ﬂux must always be an integer times 2πℏ/q! If you would think of the
ring as a classical object with an ideally perfect (that is, inﬁnite) conductivity,
you would think that whatever ﬂux was initially found through it would just stay
there—any amount of ﬂux at all could be trapped. But the quantum-mechanical
theory of superconductivity says that the ﬂux can be zero, or 2πℏ/q, or 4πℏ/q,
or 6πℏ/q, and so on, but no value in between. It must be a multiple of a basic
quantum mechanical unit.
London10 predicted that the ﬂux trapped by a superconducting ring would
be quantized and said that the possible values of the ﬂux would be given by
10 F. London, Superﬂuids; John Wiley and Sons, Inc., New York, 1950, Vol. I, p. 152.
21-20","355"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","598","Eq. (21.29) with q equal to the electronic charge. According to London the basic
unit of ﬂux should be 2πℏ/qe, which is about 4 × 10−7 gauss · cm2. To visualize
such a ﬂux, think of a tiny cylinder a tenth of a millimeter in diameter; the
magnetic ﬁeld inside it when it contains this amount of ﬂux is about one percent
of the earth’s magnetic ﬁeld. It should be possible to observe such a ﬂux by a
sensitive magnetic measurement.
In 1961 such a quantized ﬂux was looked for and found by Deaver and
Fairbank11 at Stanford University and at about the same time by Doll and
Näbauer12 in Germany.
In the experiment of Deaver and Fairbank, a tiny cylinder of superconductor
was made by electroplating a thin layer of tin on a one-centimeter length of
No. 56 (1.3 × 10−3 cm diameter) copper wire. The tin becomes superconducting
below 3.8◦K while the copper remains a normal metal. The wire was put in a
small controlled magnetic ﬁeld, and the temperature reduced until the tin became
superconducting. Then the external source of ﬁeld was removed. You would
expect this to generate a current by Lenz’s law so that the ﬂux inside would not
change. The little cylinder should now have magnetic moment proportional to
the ﬂux inside. The magnetic moment was measured by jiggling the wire up and
down (like the needle on a sewing machine, but at the rate of 100 cycles per
second) inside a pair of little coils at the ends of the tin cylinder. The induced
voltage in the coils was then a measure of the magnetic moment.
When the experiment was done by Deaver and Fairbank, they found that
the ﬂux was quantized, but that the basic unit was only one-half as large as
London had predicted. Doll and Näbauer got the same result. At ﬁrst this was
quite mysterious,† but we now understand why it should be so. According to
the Bardeen, Cooper, and Schrieﬀer theory of superconductivity, the q which
appears in Eq. (21.29) is the charge of a pair of electrons and so is equal to 2qe.
The basic ﬂux unit is
Φ0 = πℏ
qe
≈2 × 10−7 gauss · cm2
(21.30)
or one-half the amount predicted by London. Everything now ﬁts together, and
† It has once been suggested by Onsager that this might happen (see Deaver and Fairbank,
Ref. 11), although no one else ever understood why.
11 B. S. Deaver, Jr., and W. M. Fairbank, Phys. Rev. Letters 7, 43 (1961).
12 R. Doll and M. Näbauer, Phys. Rev. Letters 7, 51 (1961).
21-21","437"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","599","the measurements show the existence of the predicted purely quantum-mechanical
eﬀect on a large scale.
21-8 The dynamics of superconductivity
The Meissner eﬀect and the ﬂux quantization are two conﬁrmations of our
general ideas. Just for the sake of completeness I would like to show you what
the complete equations of a superconducting ﬂuid would be from this point of
view—it is rather interesting. Up to this point I have only put the expression
for ψ into equations for charge density and current. If I put it into the complete
Schrödinger equation I get equations for ρ and θ. It should be interesting to see
what develops, because here we have a “ﬂuid” of electron pairs with a charge
density ρ and a mysterious θ—we can try to see what kind of equations we get
for such a “ﬂuid”! So we substitute the wave function of Eq. (21.17) into the
Schrödinger equation (21.3) and remember that ρ and θ are real functions of x, y,
z, and t. If we separate real and imaginary parts we obtain then two equations.
To write them in a shorter form I will—following Eq. (21.19)—write
ℏ
m ∇θ −q
m A = v.
(21.31)
One of the equations I get is then
∂ρ
∂t = −∇· ρv.
(21.32)
Since ρv is ﬁrst J, this is just the continuity equation once more. The other
equation I obtain tells how θ varies; it is
ℏ∂θ
∂t = −m
2 v2 −qφ + ℏ2
2m
 1
√ρ ∇2(√ρ)

.
(21.33)
Those who are thoroughly familiar with hydrodynamics (of which I’m sure few of
you are) will recognize this as the equation of motion for an electrically charged
ﬂuid if we identify ℏθ as the “velocity potential”—except that the last term,
which should be the energy of compression of the ﬂuid, has a rather strange
dependence on the density ρ. In any case, the equation says that the rate of
change of the quantity ℏθ is given by a kinetic energy term, −1
2mv2, plus a
potential energy term, −qφ, with an additional term, containing the factor ℏ2,
21-22","353"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","600","which we could call a “quantum mechanical energy.” We have seen that inside a
superconductor ρ is kept very uniform by the electrostatic forces, so this term can
almost certainly be neglected in every practical application provided we have only
one superconducting region. If we have a boundary between two superconductors
(or other circumstances in which the value of ρ may change rapidly) this term
can become important.
For those who are not so familiar with the equations of hydrodynamics, I can
rewrite Eq. (21.33) in a form that makes the physics more apparent by using
Eq. (21.31) to express θ in terms of v. Taking the gradient of the whole of
Eq. (21.33) and expressing ∇θ in terms of A and v by using (21.31), I get
∂v
∂t = q
m

−∇φ−∂A
∂t

−v×(∇×v)−(v·∇)v+∇ℏ2
2m2
 1
√ρ ∇2√ρ

. (21.34)
What does this equation mean? First, remember that
−∇φ −∂A
∂t = E.
(21.35)
Next, notice that if I take the curl of Eq. (21.31), I get
∇× v = −q
m ∇× A,
(21.36)
since the curl of a gradient is always zero. But ∇× A is the magnetic ﬁeld B,
so the ﬁrst two terms can be written as
q
m(E + v × B).
Finally, you should understand that ∂v/∂t stands for the rate of change of the
velocity of the ﬂuid at a point. If you concentrate on a particular particle, its
acceleration is the total derivative of v (or, as it is sometimes called in ﬂuid
dynamics, the “comoving acceleration”), which is related to ∂v/∂t by13
dv
dt

comoving
= ∂v
∂t + (v · ∇)v.
(21.37)
This extra term also appears as the third term on the right side of Eq. (21.34).
13 See Volume II, Section 40-2.
21-23","301"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","601","Taking it to the left side, I can write Eq. (21.34) in the following way:
m dv
dt

comoving
= q(E + v × B) + ∇ℏ2
2m2
 1
√ρ ∇2√ρ

.
(21.38)
We also have from Eq. (21.36) that
∇× v = −q
m B.
(21.39)
These two equations are the equations of motion of the superconducting
electron ﬂuid. The ﬁrst equation is just Newton’s law for a charged ﬂuid in an
electromagnetic ﬁeld. It says that the acceleration of each particle of the ﬂuid
whose charge is q comes from the ordinary Lorentz force q(E + v × B) plus an
additional force, which is the gradient of some mystical quantum mechanical
potential—a force which is not very big except at the junction between two
superconductors. The second equation says that the ﬂuid is “ideal”—the curl
of v has zero divergence (the divergence of B is always zero). That means that
the velocity can be expressed in terms of velocity potential. Ordinarily one writes
that ∇× v = 0 for an ideal ﬂuid, but for an ideal charged ﬂuid in a magnetic
ﬁeld, this gets modiﬁed to Eq. (21.39).
So, Schrödinger’s equation for the electron pairs in a superconductor gives us
the equations of motion of an electrically charged ideal ﬂuid. Superconductivity
is the same as the problem of the hydrodynamics of a charged liquid. If you want
to solve any problem about superconductors you take these equations for the
ﬂuid [or the equivalent pair, Eqs. (21.32) and (21.33)], and combine them with
Maxwell’s equations to get the ﬁelds. (The charges and currents you use to get
the ﬁelds must, of course, include the ones from the superconductor as well as
from the external sources.)
Incidentally, I believe that Eq. (21.38) is not quite correct, but ought to
have an additional term involving the density. This new term does not depend
on quantum mechanics, but comes from the ordinary energy associated with
variations of density. Just as in an ordinary ﬂuid there should be a potential
energy density proportional to the square of the deviation of ρ from ρ0, the
undisturbed density (which is, here, also equal to the charge density of the crystal
lattice). Since there will be forces proportional to the gradient of this energy, there
should be another term in Eq. (21.38) of the form: (const) ∇(ρ−ρ0)2. This term
did not appear from the analysis because it comes from the interactions between
21-24","410"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","602","particles, which I neglected in using an independent-particle approximation. It is,
however, just the force I referred to when I made the qualitative statement that
electrostatic forces would tend to keep ρ nearly constant inside a superconductor.
21-9 The Josephson junction
I would like to discuss next a very interesting situation that was noticed
by Josephson14 while analyzing what might happen at a junction between two
superconductors. Suppose we have two superconductors which are connected by
a thin layer of insulating material as in Fig. 21-6. Such an arrangement is now
called a “Josephson junction.” If the insulating layer is thick, the electrons can’t
get through; but if the layer is thin enough, there can be an appreciable quantum
mechanical amplitude for electrons to jump across. This is just another example
of the quantum-mechanical penetration of a barrier. Josephson analyzed this
situation and discovered that a number of strange phenomena should occur.
SUPERCONDUCTOR
INSULATOR
1
2
Fig. 21-6. Two superconductors separated by a thin insulator.
In order to analyze such a junction I’ll call the amplitude to ﬁnd an electron on
one side, ψ1, and the amplitude to ﬁnd it on the other, ψ2. In the superconducting
state the wave function, ψ1 is the common wave function of all the electrons
on one side, and ψ2 is the corresponding function on the other side. I could do
this problem for diﬀerent kinds of superconductors, but let us take a very simple
situation in which the material is the same on both sides so that the junction
14 B. D. Josephson, Physics Letters 1, 251 (1962).
21-25","266"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","603","is symmetrical and simple. Also, for a moment let there be no magnetic ﬁeld.
Then the two amplitudes should be related in the following way:
iℏ∂ψ1
∂t = U1ψ1 + Kψ2,
iℏ∂ψ2
∂t = U2ψ2 + Kψ1.
The constant K is a characteristic of the junction. If K were zero, these two
equations would just describe the lowest energy state—with energy U—of each
superconductor. But there is coupling between the two sides by the amplitude K
that there may be leakage from one side to the other. (It is just the “ﬂip-ﬂop”
amplitude of a two-state system.) If the two sides are identical, U1 would equal U2
and I could just subtract them oﬀ. But now suppose that we connect the two
superconducting regions to the two terminals of a battery so that there is a
potential diﬀerence V across the junction. Then U1 −U2 = qV . I can, for
convenience, deﬁne the zero of energy to be halfway between, then the two
equations are
iℏ∂ψ1
∂t = qV
2 ψ1 + Kψ2,
iℏ∂ψ2
∂t = −qV
2 ψ2 + Kψ1.
(21.40)
These are the standard equations for two quantum mechanical states coupled
together. This time, let’s analyze these equations in another way. Let’s make
the substitutions
ψ1 = √ρ1eiθ1,
ψ2 = √ρ2eiθ2,
(21.41)
where θ1 and θ2 are the phases on the two sides of the junction and ρ1 and ρ2 are
the density of electrons at those two points. Remember that in actual practice
ρ1 and ρ2 are almost exactly the same and are equal to ρ0, the normal density of
electrons in the superconducting material. Now if you substitute these equations
for ψ1 and ψ2 into (21.40), you get four equations by equating the real and
21-26","291"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","604","imaginary parts in each case. Letting (θ2 −θ1) = δ, for short, the result is
˙ρ1 = +2
ℏK√ρ2ρ1 sin δ,
˙ρ2 = −2
ℏK√ρ2ρ1 sin δ,
(21.42)
˙θ1 = −K
ℏ
rρ2
ρ1
cos δ −qV
2ℏ,
˙θ2 = −K
ℏ
rρ1
ρ2
cos δ + qV
2ℏ.
(21.43)
The ﬁrst two equations say that ˙ρ1 = −˙ρ2. “But,” you say, “they must both be
zero if ρ1 and ρ2 are both constant and equal to ρ0.” Not quite. These equations
are not the whole story. They say what ˙ρ1 and ˙ρ2 would be if there were no extra
electric forces due to an unbalance between the electron ﬂuid and the background
of positive ions. They tell how the densities would start to change, and therefore
describe the kind of current that would begin to ﬂow. This current from side 1
to side 2 would be just ˙ρ1 (or −˙ρ2), or
J = 2K
ℏ
√ρ1ρ2 sin δ.
(21.44)
Such a current would soon charge up side 2, except that we have forgotten that
the two sides are connected by wires to the battery. The current that ﬂows will
not charge up region 2 (or discharge region 1) because currents will ﬂow to keep
the potential constant. These currents from the battery have not been included
in our equations. When they are included, ρ1 and ρ2 do not in fact change, but
the current across the junction is still given by Eq. (21.44).
Since ρ1 and ρ2 do remain constant and equal to ρ0, let’s set 2Kρ0/ℏ= J0,
and write
J = J0 sin δ.
(21.45)
J0, like K, is then a number which is a characteristic of the particular junction.
The other pair of equations (21.43) tells us about θ1 and θ2 We are interested
in the diﬀerence δ = θ2 −θ1 to use Eq. (21.45); what we get is
˙δ = ˙θ2 −˙θ1 = qV
ℏ.
(21.46)
21-27","321"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","605","That means that we can write
δ(t) = δ0 + q
ℏ
Z
V (t) dt,
(21.47)
where δ0 is the value of δ at t = 0. Remember also that q is the charge of a pair,
namely, q = 2qe. In Eqs. (21.45) and (21.47) we have an important result, the
general theory of the Josephson junction.
Now what are the consequences? First, put on a dc voltage. If you put on a
dc voltage, V0, the argument of the sine becomes (δ0 + (q/ℏ)V0t). Since ℏis a
small number (compared to ordinary voltage and times), the sine oscillates rather
rapidly and the net current is nothing. (In practice, since the temperature is not
zero, you would get a small current due to the conduction by “normal” electrons.)
On the other hand if you have zero voltage across the junction, you can get a
current! With no voltage the current can be any amount between +J0 and −J0
(depending on the value of δ0). But try to put a voltage across it and the current
goes to zero. This strange behavior has recently been observed experimentally.15
There is another way of getting a current—by applying a voltage at a very
high frequency in addition to a dc voltage. Let
V = V0 + v cos ωt,
where v ≪V . Then δ(t) is
δ0 + q
ℏV0t + q
ℏ
v
ω sin ωt.
Now for ∆x small,
sin (x + ∆x) ≈sin x + ∆x cos x.
Using this approximation for sin δ, I get
J = J0
h
sin

δ0 + q
ℏV0t

+ q
ℏ
v
ω sin ωt cos

δ0 + q
ℏV0t
i
.
The ﬁrst term is zero on the average, but the second term is not if
ω = q
ℏV0.
15 P. W. Anderson and J. M. Rowell, Phys. Rev. Letters 10, 230 (1963).
21-28","318"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","606","There should be a current if the ac voltage has just this frequency. Shapiro16
claims to have observed such a resonance eﬀect.
If you look up papers on the subject you will ﬁnd that they often write the
formula for the current as
J = J0 sin

δ0 + 2qe
ℏ
Z
A · ds

,
(21.48)
where the integral is to be taken across the junction. The reason for this is that
when there’s a vector potential across the junction the ﬂip-ﬂop amplitude is
modiﬁed in phase in the way that we explained earlier. If you chase that extra
phase through, it comes out as given above.
Finally, I would like to describe a very dramatic and interesting experiment
which has recently been made on the interference of the currents from each of
two junctions. In quantum mechanics we’re used to the interference between
amplitudes from two diﬀerent slits. Now we’re going to do the interference
between two junctions caused by the diﬀerence in the phase of the arrival of the
currents through two diﬀerent paths. In Fig. 21-7, I show two diﬀerent junctions,
“a” and “b”, connected in parallel. The ends, P and Q, are connected to our
electrical instruments which measure any current ﬂow. The external current,
SUPERCONDUCTOR
a
b
INSULATOR
LOOP Γ
Jtotal
P
Q
Fig. 21-7. Two Josephson junctions in parallel.
16 S. Shapiro, Phys. Rev. Letters 11, 80 (1963).
21-29","238"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","607","Jtotal, will be the sum of the currents through the two junctions. Let Ja and Jb
be the currents through the two junctions, and let their phases be δa and δb. Now
the phase diﬀerence of the wave functions between P and Q must be the same
whether you go on one route or the other. Along the route through junction “a”,
the phase diﬀerence between P and Q is δa plus the line integral of the vector
potential along the upper route:
∆PhaseP →Q = δa + 2qe
ℏ
Z
upper
A · ds.
(21.49)
Why? Because the phase θ is related to A by Eq. (21.26). If you integrate that
equation along some path, the left-hand side gives the phase change, which is
then just proportional to the line integral of A, as we have written here. The
phase change along the lower route can be written similarly
∆PhaseP →Q = δb + 2qe
ℏ
Z
lower
A · ds.
(21.50)
These two must be equal; and if I subtract them I get that the diﬀerence of the
deltas must be the line integral of A around the circuit:
δb −δa = 2qe
ℏ
I
Γ
A · ds.
Here the integral is around the closed loop Γ of Fig. 21-7 which circles through
both junctions. The integral over A is the magnetic ﬂux Φ through the loop. So
the two δ’s are going to diﬀer by 2qe/ℏtimes the magnetic ﬂux Φ which passes
between the two branches of the circuit:
δb −δa = 2qe
ℏΦ.
(21.51)
I can control this phase diﬀerence by changing the magnetic ﬁeld on the circuit,
so I can adjust the diﬀerences in phases and see whether or not the total current
that ﬂows through the two junctions shows any interference of the two parts.
The total current will be the sum of Ja and Jb. For convenience, I will write
δa = δ0 + qe
ℏΦ,
δb = δ0 −qe
ℏΦ.
21-30","330"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","608","Then,
Jtotal = J0

sin

δ0 + qe
ℏΦ

+ sin

δ0 −qe
ℏΦ

= 2J0 sin δ0 cos qeΦ
ℏ.
(21.52)
Now we don’t know anything about δ0, and nature can adjust that anyway
she wants depending on the circumstances. In particular, it will depend on the
external voltage we apply to the junction. No matter what we do, however, sin δ0
can never get bigger than 1. So the maximum current for any given Φ is given by
Jmax = 2J0
cos qe
ℏΦ
 .
This maximum current will vary with Φ and will itself have maxima whenever
Φ = n πℏ
qe
,
with n some integer. That is to say that the current takes on its maximum values
where the ﬂux linkage has just those quantized values we found in Eq. (21.30)!
The Josephson current through a double junction was recently measured17 as
a function of the magnetic ﬁeld in the area between the junctions. The results
are shown in Fig. 21-8. There is a general background of current from various
eﬀects we have neglected, but the rapid oscillations of the current with changes
in the magnetic ﬁeld are due to the interference term cos qeΦ/ℏof Eq. (21.52).
One of the intriguing questions about quantum mechanics is the question
of whether the vector potential exists in a place where there’s no ﬁeld.18 This
experiment I have just described has also been done with a tiny solenoid between
the two junctions so that the only signiﬁcant magnetic B ﬁeld is inside the
solenoid and a negligible amount is on the superconducting wires themselves.
Yet it is reported that the amount of current depends oscillatorily on the ﬂux
of magnetic ﬁeld inside that solenoid even though that ﬁeld never touches the
wires—another demonstration of the “physical reality” of the vector potential.19
I don’t know what will come next. But look what can be done. First, notice
that the interference between two junctions can be used to make a sensitive
17 Jaklevic, Lambe, Silver, and Mercereau, Phys. Rev. Letters 12, 159 (1964).
18 Jaklevic, Lambe, Silver, and Mercereau, Phys. Rev. Letters 12, 274 (1964).
19 See Volume II, Chapter 15, Section 15-5.
21-31","369"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","609","MAGNETIC FIELD (MILLIGAUSS)
−500
−400
−300
−200
−100
0
100
200
300
400
500
JOSEPHSON CURRENT
(ARBITRARY UNITS)
Fig. 21-8. A recording of the current through a pair of Josephson
junctions as a function of the magnetic ﬁeld in the region between the
two junctions (see Fig. 21-7). [This recording was provided by R. C.
Jaklevic, J. Lambe, A. H. Silver, and J. E. Mercereau of the Scientiﬁc
Laboratory, Ford Motor Company.]
magnetometer. If a pair of junctions is made with an enclosed area of, say, 1 mm2,
the maxima in the curve of Fig. 21-8 would be separated by 2 × 10−6 gauss. It
is certainly possible to tell when you are 1/10 of the way between two peaks;
so it should be possible to use such a junction to measure magnetic ﬁelds as
small as 2 × 10−7 gauss—or to measure larger ﬁelds to such a precision. One
should be able to go even further. Suppose for example we put a set of 10 or
20 junctions close together and equally spaced. Then we can have the interference
between 10 or 20 slits and as we change the magnetic ﬁeld we will get very sharp
maxima and minima. Instead of a 2-slit interference we can have a 20- or perhaps
even a 100-slit interferometer for measuring the magnetic ﬁeld. Perhaps we can
predict that the measurement of magnetic ﬁelds will—by using the eﬀects of
quantum-mechanical interference—eventually become almost as precise as the
measurement of wavelength of light.
These then are some illustrations of things that are happening in modern
times—the transistor, the laser, and now these junctions, whose ultimate practical
applications are still not known. The quantum mechanics which was discovered
in 1926 has had nearly 40 years of development, and rather suddenly it has begun
21-32","299"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","610","to be exploited in many practical and real ways. We are really getting control of
nature on a very delicate and beautiful level.
I am sorry to say, gentlemen, that to participate in this adventure it is
absolutely imperative that you learn quantum mechanics as soon as possible. It
was our hope that in this course we would ﬁnd a way to make comprehensible to
you at the earliest possible moment the mysteries of this part of physics.
21-33","79"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","611","Feynman’s Epilogue
Well, I’ve been talking to you for two years and now I’m going to quit. In
some ways I would like to apologize, and other ways not. I hope—in fact, I know—
that two or three dozen of you have been able to follow everything with great
excitement, and have had a good time with it. But I also know that “the powers
of instruction are of very little eﬃcacy except in those happy circumstances in
which they are practically superﬂuous.” So, for the two or three dozen who have
understood everything, may I say I have done nothing but shown you the things.
For the others, if I have made you hate the subject, I’m sorry. I never taught
elementary physics before, and I apologize. I just hope that I haven’t caused a
serious trouble to you, and that you do not leave this exciting business. I hope
that someone else can teach it to you in a way that doesn’t give you indigestion,
and that you will ﬁnd someday that, after all, it isn’t as horrible as it looks.
Finally, may I add that the main purpose of my teaching has not been to
prepare you for some examination—it was not even to prepare you to serve
industry or the military. I wanted most to give you some appreciation of the
wonderful world and the physicist’s way of looking at it, which, I believe, is a
major part of the true culture of modern times. (There are probably professors of
other subjects who would object, but I believe that they are completely wrong.)
Perhaps you will not only have some appreciation of this culture; it is even
possible that you may want to join in the greatest adventure that the human
mind has ever begun.
Epilogue-1","300"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","612","Appendix
Much of the work of this volume assumes a knowledge of the subject of atomic
magnetism which is treated in Chapters 34 and 35 of Volume II. For the convenience of
those readers who may not have Volume II at hand, these two chapters are reproduced
here.
Their contents are as follows:
Chapter 34.
The Magnetism of Matter
34-1
Diamagnetism and paramagnetism
34-2
Magnetic moments and angular momentum
34-3
The precession of atomic magnets
34-4
Diamagnetism
34-5
Larmor’s theorem
34-6
Classical physics gives neither diamagnetism nor paramagnetism
34-7
Angular momentum in quantum mechanics
34-8
The magnetic energy of atoms
Chapter 35.
Paramagnetism and Magnetic Resonance
35-1
Quantized magnetic states
35-2
The Stern-Gerlach experiment
35-3
The Rabi molecular-beam method
35-4
The paramagnetism of bulk materials
35-5
Cooling by adiabatic demagnetization
35-6
Nuclear magnetic resonance","134"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","613","34
The Magnetism of Matter
Review: Section 15-1, Vol. II, “The forces on a current loop; energy of a
dipole.”
34-1 Diamagnetism and paramagnetism
In this chapter we are going to talk about the magnetic properties of materials.
The material which has the most striking magnetic properties is, of course, iron.
Similar magnetic properties are shared also by the elements nickel, cobalt, and—
at suﬃciently low temperatures (below 16◦C)—by gadolinium, as well as by a
number of peculiar alloys. That kind of magnetism, called ferromagnetism, is
suﬃciently striking and complicated that we will discuss it in a special chapter.
However, all ordinary substances do show some magnetic eﬀects, although very
small ones—a thousand to a million times less than the eﬀects in ferromagnetic
materials. Here we are going to describe ordinary magnetism, that is to say, the
magnetism of substances other than the ferromagnetic ones.
This small magnetism is of two kinds. Some materials are attracted toward
magnetic ﬁelds; others are repelled. Unlike the electrical eﬀect in matter, which
always causes dielectrics to be attracted, there are two signs to the magnetic eﬀect.
These two signs can be easily shown with the help of a strong electromagnet
which has one sharply pointed pole piece and one ﬂat pole piece, as drawn in
Fig. 34-1. The magnetic ﬁeld is much stronger near the pointed pole than near
the ﬂat pole. If a small piece of material is fastened to a long string and suspended
between the poles, there will, in general, be a small force on it. This small force
can be seen by the slight displacement of the hanging material when the magnet
is turned on. The few ferromagnetic materials are attracted very strongly toward
the pointed pole; all other materials feel only a very weak force. Some are weakly
attracted to the pointed pole; and some are weakly repelled.
The eﬀect is most easily seen with a small cylinder of bismuth, which is
repelled from the high-ﬁeld region. Substances which are repelled in this way
34-1","337"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","614","STRING
SMALL PIECE OF MATERIAL
N
S
LINES OF B
POLES OF A STRONG
ELECTROMAGNET
Fig. 34-1. A small cylinder of bismuth is weakly repelled by the sharp
pole; a piece of aluminum is attracted.
are called diamagnetic. Bismuth is one of the strongest diamagnetic materials,
but even with it, the eﬀect is still quite weak. Diamagnetism is always very
weak. If a small piece of aluminum is suspended between the poles, there is
also a weak force, but toward the pointed pole. Substances like aluminum are
called paramagnetic. (In such an experiment, eddy-current forces arise when the
magnet is turned on and oﬀ, and these can give oﬀstrong impulses. You must be
careful to look for the net displacement after the hanging object settles down.)
We want now to describe brieﬂy the mechanisms of these two eﬀects. First, in
many substances the atoms have no permanent magnetic moments, or rather, all
the magnets within each atom balance out so that the net moment of the atom is
zero. The electron spins and orbital motions all exactly balance out, so that any
particular atom has no average magnetic moment. In these circumstances, when
you turn on a magnetic ﬁeld little extra currents are generated inside the atom by
induction. According to Lenz’s law, these currents are in such a direction as to
oppose the increasing ﬁeld. So the induced magnetic moments of the atoms are
directed opposite to the magnetic ﬁeld. This is the mechanism of diamagnetism.
Then there are some substances for which the atoms do have a permanent
magnetic moment—in which the electron spins and orbits have a net circulating
current that is not zero. So besides the diamagnetic eﬀect (which is always
present), there is also the possibility of lining up the individual atomic magnetic
34-2","299"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","615","moments. In this case, the moments try to line up with the magnetic ﬁeld (in
the way the permanent dipoles of a dielectric are lined up by the electric ﬁeld),
and the induced magnetism tends to enhance the magnetic ﬁeld. These are the
paramagnetic substances. Paramagnetism is generally fairly weak because the
lining-up forces are relatively small compared with the forces from the thermal
motions which try to derange the order. It also follows that paramagnetism is
usually sensitive to the temperature. (The paramagnetism arising from the spins
of the electrons responsible for conduction in a metal constitutes an exception.
We will not be discussing this phenomenon here.) For ordinary paramagnetism,
the lower the temperature, the stronger the eﬀect. There is more lining-up at low
temperatures when the deranging eﬀects of the collisions are less. Diamagnetism,
on the other hand, is more or less independent of the temperature. In any
substance with built-in magnetic moments there is a diamagnetic as well as a
paramagnetic eﬀect, but the paramagnetic eﬀect usually dominates.
In Chapter 11 of Vol. II we described a ferroelectric material, in which all
the electric dipoles get lined up by their own mutual electric ﬁelds. It is also
possible to imagine the magnetic analog of ferroelectricity, in which all the atomic
moments would line up and lock together. If you make calculations of how this
should happen, you will ﬁnd that because the magnetic forces are so much smaller
than the electric forces, thermal motions should knock out this alignment even at
temperatures as low as a few tenths of a degree Kelvin. So it would be impossible
at room temperature to have any permanent lining up of the magnets.
On the other hand, this is exactly what does happen in iron—it does get lined
up. There is an eﬀective force between the magnetic moments of the diﬀerent
atoms of iron which is much, much greater than the direct magnetic interaction.
It is an indirect eﬀect which can be explained only by quantum mechanics. It is
about ten thousand times stronger than the direct magnetic interaction, and is
what lines up the moments in ferromagnetic materials. We discuss this special
interaction in a later chapter.
Now that we have tried to give you a qualitative explanation of diamagnetism
and paramagnetism, we must correct ourselves and say that it is not possible to
understand the magnetic eﬀects of materials in any honest way from the point
of view of classical physics. Such magnetic eﬀects are a completely quantum-
mechanical phenomenon. It is, however, possible to make some phoney classical
arguments and to get some idea of what is going on. We might put it this way.
You can make some classical arguments and get guesses as to the behavior of the
material, but these arguments are not “legal” in any sense because it is absolutely
34-3","477"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","616","essential that quantum mechanics be involved in every one of these magnetic
phenomena. On the other hand, there are situations, such as in a plasma or a
region of space with many free electrons, where the electrons do obey the laws
of classical mechanics. And in those circumstances, some of the theorems from
classical magnetism are worth while. Also, the classical arguments are of some
value for historical reasons. The ﬁrst few times that people were able to guess at
the meaning and behavior of magnetic materials, they used classical arguments.
Finally, as we have already illustrated, classical mechanics can give us some useful
guesses as to what might happen—even though the really honest way to study
this subject would be to learn quantum mechanics ﬁrst and then to understand
the magnetism in terms of quantum mechanics.
On the other hand, we don’t want to wait until we learn quantum mechanics
inside out to understand a simple thing like diamagnetism. We will have to
lean on the classical mechanics as kind of half showing what happens, realizing,
however, that the arguments are really not correct. We therefore make a series
of theorems about classical magnetism that will confuse you because they will
prove diﬀerent things. Except for the last theorem, every one of them will be
wrong. Furthermore, they will all be wrong as a description of the physical world,
because quantum mechanics is left out.
34-2 Magnetic moments and angular momentum
The ﬁrst theorem we want to prove from classical mechanics is the following:
If an electron is moving in a circular orbit (for example, revolving around a
nucleus under the inﬂuence of a central force), there is a deﬁnite ratio between
the magnetic moment and the angular momentum. Let’s call J the angular
momentum and µ the magnetic moment of the electron in the orbit.
The
magnitude of the angular momentum is the mass of the electron times the
velocity times the radius. (See Fig. 34-2.) It is directed perpendicular to the
plane of the orbit.
J = mvr.
(34.1)
(This is, of course, a nonrelativistic formula, but it is a good approximation for
atoms, because for the electrons involved v/c is generally of the order of e2/ℏc ≈
1/137, or about 1 percent.)
The magnetic moment of the same orbit is the current times the area. (See
Section 14-5, Vol. II.) The current is the charge per unit time which passes any
34-4","406"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","617","m, q
v
J
µ
r
Fig. 34-2. For any circular orbit the magnetic moment µ is q/2m times
the angular momentum J.
point on the orbit, namely, the charge q times the frequency of rotation. The
frequency is the velocity divided by the circumference of the orbit; so
I = q
v
2πr.
The area is πr2, so the magnetic moment is
µ = qvr
2 .
(34.2)
It is also directed perpendicular to the plane of the orbit. So J and µ are in the
same direction:
µ =
q
2m J (orbit).
(34.3)
Their ratio depends neither on the velocity nor on the radius. For any particle
moving in a circular orbit the magnetic moment is equal to q/2m times the
angular momentum. For an electron, the charge is negative—we can call it −qe;
so for an electron
µ = −qe
2m J (electron orbit).
(34.4)
That’s what we would expect classically and, miraculously enough, it is also
true quantum-mechanically. It’s one of those things. However, if you keep going
with the classical physics, you ﬁnd other places where it gives the wrong answers,
and it is a great game to try to remember which things are right and which things
are wrong. We might as well give you immediately what is true in general in
quantum mechanics. First, Eq. (34.4) is true for orbital motion, but that’s not
the only magnetism that exists. The electron also has a spin rotation about its
34-5","247"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","618","own axis (something like the earth rotating on its axis), and as a result of that
spin it has both an angular momentum and a magnetic moment. But for reasons
that are purely quantum-mechanical—there is no classical explanation—the ratio
of µ to J for the electron spin is twice as large as it is for orbital motion of the
spinning electron:
µ = −qe
m J (electron spin).
(34.5)
In any atom there are, generally speaking, several electrons and some combi-
nation of spin and orbit rotations which builds up a total angular momentum and
a total magnetic moment. Although there is no classical reason why it should be
so, it is always true in quantum mechanics that (for an isolated atom) the direc-
tion of the magnetic moment is exactly opposite to the direction of the angular
momentum. The ratio of the two is not necessarily either −qe/m or −qe/2m,
but somewhere in between, because there is a mixture of the contributions from
the orbits and the spins. We can write
µ = −g
 qe
2m

J,
(34.6)
where g is a factor which is characteristic of the state of the atom. It would be 1
for a pure orbital moment, or 2 for a pure spin moment, or some other number
in between for a complicated system like an atom. This formula does not, of
course, tell us very much. It says that the magnetic moment is parallel to the
angular momentum, but can have any magnitude. The form of Eq. (34.6) is
convenient, however, because g—called the “Landé g-factor”—is a dimensionless
constant whose magnitude is of the order of one. It is one of the jobs of quantum
mechanics to predict the g-factor for any particular atomic state.
You might also be interested in what happens in nuclei. In nuclei there are
protons and neutrons which may move around in some kind of orbit and at the
same time, like an electron, have an intrinsic spin. Again the magnetic moment
is parallel to the angular momentum. Only now the order of magnitude of the
ratio of the two is what you would expect for a proton going around in a circle,
with m in Eq. (34.3) equal to the proton mass. Therefore it is usual to write for
nuclei
µ = g
 qe
2mp

J,
(34.7)
where mp is the mass of the proton, and g—called the nuclear g-factor—is a
number near one, to be determined for each nucleus.
34-6","417"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","619","Another important diﬀerence for a nucleus is that the spin magnetic moment
of the proton does not have a g-factor of 2, as the electron does. For a proton,
g = 2 · (2.79). Surprisingly enough, the neutron also has a spin magnetic moment,
and its magnetic moment relative to its angular momentum is 2 · (−1.91). The
neutron, in other words, is not exactly “neutral” in the magnetic sense. It is like
a little magnet, and it has the kind of magnetic moment that a rotating negative
charge would have.
34-3 The precession of atomic magnets
One of the consequences of having the magnetic moment proportional to
the angular momentum is that an atomic magnet placed in a magnetic ﬁeld
will precess. First we will argue classically. Suppose that we have the magnetic
moment µ suspended freely in a uniform magnetic ﬁeld. It will feel a torque τ,
equal to µ × B, which tries to bring it in line with the ﬁeld direction. But the
atomic magnet is a gyroscope—it has the angular momentum J. Therefore the
torque due to the magnetic ﬁeld will not cause the magnet to line up. Instead, the
magnet will precess, as we saw when we analyzed a gyroscope in Chapter 20 of
Volume I. The angular momentum—and with it the magnetic moment—precesses
about an axis parallel to the magnetic ﬁeld. We can ﬁnd the rate of precession
by the same method we used in Chapter 20 of the ﬁrst volume.
Suppose that in a small time ∆t the angular momentum changes from J
to J′, as drawn in Fig. 34-3, staying always at the same angle θ with respect
to the direction of the magnetic ﬁeld B. Let’s call ωp the angular velocity of
the precession, so that in the time ∆t the angle of precession is ωp ∆t. From
the geometry of the ﬁgure, we see that the change of angular momentum in the
time ∆t is
∆J = (J sin θ)(ωp ∆t).
So the rate of change of the angular momentum is
dJ
dt = ωpJ sin θ,
(34.8)
which must be equal to the torque:
τ = µB sin θ.
(34.9)
34-7","363"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","620","B
ωp
θ
J
J′
∆J
|J| sin θ
Fig. 34-3. An object with angular momentum J and a parallel magnetic
moment µ placed in a magnetic ﬁeld B precesses with the angular
velocity ωp.
The angular velocity of precession is then
ωp = µ
J B.
(34.10)
Substituting µ/J from Eq. (34.6), we see that for an atomic system
ωp = g qeB
2m ;
(34.11)
the precession frequency is proportional to B. It is handy to remember that for
an atom (or electron)
fp = ωp
2π = (1.4 megacycles/gauss)gB,
(34.12)
and that for a nucleus
fp = ωp
2π = (0.76 kilocycles/gauss)gB.
(34.13)
(The formulas for atoms and nuclei are diﬀerent only because of the diﬀerent
conventions for g for the two cases.)
34-8","127"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","621","According to the classical theory, then, the electron orbits—and spins—in an
atom should precess in a magnetic ﬁeld. Is it also true quantum-mechanically? It
is essentially true, but the meaning of the “precession” is diﬀerent. In quantum
mechanics one cannot talk about the direction of the angular momentum in the
same sense as one does classically; nevertheless, there is a very close analogy—so
close that we continue to call it “precession.” We will discuss it later when we
talk about the quantum-mechanical point of view.
34-4 Diamagnetism
Next we want to look at diamagnetism from the classical point of view. It
can be worked out in several ways, but one of the nice ways is the following.
Suppose that we slowly turn on a magnetic ﬁeld in the vicinity of an atom. As
the magnetic ﬁeld changes an electric ﬁeld is generated by magnetic induction.
From Faraday’s law, the line integral of E around any closed path is the rate of
change of the magnetic ﬂux through the path. Suppose we pick a path Γ which is
a circle of radius r concentric with the center of the atom, as shown in Fig. 34-4.
The average tangential electric ﬁeld E around this path is given by
E2πr = −d
dt (Bπr2),
and there is a circulating electric ﬁeld whose strength is
E = −r
2
dB
dt .
B
r
Path Γ
q
F
Fig. 34-4. The induced electric forces on the electrons in an atom.
34-9","247"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","622","The induced electric ﬁeld acting on an electron in the atom produces a
torque equal to −qeEr, which must equal the rate of change of the angular
momentum dJ/dt:
dJ
dt = qer2
2
dB
dt .
(34.14)
Integrating with respect to time from zero ﬁeld, we ﬁnd that the change in angular
momentum due to turning on the ﬁeld is
∆J = qer2
2
B.
(34.15)
This is the extra angular momentum from the twist given to the electrons as the
ﬁeld is turned on.
This added angular momentum makes an extra magnetic moment which,
because it is an orbital motion, is just −qe/2m times the angular momentum.
The induced diamagnetic moment is
∆µ = −qe
2m ∆J = −q2
er2
4m B.
(34.16)
The minus sign (as you can see is right by using Lenz’s law) means that the
added moment is opposite to the magnetic ﬁeld.
We would like to write Eq. (34.16) a little diﬀerently. The r2 which appears
is the radius from an axis through the atom parallel to B, so if B is along the
z-direction, it is x2 + y2. If we consider spherically symmetric atoms (or average
over atoms with their natural axes in all directions) the average of x2 + y2 is 2/3
of the average of the square of the true radial distance from the center point of
the atom. It is therefore usually more convenient to write Eq. (34.16) as
∆µ = −q2
e
6m⟨r2⟩avB.
(34.17)
In any case, we have found an induced atomic moment proportional to the
magnetic ﬁeld B and opposing it. This is diamagnetism of matter. It is this
magnetic eﬀect that is responsible for the small force on a piece of bismuth in a
nonuniform magnetic ﬁeld. (You could compute the force by working out the
energy of the induced moments in the ﬁeld and seeing how the energy changes as
the material is moved into or out of the high-ﬁeld region.)
34-10","329"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","623","We are still left with the problem: What is the mean square radius, ⟨r2⟩av?
Classical mechanics cannot supply an answer. We must go back and start over
with quantum mechanics. In an atom we cannot really say where an electron is,
but only know the probability that it will be at some place. If we interpret ⟨r2⟩av to
mean the average of the square of the distance from the center for the probability
distribution, the diamagnetic moment given by quantum mechanics is just the
same as formula (34.17). This equation, of course, is the moment for one electron.
The total moment is given by the sum over all the electrons in the atom. The
surprising thing is that the classical argument and quantum mechanics give
the same answer, although, as we shall see, the classical argument that gives
Eq. (34.17) is not really valid in classical mechanics.
The same diamagnetic eﬀect occurs even when an atom already has a per-
manent moment. Then the system will precess in the magnetic ﬁeld. As the
whole atom precesses, it takes up an additional small angular velocity, and that
slow turning gives a small current which represents a correction to the magnetic
moment. This is just the diamagnetic eﬀect represented in another way. But we
don’t really have to worry about that when we talk about paramagnetism. If
the diamagnetic eﬀect is ﬁrst computed, as we have done here, we don’t have
to worry about the fact that there is an extra little current from the precession.
That has already been included in the diamagnetic term.
34-5 Larmor’s theorem
We can already conclude something from our results so far. First of all, in
the classical theory the moment µ was always proportional to J, with a given
constant of proportionality for a particular atom. There wasn’t any spin of the
electrons, and the constant of proportionality was always −qe/2m; that is to
say, in Eq. (34.6) we should set g = 1. The ratio of µ to J was independent
of the internal motion of the electrons. Thus, according to the classical theory,
all systems of electrons would precess with the same angular velocity. (This is
not true in quantum mechanics.) This result is related to a theorem in classical
mechanics that we would now like to prove. Suppose we have a group of electrons
which are all held together by attraction toward a central point—as the electrons
are attracted by a nucleus. The electrons will also be interacting with each other,
and can, in general, have complicated motions. Suppose you have solved for
the motions with no magnetic ﬁeld and then want to know what the motions
would be with a weak magnetic ﬁeld. The theorem says that the motion with a
34-11","461"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","624","weak magnetic ﬁeld is always one of the no-ﬁeld solutions with an added rotation,
about the axis of the ﬁeld, with the angular velocity ωL = qeB/2m. (This is the
same as ωp, if g = 1.) There are, of course, many possible motions. The point is
that for every motion without the magnetic ﬁeld there is a corresponding motion
in the ﬁeld, which is the original motion plus a uniform rotation. This is called
Larmor’s theorem, and ωL is called the Larmor frequency.
We would like to show how the theorem can be proved, but we will let you
work out the details. Take, ﬁrst, one electron in a central force ﬁeld. The force on
it is just F (r), directed toward the center. If we now turn on a uniform magnetic
ﬁeld, there is an additional force, qv × B; so the total force is
F (r) + qv × B.
(34.18)
Now let’s look at the same system from a coordinate system rotating with angular
velocity ω about an axis through the center of force and parallel to B. This is
no longer an inertial system, so we have to put in the proper pseudo forces—the
centrifugal and Coriolis forces we talked about in Chapter 19 of Volume I. We
found there that in a frame rotating with angular velocity ω, there is an apparent
tangential force proportional to vr, the radial component of velocity:
Ft = −2mωvr.
(34.19)
And there is an apparent radial force which is given by
Fr = mω2r + 2mωvt,
(34.20)
where vt is the tangential component of the velocity, measured in the rotating
frame. (The radial component vr for rotating and inertial frames is the same.)
Now for small enough angular velocities (that is, if ωr ≪vt), we can neglect
the ﬁrst term (centrifugal) in Eq. (34.20) in comparison with the second (Coriolis).
Then Eqs. (34.19) and (34.20) can be written together as
F = −2mω × v.
(34.21)
If we now combine a rotation and a magnetic ﬁeld, we must add the force in
Eq. (34.21) to that in Eq. (34.18). The total force is
F (r) + qv × B + 2mv × ω
(34.22)
34-12","367"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","625","[we reverse the cross product and the sign of Eq. (34.21) to get the last term].
Looking at our result, we see that if
2mω = −qB
the two terms on the right cancel, and in the moving frame the only force is F (r).
The motion of the electron is just the same as with no magnetic ﬁeld—and, of
course, no rotation. We have proved Larmor’s theorem for one electron. Since
the proof assumes a small ω, it also means that the theorem is true only for weak
magnetic ﬁelds. The only thing we could ask you to improve on is to take the
case of many electrons mutually interacting with each other, but all in the same
central ﬁeld, and prove the same theorem. So no matter how complex an atom
is, if it has a central ﬁeld the theorem is true. But that’s the end of the classical
mechanics, because it isn’t true in fact that the motions precess in that way. The
precession frequency ωp of Eq. (34.11) is only equal to ωL if g happens to be
equal to 1.
34-6 Classical physics gives neither diamagnetism nor paramagnetism
Now we would like to demonstrate that according to classical mechanics there
can be no diamagnetism and no paramagnetism at all. It sounds crazy—ﬁrst,
we have proved that there are paramagnetism, diamagnetism, precessing orbits,
and so on, and now we are going to prove that it is all wrong. Yes!—We are
going to prove that if you follow the classical mechanics far enough, there are no
such magnetic eﬀects—they all cancel out. If you start a classical argument in a
certain place and don’t go far enough, you can get any answer you want. But the
only legitimate and correct proof shows that there is no magnetic eﬀect whatever.
It is a consequence of classical mechanics that if you have any kind of system—
a gas with electrons, protons, and whatever—kept in a box so that the whole thing
can’t turn, there will be no magnetic eﬀect. It is possible to have a magnetic eﬀect
if you have an isolated system, like a star held together by itself, which can start
rotating when you put on the magnetic ﬁeld. But if you have a piece of material
that is held in place so that it can’t start spinning, then there will be no magnetic
eﬀects. What we mean by holding down the spin is summarized this way: At a
given temperature we suppose that there is only one state of thermal equilibrium.
The theorem then says that if you turn on a magnetic ﬁeld and wait for the system
to get into thermal equilibrium, there will be no paramagnetism or diamagnetism—
there will be no induced magnetic moment. Proof: According to statistical
34-13","466"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","626","mechanics, the probability that a system will have any given state of motion is
proportional to e−U/kT , where U is the energy of that motion. Now what is the
energy of motion? For a particle moving in a constant magnetic ﬁeld, the energy is
the ordinary potential energy plus mv2/2, with nothing additional for the magnetic
ﬁeld. [You know that the forces from electromagnetic ﬁelds are q(E +v ×B), and
that the rate of work F · v is just qE · v, which is not aﬀected by the magnetic
ﬁeld.] So the energy of a system, whether it is in a magnetic ﬁeld or not, is always
given by the kinetic energy plus the potential energy. Since the probability of any
motion depends only on the energy—that is, on the velocity and position—it is the
same whether or not there is a magnetic ﬁeld. For thermal equilibrium, therefore,
the magnetic ﬁeld has no eﬀect. If we have one system in a box, and then have
another system in a second box, this time with a magnetic ﬁeld, the probability
of any particular velocity at any point in the ﬁrst box is the same as in the second.
If the ﬁrst box has no average circulating current (which it will not have if it is
in equilibrium with the stationary walls), there is no average magnetic moment.
Since in the second box all the motions are the same, there is no average magnetic
moment there either. Hence, if the temperature is kept constant and thermal
equilibrium is re-established after the ﬁeld is turned on, there can be no magnetic
moment induced by the ﬁeld—according to classical mechanics. We can only get
a satisfactory understanding of magnetic phenomena from quantum mechanics.
Unfortunately, we cannot assume that you have a thorough understanding
of quantum mechanics, so this is hardly the place to discuss the matter. On
the other hand, we don’t always have to learn something ﬁrst by learning the
exact rules and then by learning how they are applied in diﬀerent cases. Almost
every subject that we have taken up in this course has been treated in a diﬀerent
way. In the case of electricity, we wrote the Maxwell equations on “Page One”
and then deduced all the consequences. That’s one way. But we will not now
try to begin a new “Page One,” writing the equations of quantum mechanics
and deducing everything from them. We will just have to tell you some of the
consequences of quantum mechanics, before you learn where they come from. So
here we go.
34-7 Angular momentum in quantum mechanics
We have already given you a relation between the magnetic moment and the
angular momentum. That’s pleasant. But what do the magnetic moment and
the angular momentum mean in quantum mechanics? In quantum mechanics it
34-14","472"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","627","turns out to be best to deﬁne things like magnetic moments in terms of the other
concepts such as energy, in order to make sure that one knows what it means.
Now, it is easy to deﬁne a magnetic moment in terms of energy, because the
energy of a moment in a magnetic ﬁeld is, in the classical theory, µ·B. Therefore,
the following deﬁnition has been taken in quantum mechanics: If we calculate
the energy of a system in a magnetic ﬁeld and we ﬁnd that it is proportional
to the ﬁeld strength (for small ﬁeld), the coeﬃcient is called the component of
magnetic moment in the direction of the ﬁeld. (We don’t have to get so elegant
for our work now; we can still think of the magnetic moment in the ordinary, to
some extent classical, sense.)
Now we would like to discuss the idea of angular momentum in quantum
mechanics—or rather, the characteristics of what, in quantum mechanics, is
called angular momentum. You see, when you go to new kinds of laws, you
can’t just assume that each word is going to mean exactly the same thing. You
may think, say, “Oh, I know what angular momentum is. It’s that thing that
is changed by a torque.” But what’s a torque? In quantum mechanics we have
to have new deﬁnitions of old quantities. It would, therefore, be legally best to
call it by some other name such as “quantangular momentum,” or something
like that, because it is the angular momentum as deﬁned in quantum mechanics.
But if we can ﬁnd a quantity in quantum mechanics which is identical to our old
idea of angular momentum when the system becomes large enough, there is no
use in inventing an extra word. We might as well just call it angular momentum.
With that understanding, this odd thing that we are about to describe is angular
momentum. It is the thing which in a large system we recognize as angular
momentum in classical mechanics.
First, we take a system in which angular momentum is conserved, such as an
atom all by itself in empty space. Now such a thing (like the earth spinning on
its axis) could, in the ordinary sense, be spinning around any axis one wished to
choose. And for a given spin, there could be many diﬀerent “states,” all of the
same energy, each “state” corresponding to a particular direction of the axis of the
angular momentum. So in the classical theory, with a given angular momentum,
there is an inﬁnite number of possible states, all of the same energy.
It turns out in quantum mechanics, however, that several strange things
happen. First, the number of states in which such a system can exist is limited—
there is only a ﬁnite number. If the system is small, the ﬁnite number is very
small, and if the system is large, the ﬁnite number gets very, very large. Second,
we cannot describe a “state” by giving the direction of its angular momentum, but
34-15","504"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","628","only by giving the component of the angular momentum along some direction—say
in the z-direction. Classically, an object with a given total angular momentum J
could have, for its z-component, any value from +J to −J.
But quantum-
mechanically, the z-component of angular momentum can have only certain
discrete values. Any given system—a particular atom, or a nucleus, or anything—
with a given energy, has a characteristic number j, and its z-component of angular
momentum can only be one of the following set of values:
jℏ
(j −1)ℏ
(j −2)ℏ
...
−(j −2)ℏ
−(j −1)ℏ
−jℏ
(34.23)
The largest z-component is j times ℏ; the next smaller is one unit of ℏless, and
so on down to −jℏ. The number j is called “the spin of the system.” (Some
people call it the “total angular momentum quantum number”; but we’ll call it
the “spin.”)
You may be worried that what we are saying can only be true for some
“special” z-axis. But that is not so. For a system whose spin is j, the component
of angular momentum along any axis can have only one of the values in (34.23).
Although it is quite mysterious, we ask you just to accept it for the moment. We
will come back and discuss the point later. You may at least be pleased to hear
that the z-component goes from some number to minus the same number, so
that we at least don’t have to decide which is the plus direction of the z-axis.
(Certainly, if we said that it went from +j to minus a diﬀerent amount, that
would be inﬁnitely mysterious, because we wouldn’t have been able to deﬁne the
z-axis, pointing the other way.)
Now if the z-component of angular momentum must go down by integers
from +j to −j, then j must be an integer. No! Not quite; twice j must be an
integer. It is only the diﬀerence between +j and −j that must be an integer.
So, in general, the spin j is either an integer or a half-integer, depending on
whether 2j is even or odd. Take, for instance, a nucleus like lithium, which has a
34-16","361"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","629","spin of three-halves, j = 3/2. Then the angular momentum around the z-axis, in
units of ℏ, is one of the following:
+3/2
+1/2
−1/2
−3/2.
There are four possible states, each of the same energy, if the nucleus is in empty
space with no external ﬁelds. If we have a system whose spin is two, then the
z-component of angular momentum has only the values, in units of ℏ,
2
1
0
−1
−2.
If you count how many states there are for a given j, there are (2j+1) possibilities.
In other words, if you tell me the energy and also the spin j, it turns out that
there are exactly (2j + 1) states with that energy, each state corresponding to
one of the diﬀerent possible values of the z-component of the angular momentum.
We would like to add one other fact. If you pick out any atom of known j
at random and measure the z-component of the angular momentum, then you
may get any one of the possible values, and each of the values is equally likely.
All of the states are in fact single states, and each is just as good as any other.
Each one has the same “weight” in the world. (We are assuming that nothing
has been done to sort out a special sample.) This fact has, incidentally, a simple
classical analog. If you ask the same question classically: What is the likelihood
of a particular z-component of angular momentum if you take a random sample
of systems, all with the same total angular momentum?—the answer is that all
values from the maximum to the minimum are equally likely. (You can easily
work that out.) The classical result corresponds to the equal probability of the
(2j + 1) possibilities in quantum mechanics.
From what we have so far, we can get another interesting and somewhat
surprising conclusion. In certain classical calculations the quantity that appears
in the ﬁnal result is the square of the magnitude of the angular momentum J—in
other words, J · J. It turns out that it is often possible to guess at the correct
34-17","357"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","630","quantum-mechanical formula by using the classical calculation and the following
simple rule: Replace J2 = J · J by j(j + 1)ℏ2. This rule is commonly used,
and usually gives the correct result, but not always. We can give the following
argument to show why you might expect this rule to work.
The scalar product J · J can be written as
J · J = J2
x + J2
y + J2
z .
Since it is a scalar, it should be the same for any orientation of the spin. Suppose
we pick samples of any given atomic system at random and make measurements
of J2
x, or J2
y, or J2
z , the average value should be the same for each. (There is no
special distinction for any one of the directions.) Therefore, the average of J · J
is just equal to three times the average of any component squared, say of J2
z ;
⟨J · J⟩av = 3⟨J2
z ⟩av.
But since J · J is the same for all orientations, its average is, of course, just its
constant value; we have
J · J = 3⟨J2
z ⟩av.
(34.24)
If we now say that we will use the same equation for quantum mechanics,
we can easily ﬁnd ⟨J2
z ⟩av. We just have to take the sum of the (2j + 1) possible
values of J2
z , and divide by the total number;
⟨J2
z ⟩av = j2 + (j −1)2 + · · · + (−j + 1)2 + (−j)2
2j + 1
ℏ2.
(34.25)
For a system with a spin of 3/2, it goes like this:
⟨J2
z ⟩av = (3/2)2 + (1/2)2 + (−1/2)2 + (−3/2)2
4
ℏ2 = 5
4 ℏ2.
We conclude that
J · J = 3⟨J2
z ⟩av = 3 5
4ℏ2 = 3
2( 3
2 + 1)ℏ2.
We will leave it for you to show that Eq. (34.25), together with Eq. (34.24), gives
the general result
J · J = j(j + 1)ℏ2.
(34.26)
Although we would think classically that the largest possible value of the z-
component of J is just the magnitude of J—namely,
√
J · J—quantum mechan-
ically the maximum of Jz is always a little less than that, because jℏis always
less than
p
j(j + 1)ℏ. The angular momentum is never “completely along the
z-direction.”
34-18","399"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","631","34-8 The magnetic energy of atoms
Now we want to talk again about the magnetic moment. We have said that
in quantum mechanics the magnetic moment of a particular atomic system can
be written in terms of the angular momentum by Eq. (34.6);
µ = −g
 qe
2m

J,
(34.27)
where −qe and m are the charge and mass of the electron.
An atomic magnet placed in an external magnetic ﬁeld will have an extra
magnetic energy which depends on the component of its magnetic moment along
the ﬁeld direction. We know that
Umag = −µ · B.
(34.28)
Choosing our z-axis along the direction of B,
Umag = −µzB.
(34.29)
Using Eq. (34.27), we have that
Umag = g
 qe
2m

JzB.
Quantum mechanics says that Jz can have only certain values: jℏ, (j −1)ℏ,
. . . , −jℏ. Therefore, the magnetic energy of an atomic system is not arbitrary;
it can have only certain values. Its maximum value, for instance, is
g
 qe
2m

ℏjB.
The quantity qeℏ/2m is usually given the name “the Bohr magneton” and
written µB:
µB = qeℏ
2m.
The possible values of the magnetic energy are
Umag = gµBB Jz
ℏ,
where Jz/ℏtakes on the possible values j, (j −1), (j −2), . . . , (−j + 1), −j.
34-19","225"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","632","In other words, the energy of an atomic system is changed when it is put in a
magnetic ﬁeld by an amount that is proportional to the ﬁeld, and proportional
to Jz. We say that the energy of an atomic system is “split into 2j +1 levels” by a
magnetic ﬁeld. For instance, an atom whose energy is U0 outside a magnetic ﬁeld
and whose j is 3/2, will have four possible energies when placed in a ﬁeld. We
can show these energies by an energy-level diagram like that drawn in Fig. 34-5.
Any particular atom can have only one of the four possible energies in any given
ﬁeld B. That is what quantum mechanics says about the behavior of an atomic
system in a magnetic ﬁeld.
The simplest “atomic” system is a single electron. The spin of an electron
is 1/2, so there are two possible states: Jz = ℏ/2 and Jz = −ℏ/2. For an electron,
at rest (no orbital motion), the spin magnetic moment has a g-value of 2, so the
magnetic energy can be either ±µBB. The possible energies in a magnetic ﬁeld
are shown in Fig. 34-6. Speaking loosely we say that the electron either has its
spin “up” (along the ﬁeld) or “down” (opposite the ﬁeld).
For systems with higher spins, there are more states. We can think that the
spin is “up” or “down” or cocked at some “angle” in between, depending on the
value of Jz.
We will use these quantum mechanical results to discuss the magnetic prop-
erties of materials in the next chapter.
34-20","264"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","633","B
Umag
0
Jz= + 3
2 ¯h
Jz= + 1
2 ¯h
Jz= −1
2 ¯h
Jz= −3
2 ¯h
Fig. 34-5. The possible magnetic energies of an atomic system with
a spin of 3/2 in a magnetic ﬁled B.
B
Umag
0
Jz= + 1
2 ¯h
Jz= −1
2 ¯h
Fig. 34-6. The two possible energy states of an electron in a magnetic
ﬁeld B.
34-21","69"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","634","35
Paramagnetism and Magnetic Resonance
Review: Chapter 11, Vol. II, Inside Dielectrics
35-1 Quantized magnetic states
In the last chapter we described how in quantum mechanics the angular
momentum of a thing does not have an arbitrary direction, but its component
along a given axis can take on only certain equally spaced, discrete values. It
is a shocking and peculiar thing. You may think that perhaps we should not go
into such things until your minds are more advanced and ready to accept this
kind of an idea. Actually, your minds will never become more advanced—in the
sense of being able to accept such a thing easily. There isn’t any descriptive way
of making it intelligible that isn’t so subtle and advanced in its own form that
it is more complicated than the thing you were trying to explain. The behavior
of matter on a small scale—as we have remarked many times—is diﬀerent from
anything that you are used to and is very strange indeed. As we proceed with
classical physics, it is a good idea to try to get a growing acquaintance with the
behavior of things on a small scale, at ﬁrst as a kind of experience without any
deep understanding. Understanding of these matters comes very slowly, if at
all. Of course, one does get better able to know what is going to happen in a
quantum-mechanical situation—if that is what understanding means—but one
never gets a comfortable feeling that these quantum-mechanical rules are “natural.”
Of course they are, but they are not natural to our own experience at an ordinary
level. We should explain that the attitude that we are going to take with regard
to this rule about angular momentum is quite diﬀerent from many of the other
things we have talked about. We are not going to try to “explain” it, but we must
at least tell you what happens; it would be dishonest to describe the magnetic
properties of materials without mentioning the fact that the classical description
of magnetism—of angular momentum and magnetic moments—is incorrect.
One of the most shocking and disturbing features about quantum mechanics
is that if you take the angular momentum along any particular axis you ﬁnd that
35-1","370"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","635","it is always an integer or half-integer times ℏ. This is so no matter which axis
you take. The subtleties involved in that curious fact—that you can take any
other axis and ﬁnd that the component for it is also locked to the same set of
values—we will leave to a later chapter, when you will experience the delight of
seeing how this apparent paradox is ultimately resolved.
We will now just accept the fact that for every atomic system there is a
number j, called the spin of the system—which must be an integer or a half-
integer—and that the component of the angular momentum along any particular
axis will always have one of the following values between +jℏand −jℏ:
Jz = one of





















j
j −1
j −2
...
−j + 2
−j + 1
−j





















· ℏ.
(35.1)
We have also mentioned that every simple atomic system has a magnetic
moment which has the same direction as the angular momentum. This is true
not only for atoms and nuclei but also for the fundamental particles. Each
fundamental particle has its own characteristic value of j and its magnetic
moment. (For some particles, both are zero.) What we mean by “the magnetic
moment” in this statement is that the energy of the system in a magnetic ﬁeld,
say in the z-direction, can be written as −µzB for small magnetic ﬁelds. We
must have the condition that the ﬁeld should not be too great, otherwise it could
disturb the internal motions of the system and the energy would not be a measure
of the magnetic moment that was there before the ﬁeld was turned on. But if
the ﬁeld is suﬃciently weak, the ﬁeld changes the energy by the amount
∆U = −µzB,
(35.2)
with the understanding that in this equation we are to replace µz by
µz = g
 q
2m

Jz,
(35.3)
where Jz has one of the values in Eq. (35.1).
35-2","370"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","636","Suppose we take a system with a spin j = 3/2. Without a magnetic ﬁeld,
the system has four diﬀerent possible states corresponding to the diﬀerent values
of Jz, all of which have exactly the same energy. But the moment we turn on
the magnetic ﬁeld, there is an additional energy of interaction which separates
these states into four slightly diﬀerent energy levels. The energies of these levels
are given by a certain energy proportional to B, multiplied by ℏtimes 3/2, 1/2,
−1/2, and −3/2—the values of Jz. The splitting of the energy levels for atomic
systems with spins of 1/2, 1, and 3/2 are shown in the diagrams of Fig. 35-1.
(Remember that for any arrangement of electrons the magnetic moment is always
directed opposite to the angular momentum.)
(a)
U
U0
B
j = 1/2
Jz = +¯h/2
Jz = −¯h/2
(b)
U
U0
B
j = 1
Jz = +¯h
Jz = 0¯h
Jz = −¯h
(c)
U
U0
B
j = 3/2
Jz = +3¯h/2
Jz = +¯h/2
Jz = −¯h/2
Jz = −3¯h/2
¯hωp
¯hωp
¯hωp
Fig. 35-1. An atomic system with spin j has (2j + 1) possible energy
values in a magnetic ﬁeld B. The energy splitting is proportional to B
for small ﬁelds.
35-3","212"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","637","You will notice from the diagrams that the “center of gravity” of the energy
levels is the same with and without a magnetic ﬁeld. Also notice that the spacings
from one level to the next are always equal for a given particle in a given magnetic
ﬁeld. We are going to write the energy spacing, for a given magnetic ﬁeld B,
as ℏωp—which is just a deﬁnition of ωp. Using Eqs. (35.2) and (35.3), we have
ℏωp = g q
2m ℏB
or
ωp = g q
2m B.
(35.4)
The quantity g(q/2m) is just the ratio of the magnetic moment to the angular
momentum—it is a property of the particle. Equation (35.4) is the same formula
that we got in Chapter 34 for the angular velocity of precession in a magnetic ﬁeld,
for a gyroscope whose angular momentum is J and whose magnetic moment is µ.
35-2 The Stern-Gerlach experiment
The fact that the angular momentum is quantized is such a surprising thing
that we will talk a little bit about it historically. It was a shock from the moment
it was discovered (although it was expected theoretically). It was ﬁrst observed
in an experiment done in 1922 by Stern and Gerlach. If you wish, you can
consider the experiment of Stern-Gerlach as a direct justiﬁcation for a belief in
the quantization of angular momentum. Stern and Gerlach devised an experiment
for measuring the magnetic moment of individual silver atoms. They produced a
beam of silver atoms by evaporating silver in a hot oven and letting some of them
come out through a series of small holes. This beam was directed between the
pole tips of a special magnet, as shown in Fig. 35-2. Their idea was the following.
If the silver atom has a magnetic moment µ, then in a magnetic ﬁeld B it has
an energy −µzB, where z is the direction of the magnetic ﬁeld. In the classical
theory, µz would be equal to the magnetic moment times the cosine of the angle
between the moment and the magnetic ﬁeld, so the extra energy in the ﬁeld
would be
∆U = −µB cos θ.
(35.5)
Of course, as the atoms come out of the oven, their magnetic moments would
point in every possible direction, so there would be all values of θ. Now if the
magnetic ﬁeld varies very rapidly with z—if there is a strong ﬁeld gradient—then
35-4","403"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","638","OVEN
HOLE
VACUUM
MAGNET
GLASS
PLATE
Fig. 35-2. The experiment of Stern and Gerlach.
35-5","15"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","639","the magnetic energy will also vary with position, and there will be a force on the
magnetic moments whose direction will depend on whether cosine θ is positive
or negative. The atoms will be pulled up or down by a force proportional to the
derivative of the magnetic energy; from the principle of virtual work,
Fz = −∂U
∂z = µ cos θ ∂B
∂z .
(35.6)
Stern and Gerlach made their magnet with a very sharp edge on one of the
pole tips in order to produce a very rapid variation of the magnetic ﬁeld. The
beam of silver atoms was directed right along this sharp edge, so that the atoms
would feel a vertical force in the inhomogeneous ﬁeld. A silver atom with its
magnetic moment directed horizontally would have no force on it and would go
straight past the magnet. An atom whose magnetic moment was exactly vertical
would have a force pulling it up toward the sharp edge of the magnet. An atom
whose magnetic moment was pointed downward would feel a downward push.
Thus, as they left the magnet, the atoms would be spread out according to their
vertical components of magnetic moment. In the classical theory all angles are
possible, so that when the silver atoms are collected by deposition on a glass
plate, one should expect a smear of silver along a vertical line. The height of
the line would be proportional to the magnitude of the magnetic moment. The
abject failure of classical ideas was completely revealed when Stern and Gerlach
saw what actually happened. They found on the glass plate two distinct spots.
The silver atoms had formed two beams.
That a beam of atoms whose spins would apparently be randomly oriented
gets split up into two separate beams is most miraculous. How does the magnetic
moment know that it is only allowed to take on certain components in the direction
of the magnetic ﬁeld? Well, that was really the beginning of the discovery of the
quantization of angular momentum, and instead of trying to give you a theoretical
explanation, we will just say that you are stuck with the result of this experiment
just as the physicists of that day had to accept the result when the experiment
was done. It is an experimental fact that the energy of an atom in a magnetic
ﬁeld takes on a series of individual values. For each of these values the energy
is proportional to the ﬁeld strength. So in a region where the ﬁeld varies, the
principle of virtual work tells us that the possible magnetic force on the atoms
will have a set of separate values; the force is diﬀerent for each state, so the beam
of atoms is split into a small number of separate beams. From a measurement of
the deﬂection of the beams, one can ﬁnd the strength of the magnetic moment.
35-6","485"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","640","35-3 The Rabi molecular-beam method
We would now like to describe an improved apparatus for the measurement
of magnetic moments which was developed by I. I. Rabi and his collaborators.
In the Stern-Gerlach experiment the deﬂection of atoms is very small, and the
measurement of the magnetic moment is not very precise. Rabi’s technique
permits a fantastic precision in the measurement of the magnetic moments. The
method is based on the fact that the original energy of the atoms in a magnetic
ﬁeld is split up into a ﬁnite number of energy levels. That the energy of an atom
in the magnetic ﬁeld can have only certain discrete energies is really not more
surprising than the fact that atoms in general have only certain discrete energy
levels—something we mentioned often in Volume I. Why should the same thing
not hold for atoms in a magnetic ﬁeld? It does. But it is the attempt to correlate
this with the idea of an oriented magnetic moment that brings out some of the
strange implications of quantum mechanics.
When an atom has two levels which diﬀer in energy by the amount ∆U, it
can make a transition from the upper level to the lower level by emitting a light
quantum of frequency ω, where
ℏω = ∆U.
(35.7)
The same thing can happen with atoms in a magnetic ﬁeld. Only then, the energy
diﬀerences are so small that the frequency does not correspond to light, but to
microwaves or to radiofrequencies. The transitions from the lower energy level
to an upper energy level of an atom can also take place with the absorption of
light or, in the case of atoms in a magnetic ﬁeld, by the absorption of microwave
energy. Thus if we have an atom in a magnetic ﬁeld, we can cause transitions
from one state to another by applying an additional electromagnetic ﬁeld of the
proper frequency. In other words, if we have an atom in a strong magnetic ﬁeld
and we “tickle” the atom with a weak varying electromagnetic ﬁeld, there will
be a certain probability of knocking it to another level if the frequency is near
to the ω in Eq. (35.7). For an atom in a magnetic ﬁeld, this frequency is just
what we have earlier called ωp and it is given in terms of the magnetic ﬁeld
by Eq. (35.4). If the atom is tickled with the wrong frequency, the chance of
causing a transition is very small. Thus there is a sharp resonance at ωp in the
probability of causing a transition. By measuring the frequency of this resonance
in a known magnetic ﬁeld B, we can measure the quantity g(q/2m)—and hence
the g-factor—with great precision.
35-7","452"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","641","It is interesting that one comes to the same conclusion from a classical point
of view. According to the classical picture, when we place a small gyroscope
with a magnetic moment µ and an angular momentum J in an external magnetic
ﬁeld, the gyroscope will precess about an axis parallel to the magnetic ﬁeld.
(See Fig. 35-3.) Suppose we ask: How can we change the angle of the classical
gyroscope with respect to the ﬁeld—namely, with respect to the z-axis? The
magnetic ﬁeld produces a torque around a horizontal axis. Such a torque you
would think is trying to line up the magnet with the ﬁeld, but it only causes
the precession. If we want to change the angle of the gyroscope with respect to
the z-axis, we must exert a torque on it about the z-axis. If we apply a torque
which goes in the same direction as the precession, the angle of the gyroscope
will change to give a smaller component of J in the z-direction. In Fig. 35-3, the
angle between J and the z-axis would increase. If we try to hinder the precession,
J moves toward the vertical.
B
ωp
J
µ
Fig. 35-3. The classical precession of an atom with the magnetic
moment µ and the angular momentum J.
For our precessing atom in a uniform magnetic ﬁeld, how can we apply the
kind of torque we want? The answer is: with a weak magnetic ﬁeld from the
side. You might at ﬁrst think that the direction of this magnetic ﬁeld would have
to rotate with the precession of the magnetic moment, so that it was always at
right angles to the moment, as indicated by the ﬁeld B′ in Fig. 35-4(a). Such
a ﬁeld works very well, but an alternating horizontal ﬁeld is almost as good. If
we have a small horizontal ﬁeld B′, which is always in the x-direction (plus or
minus) and which oscillates with the frequency ωp, then on each one-half cycle
35-8","332"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","642","(a)
B
J
µ
B′
(b)
B
J
µ
B′ = b cos ωpt
Fig. 35-4. The angle of precession of an atomic magnet can be
changed by a horizontal magnetic ﬁeld always at right angles to µ, as
in (a), or by an oscillating ﬁeld, as in (b).
the torque on the magnetic moment reverses, so that it has a cumulative eﬀect
which is almost as eﬀective as a rotating magnetic ﬁeld. Classically, then, we
would expect the component of the magnetic moment along the z-direction to
change if we have a very weak oscillating magnetic ﬁeld at a frequency which is
exactly ωp. Classically, of course, µz would change continuously, but in quantum
mechanics the z-component of the magnetic moment cannot adjust continuously.
It must jump suddenly from one value to another. We have made the comparison
between the consequences of classical mechanics and quantum mechanics to give
you some clue as to what might happen classically and how it is related to what
actually happens in quantum mechanics. You will notice, incidentally, that the
expected resonant frequency is the same in both cases.
35-9","187"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","643","One additional remark: From what we have said about quantum mechanics,
there is no apparent reason why there couldn’t also be transitions at the fre-
quency 2ωp. It happens that there isn’t any analog of this in the classical case,
and also it doesn’t happen in the quantum theory either—at least not for the
particular method of inducing the transitions that we have described. With an
oscillating horizontal magnetic ﬁeld, the probability that a frequency 2ωp would
cause a jump of two steps at once is zero. It is only at the frequency ωp that
transitions, either upward or downward, are likely to occur.
Now we are ready to describe Rabi’s method for measuring magnetic moments.
We will consider here only the operation for atoms with a spin of 1/2. A diagram of
the apparatus is shown in Fig. 35-5. There is an oven which gives out a stream of
neutral atoms which passes down a line of three magnets. Magnet 1 is just like the
one in Fig. 35-2, and has a ﬁeld with a strong ﬁeld gradient—say, with ∂Bz/∂z pos-
itive. If the atoms have a magnetic moment, they will be deﬂected downward
if Jz = +ℏ/2, or upward if Jz = −ℏ/2 (since for electrons µ is directed opposite
to J). If we consider only those atoms which can get through the slit S1, there are
two possible trajectories, as shown. Atoms with Jz = +ℏ/2 must go along curve a
to get through the slit, and those with Jz = −ℏ/2 must go along curve b. Atoms
which start out from the oven along other paths will not get through the slit.
Magnet 2 has a uniform ﬁeld. There are no forces on the atoms in this region,
so they go straight through and enter magnet 3. Magnet 3 is just like magnet 1
but with the ﬁeld inverted, so that ∂Bz/∂z has the opposite sign. The atoms
with Jz = +ℏ/2 (we say “with spin up”), that felt a downward push in magnet 1,
get an upward push in magnet 3; they continue on the path a and go through
slit S2 to a detector. The atoms with Jz = −ℏ/2 (“with spin down”) also have
opposite forces in magnets 1 and 3 and go along the path b, which also takes
them through slit S2 to the detector.
The detector may be made in various ways, depending on the atom being
measured. For example, for atoms of an alkali metal like sodium, the detector
can be a thin, hot tungsten wire connected to a sensitive current meter. When
sodium atoms land on the wire, they are evaporated oﬀas Na+ ions, leaving an
electron behind. There is a current from the wire proportional to the number of
sodium atoms arriving per second.
In the gap of magnet 2 there is a set of coils that produces a small horizontal
magnetic ﬁeld B′. The coils are driven with a current which oscillates at a variable
frequency ω. So between the poles of magnet 2 there is a strong, constant, vertical
ﬁeld B0 and a weak, oscillating, horizontal ﬁeld B′.
35-10","524"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","644","OVEN
a
b
SLIT S1
b
a
SLIT S2
DETECTOR
Jz = +¯h/2
Jz = −¯h/2
MAGNET 1
MAGNET
2
MAGNET 3
∂Bz
∂z
∂Bz
∂z
B0
B′
b′
a′
Fig. 35-5. The Rabi molecular-beam apparatus.
35-11","37"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","645","Suppose now that the frequency ω of the oscillating ﬁeld is set at ωp—the
“precession” frequency of the atoms in the ﬁeld B. The alternating ﬁeld will
cause some of the atoms passing by to make transitions from one Jz to the
other. An atom whose spin was initially “up” (Jz = +ℏ/2) may be ﬂipped “down”
(Jz = −ℏ/2). Now this atom has the direction of its magnetic moment reversed,
so it will feel a downward force in magnet 3 and will move along the path a′,
shown in Fig. 35-5. It will no longer get through the slit S2 to the detector.
Similarly, some of the atoms whose spins were initially down (Jz = −ℏ/2) will
have their spins ﬂipped up (Jz = +ℏ/2) as they pass through magnet 2. They
will then go along the path b′ and will not get to the detector.
If the oscillating ﬁeld B′ has a frequency appreciably diﬀerent from ωp, it
will not cause any spin ﬂips, and the atoms will follow their undisturbed paths
to the detector. So you can see that the “precession” frequency ωp of the atoms
in the ﬁeld B0 can be found by varying the frequency ω of the ﬁeld B′ until a
decrease is observed in the current of atoms arriving at the detector. A decrease
in the current will occur when ω is “in resonance” with ωp. A plot of the detector
current as a function of ω might look like the one shown in Fig. 35-6. Knowing
ωp, we can obtain the g-value of the atom.
Such atomic-beam or, as they are usually called, “molecular” beam resonance
experiments are a beautiful and delicate way of measuring the magnetic properties
of atomic objects. The resonance frequency ωp can be determined with great
precision—in fact, with a greater precision than we can measure the magnetic
ﬁeld B0, which we must know to ﬁnd g.
ω
ωp
DETECTOR
CURRENT
Fig. 35-6. The current of atoms in the beam decreases when ω = ωp.
35-12","338"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","646","35-4 The paramagnetism of bulk materials
We would like now to describe the phenomenon of the paramagnetism of bulk
materials. Suppose we have a substance whose atoms have permanent magnetic
moments, for example a crystal like copper sulfate. In the crystal there are copper
ions whose inner electron shells have a net angular momentum and a net magnetic
moment. So the copper ion is an object which has a permanent magnetic moment.
Let’s say just a word about which atoms have magnetic moments and which ones
don’t. Any atom, like sodium for instance, which has an odd number of electrons,
will have a magnetic moment. Sodium has one electron in its unﬁlled shell. This
electron gives the atom a spin and a magnetic moment. Ordinarily, however,
when compounds are formed the extra electrons in the outside shell are coupled
together with other electrons whose spin directions are exactly opposite, so that
all the angular momenta and magnetic moments of the valence electrons usually
cancel out. That’s why, in general, molecules do not have a magnetic moment.
Of course if you have a gas of sodium atoms, there is no such cancellation.† Also,
if you have what is called in chemistry a “free radical”—an object with an odd
number of valence electrons—then the bonds are not completely satisﬁed, and
there is a net angular momentum.
In most bulk materials there is a net magnetic moment only if there are atoms
present whose inner electron shell is not ﬁlled. Then there can be a net angular
momentum and a magnetic moment. Such atoms are found in the “transition
element” part of the periodic table—for instance, chromium, manganese, iron,
nickel, cobalt, palladium, and platinum are elements of this kind. Also, all of the
rare earth elements have unﬁlled inner shells and permanent magnetic moments.
There are a couple of other strange things that also happen to have magnetic
moments, such as liquid oxygen, but we will leave it to the chemistry department
to explain the reason.
Now suppose that we have a box full of atoms or molecules with permanent
moments—say a gas, or a liquid, or a crystal. We would like to know what
happens if we apply an external magnetic ﬁeld. With no magnetic ﬁeld, the
atoms are kicked around by the thermal motions, and the moments wind up
pointing in all directions. But when there is a magnetic ﬁeld, it acts to line up
the little magnets; then there are more moments lying toward the ﬁeld than away
from it. The material is “magnetized.”
† Ordinary Na vapor is mostly monatomic, although there are also some molecules of Na2.
35-13","440"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","647","We deﬁne the magnetization M of a material as the net magnetic moment
per unit volume, by which we mean the vector sum of all the atomic magnetic
moments in a unit volume. If there are N atoms per unit volume and their
average moment is ⟨µ⟩av then M can be written as N times the average atomic
moment:
M = N⟨µ⟩av.
(35.8)
The deﬁnition of M corresponds to the deﬁnition of the electric polarization P
of Chapter 10 in Vol. II.
The classical theory of paramagnetism is just like the theory of the dielectric
constant we showed you in Chapter 11 of Vol. II. One assumes that each of the
atoms has a magnetic moment µ, which always has the same magnitude but which
can point in any direction. In a ﬁeld B, the magnetic energy is −µ·B = −µB cos θ,
where θ is the angle between the moment and the ﬁeld. From statistical mechanics,
the relative probability of having any angle is e−energy/kT , so angles near zero
are more likely than angles near π. Proceeding exactly as we did in Section 11-3
of Vol. II, we ﬁnd that for small magnetic ﬁelds M is directed parallel to B and
has the magnitude
M = Nµ2B
3kT .
(35.9)
[See Eq. (11.20) in Vol. II.] This approximate formula is correct only for µB/kT
much less than one.
We ﬁnd that the induced magnetization—the magnetic moment per unit
volume—is proportional to the magnetic ﬁeld.
This is the phenomenon of
paramagnetism. You will see that the eﬀect is stronger at lower temperatures and
weaker at higher temperatures. When we put a ﬁeld on a substance, it develops,
for small ﬁelds, a magnetic moment proportional to the ﬁeld. The ratio of M
to B (for small ﬁelds) is called the magnetic susceptibility.
Now we want to look at paramagnetism from the point of view of quantum
mechanics. We take ﬁrst the case of an atom with a spin of 1/2. In the absence
of a magnetic ﬁeld the atoms have a certain energy, but in a magnetic ﬁeld there
are two possible energies, one for each value of Jz. For Jz = +ℏ/2, the energy is
changed by the magnetic ﬁeld by the amount
∆U1 = +g
qeℏ
2m

· 1
2 · B.
(35.10)
35-14","387"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","648","(The energy shift ∆U is positive for an atom because the electron charge is
negative.) For Jz = −ℏ/2, the energy is changed by the amount
∆U2 = −g
qeℏ
2m

· 1
2 · B.
(35.11)
To save writing, let’s set
µ0 = g
qeℏ
2m

· 1
2;
(35.12)
then
∆U = ±µ0B.
(35.13)
The meaning of µ0 is clear: −µ0 is the z-component of the magnetic moment in
the up-spin case, and +µ0 is the z-component of the magnetic moment in the
down-spin case.
Now statistical mechanics tells us that the probability that an atom is in one
state or another is proportional to
e−(Energy of state)/kT .
With no magnetic ﬁeld the two states have the same energy; so when there is
equilibrium in a magnetic ﬁeld, the probabilities are proportional to
e−∆U/kT .
(35.14)
The number of atoms per unit volume with spin up is
Nup = ae−µ0B/kT ,
(35.15)
and the number with spin down is
Ndown = ae+µ0B/kT .
(35.16)
The constant a is to be determined so that
Nup + Ndown = N,
(35.17)
the total number of atoms per unit volume. So we get that
a =
N
e+µ0B/kT + e−µ0B/kT .
(35.18)
35-15","205"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","649","What we are interested in is the average magnetic moment along the z-axis.
The atoms with spin up will contribute a moment of −µ0, and those with spin
down will have a moment of +µ0; so the average moment is
⟨µ⟩av = Nup(−µ0) + Ndown(+µ0)
N
.
(35.19)
The magnetic moment per unit volume M is then N⟨µ⟩av. Using Eqs. (35.15),
(35.16), and (35.17), we get that
M = Nµ0
e+µ0B/kT −e−µ0B/kT
e+µ0B/kT + e−µ0B/kT .
(35.20)
This is the quantum-mechanical formula for M for atoms with j = 1/2. Inciden-
tally, this formula can also be written somewhat more concisely in terms of the
hyperbolic tangent function:
M = Nµ0 tanh µ0B
kT .
(35.21)
µ0B/kT
0
1
2
3
4
M
Nµ0
Fig. 35-7. The variation of the paramagnetic magnetization with the
magnetic ﬁeld strength B.
A plot of M as a function of B is given in Fig. 35-7. When B gets very large,
the hyperbolic tangent approaches 1, and M approaches the limiting value Nµ0.
So at high ﬁelds, the magnetization saturates. We can see why that is; at high
35-16","185"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","650","enough ﬁelds the moments are all lined up in the same direction. In other words,
they are all in the spin-down state, and each atom contributes the moment µ0.
In most normal cases—say, for typical moments, room temperatures, and the
ﬁelds one can normally get (like 10,000 gauss)—the ratio µ0B/kT is about 0.002.
One must go to very low temperatures to see the saturation. For normal temper-
atures, we can usually replace tanh x by x, and write
M = Nµ2
0B
kT
.
(35.22)
Just as we saw in the classical theory, M is proportional to B. In fact, the
formula is almost exactly the same, except that there seems to be a factor of 1/3
missing. But we still need to relate the µ0 in our quantum formula to the µ that
appears in the classical result, Eq. (35.9).
In the classical formula, what appears is µ2 = µ · µ, the square of the vector
magnetic moment, or
µ · µ =

g qe
2m
2
J · J.
(35.23)
We pointed out in the last chapter that you can very likely get the right answer
from a classical calculation by replacing J · J by j(j + 1)ℏ2. In our particular
example, we have j = 1/2, so
j(j + 1)ℏ2 = 3
4ℏ2.
Substituting this for J · J in Eq. (35.23), we get
µ · µ =

g qe
2m
2 3ℏ2
4 ,
or in terms of µ0, deﬁned in Eq. (35.12), we get
µ · µ = 3µ2
0.
Substituting this for µ2 in the classical formula, Eq. (35.9), does indeed reproduce
the correct quantum formula, Eq. (35.22).
The quantum theory of paramagnetism is easily extended to atoms of any
spin j. The low-ﬁeld magnetization is
M = Ng2 j(j + 1)
3
µ2
BB
kT ,
(35.24)
35-17","309"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","651","where
µB = qeℏ
2m
(35.25)
is a combination of constants with the dimensions of a magnetic moment. Most
atoms have moments of roughly this size. It is called the Bohr magneton. The
spin magnetic moment of the electron is almost exactly one Bohr magneton.
35-5 Cooling by adiabatic demagnetization
There is a very interesting special application of paramagnetism. At very low
temperatures it is possible to line up the atomic magnets in a strong ﬁeld. It
is then possible to get down to extremely low temperatures by a process called
adiabatic demagnetization. We can take a paramagnetic salt (for example, one
containing a number of rare-earth atoms like praseodymium-ammonium-nitrate),
and start by cooling it down with liquid helium to one or two degrees absolute in
a strong magnetic ﬁeld. Then the factor µB/kT is larger than 1—say more like
2 or 3. Most of the spins are lined up, and the magnetization is nearly saturated.
Let’s say, to make it easy, that the ﬁeld is very powerful and the temperature
is very low, so that nearly all the atoms are lined up. Then you isolate the salt
thermally (say, by removing the liquid helium and leaving a good vacuum) and
turn oﬀthe magnetic ﬁeld. The temperature of the salt goes way down.
Now if you were to turn oﬀthe ﬁeld suddenly, the jiggling and shaking,
of the atoms in the crystal lattice would gradually knock all the spins out of
alignment. Some of them would be up and some down. But if there is no ﬁeld
(and disregarding the interactions between the atomic magnets, which will make
only a slight error), it takes no energy to turn over the atomic magnets. They
could randomize their spins without any energy change and, therefore, without
any temperature change.
Suppose, however, that while the atomic magnets are being ﬂipped over by
the thermal motion there is still some magnetic ﬁeld present. Then it requires
some work to ﬂip them over opposite to the ﬁeld—they must do work against the
ﬁeld. This takes energy from the thermal motions and lowers the temperature.
So if the strong magnetic ﬁeld is not removed too rapidly, the temperature of
the salt will decrease—it is cooled by the demagnetization. From the quantum-
mechanical view, when the ﬁeld is strong all the atoms are in the lowest state,
because the odds against any being in the upper state are impossibly big. But as
the ﬁeld is lowered, it gets more and more likely that thermal ﬂuctuations will
35-18","420"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","652","knock an atom into the upper state. When that happens, the atom absorbs the
energy ∆U = µ0B. So if the ﬁeld is turned oﬀslowly, the magnetic transitions
can take energy out of the thermal vibrations of the crystal, cooling it oﬀ. It is
possible in this way to go from a temperature of a few degrees absolute down to
a temperature of a few thousandths of a degree.
Would you like to make something even colder than that? It turns out that
Nature has provided a way. We have already mentioned that there are also
magnetic moments for the atomic nuclei. Our formulas for paramagnetism work
just as well for nuclei, except that the moments of nuclei are roughly a thousand
times smaller. [They are of the order of magnitude of qℏ/2mp, where mp is
the proton mass, so they are smaller by the ratio of the masses of the electron
and proton.] With such magnetic moments, even at a temperature of 2◦K, the
factor µB/kT is only a few parts in a thousand. But if we use the paramagnetic
demagnetization process to get down to a temperature of a few thousandths of
a degree, µB/kT becomes a number near 1—at these low temperatures we can
begin to saturate the nuclear moments. That is good luck, because we can then
use the adiabatic demagnetization of the nuclear magnetism to reach still lower
temperatures. Thus it is possible to do two stages of magnetic cooling. First we
use adiabatic demagnetization of paramagnetic ions to reach a few thousandths
of a degree. Then we use the cold paramagnetic salt to cool some material which
has a strong nuclear magnetism. Finally, when we remove the magnetic ﬁeld
from this material, its temperature will go down to within a millionth of a degree
of absolute zero—if we have done everything very carefully.
35-6 Nuclear magnetic resonance
We have said that atomic paramagnetism is very small and that nuclear
magnetism is even a thousand times smaller. Yet it is relatively easy to observe
the nuclear magnetism by the phenomenon of “nuclear magnetic resonance.”
Suppose we take a substance like water, in which all of the electron spins are
exactly balanced so that their net magnetic moment is zero. The molecules
will still have a very, very tiny magnetic moment due to the nuclear magnetic
moment of the hydrogen nuclei. Suppose we put a small sample of water in a
magnetic ﬁeld B. Since the protons (of the hydrogen) have a spin of 1/2, they
will have two possible energy states. If the water is in thermal equilibrium, there
will be slightly more protons in the lower energy states—with their moments
directed parallel to the ﬁeld. There is a small net magnetic moment per unit
35-19","461"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","653","volume. Since the proton moment is only about one-thousandth of an atomic
moment, the magnetization which goes as µ2—using Eq. (35.22)—is only about
one-millionth as strong as typical atomic paramagnetism. (That’s why we have
to pick a material with no atomic magnetism.) If you work it out, the diﬀerence
between the number of protons with spin up and with spin down is only one part
in 108, so the eﬀect is indeed very small! It can still be observed, however, in the
following way.
Suppose we surround the water sample with a small coil that produces a small
horizontal oscillating magnetic ﬁeld. If this ﬁeld oscillates at the frequency ωp,
it will induce transitions between the two energy states—just as we described
for the Rabi experiment in Section 35-3. When a proton ﬂips from an upper
energy state to a lower one, it will give up the energy µzB which, as we have
seen, is equal to ℏωp. If it ﬂips from the lower energy state to the upper one, it
will absorb the energy ℏωp from the coil. Since there are slightly more protons in
the lower state than in the upper one, there will be a net absorption of energy
from the coil. Although the eﬀect is very small, the slight energy absorption can
be seen with a sensitive electronic ampliﬁer.
Just as in the Rabi molecular-beam experiment, the energy absorption will
be seen only when the oscillating ﬁeld is in resonance, that is, when
ω = ωp = g
 qe
2mp

B.
It is often more convenient to search for the resonance by varying B while keeping
ω ﬁxed. The energy absorption will evidently appear when
B = 2mp
gqe
ω.
A typical nuclear magnetic resonance apparatus is shown in Fig. 35-8. A
high-frequency oscillator drives a small coil placed between the poles of a large
electromagnet. Two small auxiliary coils around the pole tips are driven with
a 60-cycle current so that the magnetic ﬁeld is “wobbled” about its average
value by a very small amount. As an example, say that the main current of the
magnet is set to give a ﬁeld of 5000 gauss, and the auxiliary coils produce a
variation of ±1 gauss about this value. If the oscillator is set at 21.2 megacycles
per second, it will then be at the proton resonance each time the ﬁeld sweeps
through 5000 gauss [using Eq. (34.13) with g = 5.58 for the proton].
35-20","412"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","654","MAGNET
POLE
AUXILIARY
COILS
WATER
OSCILLATOR
ω
OUT
LOSS
SIGNAL
OSCILLOSCOPE
V
H SWEEP
TRIGGER
60 ∼
SOURCE
Fig. 35-8. A nuclear magnetic resonance apparatus.
The circuit of the oscillator is arranged to give an additional output signal
proportional to any change in the power being absorbed from the oscillator. This
signal is fed to the vertical deﬂection ampliﬁer of an oscilloscope. The horizontal
sweep of the oscilloscope is triggered once during each cycle of the ﬁeld-wobbling
frequency. (More usually, the horizontal deﬂection is made to follow in proportion
to the wobbling ﬁeld.)
Before the water sample is placed inside the high-frequency coil, the power
drawn from the oscillator is some value. (It doesn’t change with the magnetic
ﬁeld.) When a small bottle of water is placed in the coil, however, a signal appears
on the oscilloscope, as shown in the ﬁgure. We see a picture of the power being
absorbed by the ﬂipping over of the protons!
In practice, it is diﬃcult to know how to set the main magnet to exactly
5000 gauss. What one does is to adjust the main magnet current until the
resonance signal appears on the oscilloscope. It turns out that this is now the
most convenient way to make an accurate measurement of the strength of a
magnetic ﬁeld. Of course, at some time someone had to measure accurately the
magnetic ﬁeld and frequency to determine the g-value of the proton. But now
that this has been done, a proton resonance apparatus like that of the ﬁgure can
be used as a “proton resonance magnetometer.”
35-21","263"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","655","We should say a word about the shape of the signal. If we were to wobble
the magnetic ﬁeld very slowly, we would expect to see a normal resonance curve.
The energy absorption would read a maximum when ωp arrived exactly at the
oscillator frequency. There would be some absorption at nearby frequencies
because all the protons are not in exactly the same ﬁeld—and diﬀerent ﬁelds
mean slightly diﬀerent resonant frequencies.
One might wonder, incidentally, whether at the resonance frequency we should
see any signal at all. Shouldn’t we expect the high-frequency ﬁeld to equalize the
populations of the two states—so that there should be no signal except when the
water is ﬁrst put in? Not exactly, because although we are trying to equalize the
two populations, the thermal motions on their part are trying to keep the proper
ratios for the temperature T. If we sit at the resonance, the power being absorbed
by the nuclei is just what is being lost to the thermal motions. There is, however,
relatively little “thermal contact” between the proton magnetic moments and
the atomic motions. The protons are relatively isolated down in the center of the
electron distributions. So in pure water, the resonance signal is, in fact, usually
too small to be seen. To increase the absorption, it is necessary to increase the
“thermal contact.” This is usually done by adding a little iron oxide to the water.
The iron atoms are like small magnets; as they jiggle around in their thermal
dance, they make tiny jiggling magnetic ﬁelds at the protons. These varying
ﬁelds “couple” the proton magnets to the atomic vibrations and tend to establish
thermal equilibrium. It is through this “coupling” that protons in the higher
energy states can lose their energy so that they are again capable of absorbing
energy from the oscillator.
In practice the output signal of a nuclear resonance apparatus does not look
like a normal resonance curve. It is usually a more complicated signal with
oscillations—like the one drawn in the ﬁgure. Such signal shapes appear because
of the changing ﬁelds. The explanation should be given in terms of quantum
mechanics, but it can be shown that in such experiments the classical ideas of
precessing moments always give the correct answer. Classically, we would say
that when we arrive at resonance we start driving a lot of the precessing nuclear
magnets synchronously. In so doing, we make them precess together. These
nuclear magnets, all rotating together, will set up an induced emf in the oscillator
coil at the frequency ωp. But because the magnetic ﬁeld is increasing with time,
the precession frequency is increasing also, and the induced voltage is soon at a
frequency a little higher than the oscillator frequency. As the induced emf goes
alternately in phase and out of phase with the oscillator, the “absorbed” power
35-22","477"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","656","goes alternately positive and negative. So on the oscilloscope we see the beat
note between the proton frequency and the oscillator frequency. Because the
proton frequencies are not all identical (diﬀerent protons are in slightly diﬀerent
ﬁelds) and also possibly because of the disturbance from the iron oxide in the
water, the freely precessing moments soon get out of phase, and the beat signal
disappears.
These phenomena of magnetic resonance have been put to use in many ways
as tools for ﬁnding out new things about matter—especially in chemistry and
nuclear physics. It goes without saying that the numerical values of the magnetic
moments of nuclei tell us something about their structure. In chemistry, much has
been learned from the structure (or shape) of the resonances. Because of magnetic
ﬁelds produced by nearby nuclei, the exact position of a nuclear resonance is
shifted somewhat, depending on the environment in which any particular nucleus
ﬁnds itself. Measuring these shifts helps determine which atoms are near which
other ones and helps to elucidate the details of the structure of molecules. Equally
important is the electron spin resonance of free radicals. Although not present to
any very large extent in equilibrium, such radicals are often intermediate states
of chemical reactions. A measurement of an electron spin resonance is a delicate
test for the presence of free radicals and is often the key to understanding the
mechanism of certain chemical reactions.
35-23","238"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","657","Index
A
Aberration, I-27-12 ﬀ, I-34-18 ﬀ
Chromatic ∼, I-27-13
Spherical ∼, I-27-13, I-36-6
of an electron microscope, II-29-10
Absolute zero, I-1-8, I-2-10, I-44-19,
I-44-22
Absorption, I-31-14 f
of light, 9-23 ﬀ
of photons, 4-13 ﬀ
Absorption coeﬃcient, II-32-13
Acceleration, I-8-13 ﬀ
Angular ∼, I-18-5
Components of ∼, I-9-4 ﬀ
of gravity, I-9-6
Accelerator guide ﬁelds, II-29-10 ﬀ
Acceptor, 14-10
Acetylcholine, I-3-4
Activation energy, I-3-6, I-42-12 f
Active circuit element, II-22-9
Actomyosin, I-3-4
Adenine, I-3-9
Adiabatic compression, I-39-8
Adiabatic demagnetization, II-35-18 f,
35-18 f
Adiabatic expansion, I-44-10
Adjoint, 11-39
Hermitian ∼, 20-5
Aﬀective future, I-17-7 f
Aﬀective past, I-17-7
Air trough, I-10-7
Algebra, I-22-1 ﬀ
Four-vector ∼, I-17-12 ﬀ
Greek ∼, I-8-4
Matrix ∼, 5-24, 11-5, 20-28
Tensor ∼, 8-6
Vector ∼, I-11-10 ﬀ, II-2-3, II-2-13,
II-2-21 f, II-3-1, II-3-21 ﬀ, II-27-6,
II-27-8, 5-25, 8-2 f, 8-6
Algebraic operator, 20-4
Alnico V, II-36-23, II-37-20
Alternating-current circuits, II-22-1 ﬀ
Alternating-current generator, II-17-11 ﬀ
Amber, II-1-20, II-37-27
Ammeter, II-16-2
Ammonia maser, 9-1 ﬀ
Ammonia molecule, 8-17 ﬀ
States of an ∼, 9-1 ﬀ
Ampère’s law, II-13-6 ﬀ
Ampèrian current, II-36-4
Amplitude modulation, I-48-6 ﬀ
Amplitude of oscillation, I-21-6
Amplitudes, 8-1 ﬀ
Interfering ∼, 5-16 ﬀ
Probability ∼, I-37-16, 1-16, 3-1 ﬀ,
16-1 ﬀ
Space dependence of ∼, 13-7, 16-1 ﬀ
Time dependence of ∼, 7-1 ﬀ
Index-1","215"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","658","Transformation of ∼, 6-1 ﬀ
Analog computer, I-25-15
Angle
Brewster’s ∼, I-33-10
of incidence, I-26-6, II-33-1
of precession, II-34-7, 34-7
of reﬂection, I-26-6, II-33-1
Angstrom (unit), I-1-4
Angular acceleration, I-18-5
Angular frequency, I-21-5, I-29-4, I-29-6,
I-49-4
Angular momentum, I-7-13, I-18-8 ﬀ,
I-20-1, 18-1 ﬀ, 20-22 ﬀ
Composition of ∼, 18-25 ﬀ
Conservation of ∼, I-4-13
of a rigid body, I-20-14
of circularly polarized light, I-33-18
Orbital ∼, 19-2
Angular velocity, I-18-4 f
Anomalous dispersion, I-31-14
Anomalous refraction, I-33-15 ﬀ
Antiferromagnetic material, II-37-23
Antimatter, I-52-17 ﬀ, 11-27
Antiparticle, I-2-12, 11-23
Antiproton, 11-23
Argon, 19-30 f
Associated Legendre functions, 19-16
Astronomy and physics, I-3-10 ﬀ
Atom, I-1-3 ﬀ
Metastable ∼, I-42-17
Rutherford-Bohr model, II-5-4
Stability of ∼s, II-5-4 f
Thomson model, II-5-4
Atomic clock, I-5-10, 9-22
Atomic currents, II-13-9 ﬀ, II-32-6 f,
II-36-4 ﬀ
Atomic hypothesis, I-1-3 ﬀ
Atomic orbits, II-1-15
Atomic particles, I-2-12 ﬀ
Atomic polarizability, II-32-3
Atomic processes, I-1-8 ﬀ
and parity conservation, 18-5
Attenuation, I-31-15
Avogadro’s number, I-41-18, II-8-9
Axial vector, I-20-6, I-52-10 ﬀ, I-52-17
B
Bar (unit), I-47-7
Barkhausen eﬀect, II-37-19
Baryons, 11-23
Base states, 5-13 ﬀ, 12-1 ﬀ
of the world, 8-8 ﬀ
Battery, II-22-13
Benzene molecule, 10-17 ﬀ, 15-11 ﬀ
Bernoulli’s theorem, II-40-10 ﬀ
Bessel function, II-23-11, II-23-14,
II-23-19, II-24-7
Betatron, II-17-8 f, II-29-15
Binocular vision, I-36-6, I-36-8 f
Biology and physics, I-3-3 ﬀ
Biot-Savart law, II-14-18 f, II-21-13
Birefringence, I-33-4 ﬀ, I-33-16
Birefringent material, I-33-16 ﬀ, II-33-6,
II-39-14
Blackbody radiation, I-41-5 ﬀ
Blackbody spectrum, 4-15 ﬀ
Bohr magneton, II-34-19, II-35-18, II-37-2,
12-19, 34-19, 35-18
Bohr radius, I-38-12, 2-12, 19-5, 19-9
Boltzmann energy, II-36-24
Boltzmann factor, 14-8
Boltzmann’s constant, I-41-18, II-7-14,
14-7
Boltzmann’s law, I-40-4 f
Boltzmann theory, 21-13
Boron, 19-30
Bose particles, 4-1 ﬀ, 15-10 f
Boundary layer, II-41-15
Boundary-value problems, II-7-2
Boyle’s law, I-40-16
Index-2","291"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","659","“Boys” camera, II-9-21
Bragg-Nye crystal model, II-30-22 ﬀ
Breaking-drop theory, II-9-18 f
Bremsstrahlung, I-34-12 f
Brewster’s angle, I-33-10
Brownian motion, I-1-16, I-6-8, I-41-1 ﬀ,
I-46-2 f, I-46-9
Brush discharge, II-9-20
Bulk modulus, II-38-6
C
Calculus
Diﬀerential ∼, II-2-1 ﬀ
Integral ∼, II-3-1 ﬀ
of variations, II-19-6
Cantilever beam, II-38-19
Capacitance, I-23-9
Mutual ∼, II-22-38
Capacitor, I-23-8, II-22-5 ﬀ
at high frequencies, II-23-4 ﬀ
Parallel-plate ∼, I-14-16 f, II-6-22 ﬀ,
II-8-5
Capacity, II-6-23
of a condenser, II-8-4
Capillary action, I-51-16
Carnot cycle, I-44-8 ﬀ, I-45-4, I-45-7
Carriers
Negative ∼, 14-3
Positive ∼, 14-3
Carrier signal, I-48-6
Catalyst, I-42-13
Cavendish’s experiment, I-7-15 ﬀ
Cavity resonators, II-23-1 ﬀ
Cells
Cone ∼, I-35-2 ﬀ, I-35-9, I-35-14,
I-35-17 f, I-36-2 f, I-36-8
Rod ∼, I-35-2 ﬀ, I-35-9, I-35-17 f,
I-36-8, I-36-10 ﬀ, 13-16
Center of mass, I-18-1 ﬀ, I-19-1 ﬀ
Centrifugal force, I-7-9, I-12-18, I-16-3,
I-19-13 f, I-20-14, I-43-7, I-52-5,
II-34-12, II-41-18, 19-20, 19-25,
34-12
Centripetal force, I-19-15 f
Charge
Conservation of ∼, I-4-14, II-13-2 ﬀ
Image ∼, II-6-17
Line of ∼, II-5-6 f
Motion of ∼, II-29-1 ﬀ
on electron, I-12-12
Point ∼, II-1-3
Polarization ∼s, II-10-6 ﬀ
Sheet of ∼, II-5-7 ﬀ
Sphere of ∼, II-5-10 f
Charged conductor, II-6-14 f, II-8-4 ﬀ
Charge separation in a thunder cloud,
II-9-14 ﬀ
Chemical bonds, II-30-5 f
Chemical energy, I-4-3
Chemical kinetics, I-42-11 ﬀ
Chemical reaction, I-1-12 ﬀ
Chemistry and physics, I-3-1 ﬀ
Cherenkov radiation, I-51-3
Chlorophyll molecule, 15-20
Chromatic aberration, I-27-13
Chromaticity, I-35-11 f
Circuit elements, II-23-1 ﬀ
Active ∼, II-22-9
Passive ∼, II-22-9
Circuits
Alternating-current ∼, II-22-1 ﬀ
Equivalent ∼, II-22-22 ﬀ
Circular motion, I-21-6 ﬀ
Circular polarization, I-33-3
Circulation, II-1-8, II-3-14 ﬀ
Classical electron radius, I-32-6, II-28-5
Classical limit, 7-16 ﬀ
Clausius-Clapeyron equation, I-45-10 ﬀ
Index-3","284"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","660","Clausius-Mossotti equation, II-11-13 ﬀ,
II-32-11
Cleavage plane, II-30-3
Clebsch-Gordan coeﬃcients, 18-29, 18-34
Coaxial line, II-24-2
Coeﬃcient
Absorption ∼, II-32-13
Clebsch-Gordan ∼s, 18-29, 18-34
Drag ∼, II-41-11
Einstein ∼s, 9-25
of coupling, II-17-25
of friction, I-12-6
of viscosity, II-41-2
Collision, I-16-10
Elastic ∼, I-10-13 f
Collision cross section, I-43-5 f
Colloidal particles, II-7-13 ﬀ
Color vision, I-35-1 ﬀ, I-36-1 ﬀ
Physiochemistry of ∼, I-35-15 ﬀ
Commutation rule, 20-24
Complex impedance, I-23-12
Complex numbers, I-22-11 ﬀ
and harmonic motion, I-23-1 ﬀ
Complex variable, II-7-3 ﬀ
Compound (insect) eye, I-36-12 ﬀ
Compression
Adiabatic ∼, I-39-8
Isothermal ∼, I-44-10
Condensor
Energy of a ∼, II-8-4 ﬀ
Parallel-plate ∼, II-6-22 ﬀ, II-8-5
Conduction band, 14-2
Conductivity, II-32-16
Ionic ∼, I-43-9 ﬀ
Thermal ∼, II-2-16, II-12-3, II-12-6
of a gas, I-43-16 f
Conductor, II-1-3
Cone cells, I-35-2 ﬀ, I-35-9, I-35-14,
I-35-17 f, I-36-2 f, I-36-8
Conservation
of angular momentum, I-4-13, I-18-11 ﬀ,
I-20-8
of baryon number, 11-23
of charge, I-4-14, II-13-2 ﬀ
of energy, I-3-3, I-4-1 ﬀ, II-27-1 ﬀ,
II-42-24, 7-9 ﬀ
of linear momentum, I-4-13, I-10-1 ﬀ
of strangeness, 11-21
Conservative force, I-14-5 ﬀ
Constant
Boltzmann’s ∼, I-41-18, II-7-14, 14-7
Dielectric ∼, II-10-1 ﬀ
Gravitational ∼, I-7-17
Planck’s ∼, I-4-13, I-5-19, I-17-14,
I-37-18, II-15-16, II-19-18 f,
II-28-17, 1-18, 20-24, 21-2
Stefan-Boltzmann ∼, I-45-14
Constrained motion, I-14-4 f
Contraction hypothesis, I-15-8 f
Coriolis force, I-19-14 ﬀ, I-20-8, I-51-13,
I-52-5, II-34-12, 34-12
Cornea, I-35-1, I-36-5 f, I-36-18
Cornu’s spiral, I-30-16
Cosmic rays, I-2-9, II-9-4
Cosmic synchrotron radiation, I-34-10 ﬀ
Couette ﬂow, II-41-17 ﬀ
Coulomb’s law, I-28-1, I-28-3, II-1-4 f,
II-1-11, II-4-3 ﬀ, II-4-8, II-4-12,
II-4-19, II-5-11 ﬀ
Coupling, coeﬃcient of, II-17-25
Covalent bonds, II-30-5
Cross product, II-2-14, II-31-14 f
Cross section, I-5-15
Collision ∼, I-43-5 f
Nuclear ∼, I-5-15
Scattering ∼, I-32-12
Thomson scattering ∼, I-32-13
Crystal, II-30-1 ﬀ
Geometry of ∼s, II-30-1 ﬀ
Ionic ∼, II-8-8 ﬀ
Index-4","302"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","661","Molecular ∼, II-30-5
Crystal diﬀraction, I-38-8 ﬀ, 2-8 ﬀ
Crystal lattice, II-30-7 ﬀ
Cubic ∼, II-30-17
Hexagonal ∼, II-30-16
Imperfections in a ∼, 13-16 ﬀ
Monoclinic ∼, II-30-16
Orthorhombic ∼, II-30-17
Propagation in a ∼, 13-1 ﬀ
Tetragonal ∼, II-30-17
Triclinic ∼, II-30-15
Trigonal ∼, II-30-16
Cubic lattice, II-30-17
Curie point, II-37-7, II-37-20, II-37-26
Curie’s law, II-11-9
Curie temperature, II-36-29, II-36-31,
II-37-2, II-37-6, II-37-23
Curie-Weiss law, II-11-20
Curl operator, II-2-15, II-3-1
Current
Ampèrian ∼, II-36-4
Atomic ∼s, II-13-9 ﬀ, II-32-6 f, II-36-4 ﬀ
Eddy ∼, II-16-11
Electric ∼, II-13-2 ﬀ
in the atmosphere, II-9-4 ﬀ
Induced ∼s, II-16-10 ﬀ
Current density, II-13-2
Curtate cycloid, I-34-5, I-34-8
Curvature
in three-dimensional space, II-42-11 ﬀ
Intrinsic ∼, II-42-11
Mean ∼, II-42-14
Negative ∼, II-42-11
Positive ∼, II-42-11
Curved space, II-42-1 ﬀ
Cutoﬀfrequency, II-22-30
Cyclotron, II-29-10, II-29-15
Cytosine, I-3-9
D
D’Alembertian operator, II-25-13
Damped oscillation, I-24-4 ﬀ
Debye length, II-7-15
Deﬁnite energy, states of, 13-5 ﬀ
Degrees of freedom, I-25-3, I-39-19, I-40-1
Demagnetization, adiabatic, II-35-18 f,
35-18 f
Density, I-1-6
Current ∼, II-13-2
Energy ∼, II-27-3
Probability ∼, I-6-13, I-6-15, 16-9
Derivative, I-8-9 f
Partial ∼, I-14-15
Diamagnetism, II-34-1 ﬀ, II-34-9 ﬀ, 34-1 ﬀ,
34-9 ﬀ
Diamond lattice, 14-1
Dielectric, II-10-1 ﬀ, II-11-1 ﬀ
Dielectric constant, II-10-1 ﬀ
Diﬀerential calculus, I-8-7, II-2-1 ﬀ
Diﬀraction, I-30-1 ﬀ
by a screen, I-31-17 ﬀ
X-ray ∼, I-30-14, I-38-9, II-8-9, II-30-3,
2-9
Diﬀraction grating, I-30-6 ﬀ
Resolving power of a ∼, I-30-10 f
Diﬀusion, I-43-1 ﬀ
Molecular ∼, I-43-11 ﬀ
of neutrons, II-12-12 ﬀ
Dipole
Electric ∼, II-6-2 ﬀ
Magnetic ∼, II-14-13 ﬀ
Molecular ∼, II-11-1
Oscillating ∼, II-21-8 ﬀ
Dipole moment, I-12-9, II-6-5
Magnetic ∼, II-14-15
Dipole potential, II-6-8 ﬀ
Dipole radiator, I-28-7 ﬀ, I-29-6 ﬀ
Dirac equation, I-20-11
Dislocations, II-30-19
Index-5","285"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","662","and crystal growth, II-30-20 f
Screw ∼, II-30-20 f
Slip ∼, II-30-20
Dispersion, I-31-10 ﬀ
Anomalous ∼, I-31-14
Normal ∼, I-31-14
Dispersion equation, I-31-10
Distance, I-5-1 ﬀ
Distance measurement
by the color-brightness relationship of
stars, I-5-12
by triangulation, I-5-10 ﬀ
Distribution
Normal (Gaussian) ∼, I-6-15, 16-12,
16-14
Probability ∼, I-6-13 ﬀ
Divergence
of four-vectors, II-25-11
Divergence operator, II-2-14, II-3-1
DNA, I-3-8 ﬀ
Domain, II-37-11
Donor site, 14-9
Doppler eﬀect, I-17-14, I-23-18, I-34-13 ﬀ,
I-38-11, II-42-21, 2-11, 12-15
Dot product, II-2-9
of four-vectors, II-25-6
Double stars, I-7-10
Drag coeﬃcient, II-41-11
“Dry” water, II-40-1 ﬀ
Dyes, 10-21 f
Dynamical (p-) momentum, 21-8
Dynamics, I-9-1 ﬀ
Development of ∼, I-7-4
of rotation, I-18-5 f
Relativistic ∼, I-15-15 ﬀ
E
Eddy current, II-16-11
Eﬀect
Barkhausen ∼, II-37-19
Doppler ∼, I-17-14, I-23-18, I-34-13 ﬀ,
I-38-11, II-42-21, 2-11, 12-15
Hall ∼, 14-12 ﬀ
Kerr ∼, I-33-8
Meissner ∼, 21-14 ﬀ, 21-22
Mössbauer ∼, II-42-24
Purkinje ∼, I-35-4
Eﬀective mass, 13-12
Eﬃciency of an ideal engine, I-44-13 ﬀ
Eigenstates, 11-38
Eigenvalues, 11-38
Einstein coeﬃcients, 4-15, 9-25
Einstein-Podolsky-Rosen paradox, 18-16
Einstein’s equation of motion, II-42-30
Einstein’s ﬁeld equation, II-42-29
Elastica, curves of the, II-38-25
Elastic collision, I-10-13 f
Elastic constants, II-39-9, II-39-19 ﬀ
Elastic energy, I-4-3, I-4-11 f
Elasticity, II-38-1 ﬀ
Elasticity tensor, II-39-6 ﬀ
Elastic materials, II-39-1 ﬀ
Electret, II-11-16
Electrical energy, I-4-3, I-4-12, I-10-15,
II-15-5 ﬀ
Electrical forces, I-2-5 ﬀ, II-1-1 ﬀ, II-13-1
in relativistic notation, II-25-1 ﬀ
Electric charge density, II-2-15, 21-10
Electric current, II-13-2 ﬀ
in the atmosphere, II-9-4 ﬀ
Electric current density, II-2-15
Electric dipole, II-6-2 ﬀ
Electric dipole matrix element, 9-25
Electric ﬁeld, I-2-6, I-12-11 ﬀ, II-1-4 ﬀ,
II-6-1 ﬀ, II-7-1 ﬀ
Relativity of ∼, II-13-13 ﬀ
Electric ﬂux, II-1-8
Electric generator, II-16-1 ﬀ, II-22-9 ﬀ
Electric motor, II-16-1 ﬀ
Electric potential, II-4-6 ﬀ
Index-6","293"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","663","Electric susceptibility, II-10-7
Electrodynamics, II-1-5
Electromagnetic energy, I-29-3 f
Electromagnetic ﬁeld, I-2-3, I-2-7,
I-10-15 f
Electromagnetic mass, II-28-1 ﬀ
Electromagnetic radiation, I-26-1, I-28-1 ﬀ
Electromagnetic waves, I-2-7, II-21-1 ﬀ
Electromagnetism, II-1-1 ﬀ
Laws of ∼, II-1-9 ﬀ
Electromotive force (EMF), II-16-5
Electron, I-2-6, I-37-2, I-37-7 ﬀ, 1-1, 1-6 ﬀ
Charge on ∼, I-12-12
Classical ∼radius, I-32-6, II-28-5
Electron cloud, I-6-20
Electron conﬁguration, 19-29
Electronic polarization, II-11-2 ﬀ
Electron microscope, II-29-9 f
Electron-ray tube, I-12-15
Electron volt (unit), I-34-7
Electrostatic energy, II-8-1 ﬀ
in nuclei, II-8-12 ﬀ
of a point charge, II-8-22 f
of charges, II-8-1 ﬀ
of ionic crystals, II-8-8 ﬀ
Electrostatic equations
with dielectrics, II-10-10 ﬀ
Electrostatic ﬁeld, II-5-1 ﬀ, II-7-1 ﬀ
Energy in the ∼, II-8-18 ﬀ
of a grid, II-7-17 ﬀ
Electrostatic lens, II-29-5 ﬀ
Electrostatic potential, equations of the,
II-6-1 f
Electrostatics, II-4-1 ﬀ, II-5-1 ﬀ
Ellipse, I-7-2
Emission of photons, 4-13 ﬀ
Emissivity, II-6-28
Energy, I-4-1 ﬀ, II-22-24 ﬀ
Activation ∼, I-3-6, I-42-12 f
Boltzmann ∼, II-36-24
Chemical ∼, I-4-3
Conservation of ∼, I-3-3, I-4-1 ﬀ,
II-27-1 ﬀ, II-42-24, 7-9 ﬀ
Elastic ∼, I-4-3, I-4-11 f
Electrical ∼, I-4-3, I-4-12, I-10-15,
II-15-5 ﬀ
Electromagnetic ∼, I-29-3 f
Electrostatic ∼, II-8-1 ﬀ
in nuclei, II-8-12 ﬀ
of a point charge, II-8-22 f
of charges, II-8-1 ﬀ
of ionic crystals, II-8-8 ﬀ
Field ∼, II-27-1 ﬀ
Gravitational ∼, I-4-3 ﬀ
Heat ∼, I-4-3, I-4-11 f, I-10-15
in the electrostatic ﬁeld, II-8-18 ﬀ
Kinetic ∼, I-1-13, I-2-10, I-4-3, I-4-10 f
and temperature, I-39-10 ﬀ
Magnetic ∼, II-17-22 ﬀ
Mass ∼, I-4-3, I-4-12
Mechanical ∼, II-15-5 ﬀ
Nuclear ∼, I-4-3
of a condensor, II-8-4 ﬀ
Potential ∼, I-4-7, I-13-1 ﬀ, I-14-1 ﬀ,
7-9 ﬀ
Radiant ∼, I-4-3, I-4-12, I-7-20, I-10-15
Relativistic ∼, I-16-1 ﬀ
Rotational kinetic ∼, I-19-12 ﬀ
Rydberg ∼, 10-6, 19-5
Wall ∼, II-37-11
Energy density, II-27-3
Energy diagram, 14-2
Energy ﬂux, II-27-3
Energy level diagram, 14-6
Energy levels, I-38-13 ﬀ, 2-13 ﬀ, 12-12 ﬀ
of a harmonic oscillator, I-40-17 f
Energy theorem, I-50-13
Enthalpy, I-45-9
Entropy, I-44-19 ﬀ, I-46-9 ﬀ
Equation
Index-7","336"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","664","Clausius-Mossotti ∼, II-11-13 ﬀ,
II-32-11
Diﬀusion ∼
Heat ∼,
Neutron ∼, II-12-13
Dirac ∼, I-20-11
Dispersion ∼, I-31-10
Einstein’s ﬁeld ∼, II-42-29
Einstein’s ∼of motion, II-42-30
Laplace ∼, II-7-2
Maxwell’s ∼s, I-46-12, I-47-12, II-2-1,
II-2-15, II-4-1 f, II-6-1, II-7-11,
II-8-20, II-10-11, II-13-7, II-13-13,
II-13-22, II-15-24 f, II-18-1 ﬀ,
II-22-1 f, II-22-13 f, II-22-16,
II-22-23, II-23-6, II-23-13 f,
II-23-20 f, II-24-9, II-25-12,
II-25-15, II-25-19, II-26-3, II-26-20,
II-26-23, II-27-5, II-27-8, II-27-13,
II-27-17, II-28-1, II-32-7, II-33-2,
II-33-5 ﬀ, II-33-12 f, II-33-16,
II-34-14, II-36-2, II-36-5, II-36-11,
II-36-26, II-38-4, II-39-14, II-42-29,
34-14
for four-vectors, II-25-17
General solution of ∼, II-21-6 ﬀ
in a dielectric, II-32-4 ﬀ
Modiﬁcations of ∼, II-28-10 ﬀ
Solutions of ∼in free space, II-20-1 ﬀ
Solutions of ∼with currents and
charges, II-21-1 ﬀ
Solving ∼, II-18-17 ﬀ
Poisson ∼, II-6-2
Saha ∼, I-42-9
Schrödinger ∼, II-15-21, II-41-20, 16-6,
16-18 ﬀ
for the hydrogen atom, 19-1 ﬀ
in a classical context, 21-1 ﬀ
Wave ∼, I-47-1 ﬀ, II-18-17 ﬀ
Equilibrium, I-1-12
Equipotential surfaces, II-4-20 ﬀ
Equivalent circuits, II-22-22 ﬀ
Ethylene molecule, 15-13
Euclidean geometry, I-1-1, I-12-4, I-12-19,
I-17-4
Euler force, II-38-23
Evaporation, I-1-10, I-1-12
of a liquid, I-40-5 ﬀ, I-42-1 ﬀ
Excess radius, II-42-9 ﬀ, II-42-13 ﬀ,
II-42-29
Exchange force, II-37-3
Excited state, II-8-14, 13-15
Exciton, 13-16
Exclusion principle, 4-23 ﬀ
Expansion
Adiabatic ∼, I-44-10
Isothermal ∼, I-44-10
Exponential atmosphere, I-40-1 ﬀ
Eye
Compound (insect) ∼, I-36-12 ﬀ
Human ∼, I-35-1 ﬀ
F
Farad (unit), I-25-14, II-6-24
Faraday’s law of induction, II-17-3,
II-17-6, II-18-1, II-18-15, II-18-18
Fermat’s principle, I-26-5, I-26-7, I-26-9,
I-26-11, I-26-13 ﬀ, I-26-17 f
Fermi (unit), I-5-18
Fermi particles, 4-1 ﬀ, 15-11
Ferrites, II-37-25 f
Ferroelectricity, II-11-17 ﬀ
Ferromagnetic insulators, II-37-25
Ferromagnetic materials, II-37-19 ﬀ
Ferromagnetism, II-36-1 ﬀ, II-37-1 ﬀ
Field, I-14-12 ﬀ
Electric ∼, I-2-6, I-12-11 ﬀ, II-1-4 ﬀ,
II-6-1 ﬀ, II-7-1 ﬀ
Electromagnetic ∼, I-2-3, I-2-7,
I-10-15 f
Index-8","300"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","665","Electrostatic ∼, II-5-1 ﬀ, II-7-1 ﬀ
of a grid, II-7-17 ﬀ
Flux of a vector ∼, II-3-4 ﬀ
Gravitational ∼, I-12-13 ﬀ, I-13-13 ﬀ
in a cavity, II-5-17 ﬀ
Magnetic ∼, I-12-15 ﬀ, II-1-4 ﬀ,
II-13-1 f, II-14-1 ﬀ
of steady currents, II-13-6 ﬀ
Magnetizing ∼, II-36-15
of a charged conductor, II-6-14 f
of a conductor, II-5-16 f
Relativity of electric ∼, II-13-13 ﬀ
Relativity of magnetic ∼, II-13-13 ﬀ
Scalar ∼, II-2-3 ﬀ
Superposition of ∼s, I-12-15
Two-dimensional ∼s, II-7-3 ﬀ
Vector ∼, II-1-8 f, II-2-3 ﬀ
Field-emission microscope, II-6-27 ﬀ
Field energy, II-27-1 ﬀ
of a point charge, II-28-1 f
Field index, II-29-13
Field-ion microscope, II-6-27 ﬀ
Field lines, II-4-20 ﬀ
Field momentum, II-27-1 ﬀ
of a moving charge, II-28-3 f
Field strength, II-1-6
Filter, II-22-30 ﬀ
Flow
Fluid ∼, II-12-16 ﬀ
Heat ∼, II-2-16 ﬀ, II-12-2 ﬀ
Irrotational ∼, II-12-16 ﬀ, II-40-9 ﬀ
Steady ∼, II-40-10 ﬀ
Viscous ∼, II-41-6 ﬀ
Fluid ﬂow, II-12-16 ﬀ
Flux, II-4-12 ﬀ
Electric ∼, II-1-8
Energy ∼, II-27-3
of a vector ﬁeld, II-3-4 ﬀ
Flux quantization, 21-16 ﬀ
Flux rule, II-17-1
Focal length
of a lens, I-27-7 ﬀ
of a spherical surface, I-27-2 ﬀ
Focus, I-26-11, I-27-4
Force
Centrifugal ∼, I-7-9, I-12-18, I-16-3,
I-19-13 f, I-20-14, I-43-7, I-52-5,
II-34-12, II-41-18, 19-20, 19-25,
34-12
Centripetal ∼, I-19-15 f
Components of ∼, I-9-4 ﬀ
Conservative ∼, I-14-5 ﬀ
Coriolis ∼, I-19-14 ﬀ, I-20-8, I-51-13,
I-52-5, II-34-12, 34-12
Electrical ∼s, I-2-5 ﬀ, II-1-1 ﬀ, II-13-1
in relativistic notation, II-25-1 ﬀ
Electromotive ∼(EMF), II-16-5
Euler ∼, II-38-23
Exchange ∼, II-37-3
Gravitational ∼, I-2-4
Lorentz ∼, II-13-1, II-15-25
Magnetic ∼, I-12-15 ﬀ, II-1-4, II-13-1
on a current, II-13-5 f
Molecular ∼s, I-12-9 ﬀ
Moment of ∼, I-18-8
Nonconservative ∼, I-14-10 ﬀ
Nuclear ∼s, I-12-20 f, II-1-2 f, II-8-12 f,
II-28-18, II-28-20 ﬀ, 10-10 ﬀ
Pseudo ∼, I-12-17 ﬀ
Fortune teller, I-17-8
Foucault pendulum, I-16-3
Fourier analysis, I-25-7, I-50-3 ﬀ, I-50-8 ﬀ
Fourier theorem, II-7-17
Fourier transforms, I-25-7
Four-potential, II-25-15
Four-vector algebra, I-17-12 ﬀ
Four-vectors, I-15-14 f, I-17-8 ﬀ, II-25-1 ﬀ
Fovea, I-35-2 f, I-35-5, I-35-18
Frequency
Angular ∼, I-21-5, I-29-4, I-29-6, I-49-4
Index-9","347"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","666","Larmor ∼, II-34-12, 34-12
of oscillation, I-2-7
Plasma ∼, II-7-12, II-32-18 ﬀ
Fresnel’s reﬂection formulas, I-33-15
Friction, I-10-7, I-12-4 ﬀ
Coeﬃcient of ∼, I-12-6
Origin of ∼, I-12-9
G
Galilean relativity, I-10-5, I-10-11
Galilean transformation, I-12-18, I-15-4
Gallium, 19-32 f
Galvanometer, II-1-17, II-16-2
Gamma rays, I-2-8
Garnets, II-37-25 f
Gauss (unit), I-34-7, II-36-12
Gaussian distribution, I-6-15, 16-12, 16-14
Gauss’ law, II-4-18 f
Applications of ∼, II-5-1 ﬀ
for ﬁeld lines, II-4-21
Gauss’ theorem, II-3-8 ﬀ, 21-7
Generator
Alternating-current ∼, II-17-11 ﬀ
Electric ∼, II-16-1 ﬀ, II-22-9 ﬀ
Van de Graaﬀ∼, II-5-19, II-8-14
Geology and physics, I-3-12 f
Geometrical optics, I-26-2, I-27-1 ﬀ
Gradient operator, II-2-8 ﬀ, II-3-1
Gravitation, I-2-4, I-7-1 ﬀ, I-12-2, II-42-1
Theory of ∼, II-42-28 ﬀ
Gravitational acceleration, I-9-6
Gravitational constant, I-7-17
Gravitational energy, I-4-3 ﬀ
Gravitational ﬁeld, I-12-13 ﬀ, I-13-13 ﬀ
Gravitational force, I-2-4
Gravity, I-13-5 ﬀ, II-42-17 ﬀ
Acceleration of ∼, I-9-6
Greeks’ diﬃculties with speed, I-8-4 f
Green’s function, I-25-8
Ground state, II-8-14, 7-3
Group velocity, I-48-11 f
Guanine, I-3-9
Gyroscope, I-20-9 ﬀ
H
Haidinger’s brush, I-36-14
Hall eﬀect, 14-12 ﬀ
Hamiltonian, 8-16
Hamiltonian matrix, 8-1 ﬀ
Hamilton’s ﬁrst principal function,
II-19-16
Harmonic motion, I-21-6 ﬀ, I-23-1 ﬀ
Harmonic oscillator, I-10-1, I-21-1 ﬀ
Energy levels of a ∼, I-40-17 f
Forced ∼, I-21-9 ﬀ, I-23-4 ﬀ
Harmonics, I-50-1 ﬀ
Heat, I-1-5, I-13-5
Speciﬁc ∼, I-40-13 ﬀ, II-37-7
and the failure of classical physics,
I-40-16 ﬀ
at constant volume, I-45-3
Heat conduction, II-3-10 ﬀ
Heat diﬀusion equation, II-3-10 ﬀ
Heat energy, I-4-3, I-4-11 f, I-10-15
Heat engines, I-44-1 ﬀ
Heat ﬂow, II-2-16 ﬀ, II-12-2 ﬀ
Helium, I-1-8, I-3-11 f, I-49-9 f, 19-27
Liquid ∼, 4-22 f
Helmholtz’s theorem, II-40-22 f
Henry (unit), I-25-13
Hermitian adjoint, 20-5
Hexagonal lattice, II-30-16
High-voltage breakdown, II-6-25 ﬀ
Hooke’s law, I-12-11, II-10-12, II-30-29,
II-31-22, II-38-1 ﬀ, II-38-6, II-39-6,
II-39-18
Human eye, I-35-1 ﬀ
Hydrodynamics, II-40-5 ﬀ
Hydrogen, 19-26 f
Hyperﬁne splitting in ∼, 12-1 ﬀ
Index-10","315"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","667","Hydrogen atom, 19-1 ﬀ
Hydrogen molecular ion, 10-1 ﬀ
Hydrogen molecule, 10-13 ﬀ
Hydrogen wave functions, 19-21 ﬀ
Hydrostatic pressure, II-40-1
Hydrostatics, II-40-1 ﬀ
Hyperﬁne splitting in hydrogen, 12-1 ﬀ
Hysteresis curve, II-37-10 ﬀ
Hysteresis loop, II-36-16
I
Ideal gas law, I-39-16 ﬀ
Identical particles, 3-16 ﬀ, 4-1 ﬀ
Illumination, II-12-20 ﬀ
Image charge, II-6-17
Impedance, I-25-15 f, II-22-1 ﬀ
Complex ∼, I-23-12
of a vacuum, I-32-3
Impure semiconductors, 14-8 ﬀ
Incidence, angle of, I-26-6, II-33-1
Inclined plane, I-4-7
Independent particle approximation,
15-1 ﬀ
Index
Field ∼, II-29-13
of refraction, I-31-1 ﬀ, II-32-1 ﬀ
Induced currents, II-16-10 ﬀ
Inductance, I-23-10, II-16-7 ﬀ, II-17-16 ﬀ,
II-22-3 f
Mutual ∼, II-17-16 ﬀ, II-22-36 f
Self-∼, II-16-8, II-17-20 ﬀ
Induction, laws of, II-17-1 ﬀ
Inductor, I-25-13
Inertia, I-2-4, I-7-20
Moment of ∼, I-18-12 f, I-19-1 ﬀ
Principle of ∼, I-9-1
Infrared radiation, I-2-8, I-23-14, I-26-1
Insulator, II-1-3, II-10-1
Integral, I-8-11 ﬀ
Line ∼, II-3-1 ﬀ
Integral calculus, II-3-1 ﬀ
Interference, I-28-10 ﬀ, I-29-1 ﬀ
and diﬀraction, I-30-1
Two-slit ∼, 3-8 ﬀ
Interfering amplitudes, 5-16 ﬀ
Interfering waves, I-37-6, 1-6
Interferometer, I-15-8
Ion, I-1-11
Ionic bonds, II-30-5
Ionic conductivity, I-43-9 ﬀ
Ionic crystal, II-8-8 ﬀ
Ionic polarizability, II-11-17
Ionization energy, I-42-8
of hydrogen, I-38-12, 2-12
Ionosphere, II-7-9, II-7-12, II-9-6, II-32-22
Irreversibility, I-46-9 ﬀ
Irrotational ﬂow, II-12-16 ﬀ, II-40-9 ﬀ
Isotherm, II-2-5
Isothermal atmosphere, I-40-3
Isothermal compression, I-44-10
Isothermal expansion, I-44-10
Isothermal surfaces, II-2-5
Isotopes, I-3-7, I-3-12, I-39-17
J
Johnson noise, I-41-4, I-41-14
Josephson junction, 21-25 ﬀ
Joule (unit), I-13-5
Joule heating, I-24-3
K
Kármán vortex street, II-41-13
Kepler’s laws, I-7-2 f, I-7-5, I-7-7, I-9-1,
I-18-11
Kerr cell, I-33-8
Kerr eﬀect, I-33-8
Kilocalorie (unit), II-8-9
Kinematic (mv-) momentum, 21-8
Kinetic energy, I-1-13, I-2-10, I-4-3,
I-4-10 f
Index-11","282"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","668","and temperature, I-39-10 ﬀ
Rotational ∼, I-19-12 ﬀ
Kinetic theory
Applications of ∼, I-42-1 ﬀ
of gases, I-39-1 ﬀ
Kirchhoﬀ’s laws, I-25-16, II-22-14 ﬀ,
II-22-27
Kronecker delta, II-31-10
Krypton, 19-32 f
L
Lagrangian, II-19-15
Lamé elastic constants, II-39-9
Lamb-Retherford measurement, II-5-14
Landé g-factor, II-34-6, 34-6
Laplace equation, II-7-2
Laplacian operator, II-2-20
Larmor frequency, II-34-12, 34-12
Larmor’s theorem, II-34-11 ﬀ, 34-11 ﬀ
Laser, I-5-4, I-32-9, I-42-17 f, I-50-17, 9-21
Law
Ampère’s ∼, II-13-6 ﬀ
Applications of Gauss’ ∼, II-5-1 ﬀ
Biot-Savart ∼, II-14-18 f, II-21-13
Boltzmann’s ∼, I-40-4 f
Boyle’s ∼, I-40-16
Coulomb’s ∼, I-28-1, I-28-3, II-1-4 f,
II-1-11, II-4-3 ﬀ, II-4-8, II-4-12,
II-4-19, II-5-11 ﬀ
Curie’s ∼, II-11-9
Curie-Weiss ∼, II-11-20
Faraday’s ∼of induction, II-17-3,
II-17-6, II-18-1, II-18-15, II-18-18
Gauss’ ∼, II-4-18 f
for ﬁeld lines, II-4-21
Hooke’s ∼, I-12-11, II-10-12, II-30-29,
II-31-22, II-38-1 ﬀ, II-38-6, II-39-6,
II-39-18
Ideal gas ∼, I-39-16 ﬀ
Kepler’s ∼s, I-7-2 f, I-7-5, I-7-7, I-9-1,
I-18-11
Kirchhoﬀ’s ∼s, I-25-16, II-22-14 ﬀ,
II-22-27
Lenz’s ∼, II-16-9 f, II-34-2, 34-2
Newton’s ∼s, I-2-9, I-7-10, I-7-12,
I-9-1 ﬀ, I-10-1 f, I-10-5, I-11-3 f,
I-11-7 f, I-12-1 f, I-12-4, I-12-18,
I-12-20, I-13-1, I-14-10, I-15-1 ﬀ,
I-15-5, I-15-16, I-16-4, I-16-13 f,
I-18-1, I-19-4, I-20-1, I-28-5, I-39-1,
I-39-3, I-39-17, I-41-2, I-46-1,
I-46-9, I-47-4 f, II-7-9, II-19-2,
II-42-1, II-42-28
in vector notation, I-11-13 ﬀ
of reﬂection, I-26-3
Ohm’s ∼, I-23-9, I-25-12, I-43-11,
II-19-26, 14-12
Rayleigh’s ∼, I-41-10
Snell’s ∼, I-26-5, I-26-7, I-26-14, I-31-4,
II-33-1
Laws
of electromagnetism, II-1-9 ﬀ
of induction, II-17-1 ﬀ
Least action, principle of, II-19-1 ﬀ
Least time, principle of, I-26-1 ﬀ
Legendre functions, associated, 19-16
Legendre polynomials, 18-23, 19-16
Lens
Electrostatic ∼, II-29-5 ﬀ
Magnetic ∼, II-29-7 ﬀ
Quadrupole ∼, II-7-6, II-29-15 f
Lens formula, I-27-11
Lenz’s rule, II-16-9 f, II-34-2, 34-2
Liénard-Wiechert potentials, II-21-16 ﬀ
Light, I-2-7, II-21-1 ﬀ
Absorption of ∼, 9-23 ﬀ
Momentum of ∼, I-34-20 ﬀ
Polarized ∼, I-32-15
Reﬂection of ∼, II-33-1 ﬀ
Index-12","311"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","669","Refraction of ∼, II-33-1 ﬀ
Scattering of ∼, I-32-1 ﬀ
Speed of ∼, I-15-1, II-18-16 f
Light cone, I-17-6
Lightning, II-9-21 ﬀ
Light pressure, I-34-20
Light waves, I-48-1
Linear momentum
Conservation of ∼, I-4-13, I-10-1 ﬀ
Linear systems, I-25-1 ﬀ
Linear transformation, I-11-11
Line integral, II-3-1 ﬀ
Line of charge, II-5-6 f
Liquid helium, 4-22 f
Lithium, 19-27 f
Lodestone, II-1-20, II-37-27
Logarithms, I-22-3
Lorentz contraction, I-15-13
Lorentz force, II-13-1, II-15-25
Lorentz formula, II-21-21 ﬀ
Lorentz group, II-25-5
Lorentz transformation, I-15-4 f, I-17-1,
I-34-15, I-52-3, II-25-1
of ﬁelds, II-26-1 ﬀ
Lorenz condition, II-25-15
Lorenz gauge, II-18-20, II-25-15
M
Mach number, II-41-11
Magenta, 10-21
Magnetic dipole, II-14-13 ﬀ
Magnetic dipole moment, II-14-15
Magnetic energy, II-17-22 ﬀ
Magnetic ﬁeld, I-12-15 ﬀ, II-1-4 ﬀ,
II-13-1 f, II-14-1 ﬀ
of steady currents, II-13-6 ﬀ
Relativity of ∼, II-13-13 ﬀ
Magnetic force, I-12-15 ﬀ, II-1-4, II-13-1
on a current, II-13-5 f
Magnetic induction, I-12-17
Magnetic lens, II-29-7 ﬀ
Magnetic materials, II-37-1 ﬀ
Magnetic moments, II-34-4 ﬀ, 11-8, 34-4 ﬀ
Magnetic resonance, II-35-1 ﬀ, 35-1 ﬀ
Nuclear ∼, II-35-19 ﬀ, 35-19 ﬀ
Magnetic susceptibility, II-35-14, 35-14
Magnetism, I-2-7, II-34-1 ﬀ, 34-1 ﬀ
Dia∼, II-34-1 ﬀ, II-34-9 ﬀ, 34-1 ﬀ,
34-9 ﬀ
Ferro∼, II-36-1 ﬀ, II-37-1 ﬀ
Para∼, II-34-1 ﬀ, II-35-1 ﬀ, 34-1 ﬀ,
35-1 ﬀ
Magnetization currents, II-36-1 ﬀ
Magnetizing ﬁeld, II-36-15
Magnetostatics, II-4-2, II-13-1 ﬀ
Magnetostriction, II-37-12, II-37-21
Magniﬁcation, I-27-10 f
Magnons, 15-6
Maser, I-42-17
Ammonia ∼, 9-1 ﬀ
Mass, I-9-2, I-15-1
Center of ∼, I-18-1 ﬀ, I-19-1 ﬀ
Eﬀective ∼, 13-12
Electromagnetic ∼, II-28-1 ﬀ
Relativistic ∼, I-16-9 ﬀ
Mass energy, I-4-3, I-4-12
Mass-energy equivalence, I-15-17 ﬀ
Mathematics and physics, I-3-1
Matrix, 5-9
Matrix algebra, 5-24, 11-5, 20-28
Maxwell’s demon, I-46-8 f
Maxwell’s equations, I-15-3 ﬀ, I-25-5,
I-25-8, I-46-12, I-47-12, II-2-1,
II-2-15, II-4-1 f, II-6-1, II-7-11,
II-8-20, II-10-11, II-13-7, II-13-13,
II-13-22, II-15-24 f, II-18-1 ﬀ,
II-22-1 f, II-22-13 f, II-22-16,
II-22-23, II-23-6, II-23-13 f,
II-23-20 f, II-24-9, II-25-12,
II-25-15, II-25-19, II-26-3, II-26-20,
Index-13","319"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","670","II-26-23, II-27-5, II-27-8, II-27-13,
II-27-17, II-28-1, II-32-7, II-33-2,
II-33-5, II-33-7 ﬀ, II-33-12 f,
II-33-16, II-34-14, II-36-2, II-36-5,
II-36-11, II-36-26, II-38-4, II-39-14,
II-42-29, 10-12, 21-11, 21-24, 34-14
for four-vectors, II-25-17
General solution of ∼, II-21-6 ﬀ
in a dielectric, II-32-4 ﬀ
Modiﬁcations of ∼, II-28-10 ﬀ
Solutions of ∼in free space, II-20-1 ﬀ
Solutions of ∼with currents and
charges, II-21-1 ﬀ
Solving ∼, II-18-17 ﬀ
Mean free path, I-43-4 ﬀ
Mean square distance, I-6-9, I-41-15
Mechanical energy, II-15-5 ﬀ
Meissner eﬀect, 21-14 ﬀ, 21-22
Metastable atom, I-42-17
Meter (unit), I-5-18
MeV (unit), I-2-14
Michelson-Morley experiment, I-15-5 ﬀ,
I-15-13
Microscope
Electron ∼, II-29-9 f
Field-emission ∼, II-6-27 ﬀ
Field-ion ∼, II-6-27 ﬀ
Minkowski space, II-31-23
Modes, I-49-1 ﬀ
Normal ∼, I-48-17 f
Mole (unit), I-39-17
Molecular crystal, II-30-5
Molecular diﬀusion, I-43-11 ﬀ
Molecular dipole, II-11-1
Molecular forces, I-12-9 ﬀ
Molecular motion, I-41-1
Molecule, I-1-4
Nonpolar ∼, II-11-1
Polar ∼, II-11-1, II-11-5 ﬀ
Mössbauer eﬀect, II-42-24
Moment
Dipole ∼, I-12-9, II-6-5
of force, I-18-8
of inertia, I-18-12 f, I-19-1 ﬀ
Momentum, I-9-1 ﬀ, I-38-3 ﬀ, 2-3 ﬀ
Angular ∼, I-18-8 ﬀ, I-20-1, 18-1 ﬀ,
20-22 ﬀ
Composition of ∼, 18-25 ﬀ
Conservation of ∼, I-4-13, I-18-11 ﬀ,
I-20-8
of a rigid body, I-20-14
Conservation of angular ∼,
Conservation of linear ∼, I-4-13,
I-10-1 ﬀ
Dynamical (p-) ∼, 21-8
Field ∼, II-27-1 ﬀ
in quantum mechanics, I-10-16 f
Kinematic (mv-) ∼, 21-8
of light, I-34-20 ﬀ
Relativistic ∼, I-10-14 ﬀ, I-16-1 ﬀ
Momentum operator, 20-4, 20-15 ﬀ
Momentum spectrometer, II-29-2
Momentum spectrum, II-29-4
Monatomic gas, I-39-7 ﬀ, I-39-11,
I-39-17 f, I-40-13 f
Monoclinic lattice, II-30-16
Motion, I-5-1 f, I-8-1 ﬀ
Brownian ∼, I-1-16, I-6-8, I-41-1 ﬀ,
I-46-2 f, I-46-9
Circular ∼, I-21-6 ﬀ
Constrained ∼, I-14-4 f
Harmonic ∼, I-21-6 ﬀ, I-23-1 ﬀ
of charge, II-29-1 ﬀ
Orbital ∼, II-34-5, 34-5
Parabolic ∼, I-8-17
Perpetual ∼, I-46-3
Planetary ∼, I-7-1 ﬀ, I-9-11 ﬀ, I-13-9
Motor, electric, II-16-1 ﬀ
Moving charge, ﬁeld momentum of,
II-28-3 f
Index-14","322"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","671","Muscle
Smooth ∼, I-14-3
Striated (skeletal) ∼, I-14-3
Music, I-50-2
Mutual capacitance, II-22-38
Mutual inductance, II-17-16 ﬀ, II-22-36 f
mv-momentum, 21-8
N
Nabla operator (∇), II-2-12 ﬀ
Negative carriers, 14-3
Neon, 19-30
Nernst heat theorem, I-44-22
Neutral K-meson, 11-21 ﬀ
Neutral pion, 10-11
Neutron diﬀusion equation, II-12-13
Neutrons, I-2-6
Diﬀusion of ∼, II-12-12 ﬀ
Newton (unit), I-11-10
Newton · meter (unit), I-13-5
Newton’s laws, I-2-9, I-7-10, I-7-12,
I-9-1 ﬀ, I-10-1 f, I-10-5, I-11-3 f,
I-11-7 f, I-12-1 f, I-12-4, I-12-18,
I-12-20, I-13-1, I-14-10, I-15-1 ﬀ,
I-15-5, I-15-16, I-16-4, I-16-13 f,
I-18-1, I-19-4, I-20-1, I-28-5, I-39-1,
I-39-3, I-39-17, I-41-2, I-46-1,
I-46-9, I-47-4 f, II-7-9, II-19-2,
II-42-1, II-42-28
in vector notation, I-11-13 ﬀ
Nodes, I-49-3
Noise, I-50-2
Nonconservative force, I-14-10 ﬀ
Nonpolar molecule, II-11-1
Normal dispersion, I-31-14
Normal distribution, I-6-15, 16-12, 16-14
Normal modes, I-48-17 f
n-type semiconductor, 14-10
Nuclear cross section, I-5-15
Nuclear energy, I-4-3
Nuclear forces, I-12-20 f, II-1-2 f, II-8-12 f,
II-28-18, II-28-20 ﬀ, 10-10 ﬀ
Nuclear g-factor, II-34-6, 34-6
Nuclear interactions, II-8-14
Nuclear magnetic resonance, II-35-19 ﬀ,
35-19 ﬀ
Nucleon, 11-5
Nucleus, I-2-6, I-2-9 f, I-2-12 ﬀ
Numerical analysis, I-9-11
Nutation, I-20-12 f
O
Oersted (unit), II-36-12
Ohm (unit), I-25-12
Ohm’s law, I-23-9, I-25-12, I-43-11,
II-19-26, 14-12
One-dimensional lattice, 13-1 ﬀ
Operator, 8-7, 20-1 ﬀ
Algebraic ∼, 20-4
Curl ∼, II-2-15, II-3-1
D’Alembertian ∼, II-25-13
Divergence ∼, II-2-14, II-3-1
Gradient ∼, II-2-8 ﬀ, II-3-1
Laplacian ∼, II-2-20
Momentum ∼, 20-4, 20-15 ﬀ
Nabla ∼(∇), II-2-12 ﬀ
Vector ∼, II-2-12
Optic axis, I-33-5
Optic nerve, I-35-3
Optics, I-26-1 ﬀ
Geometrical ∼, I-26-2, I-27-1 ﬀ
Orbital angular momentum, 19-2
Orbital motion, II-34-5, 34-5
Orientation polarization, II-11-5 ﬀ
Oriented magnetic moment, II-35-7, 35-7
Orthorhombic lattice, II-30-17
Oscillating dipole, II-21-8 ﬀ
Oscillation
Amplitude of ∼, I-21-6
Damped ∼, I-24-4 ﬀ
Index-15","289"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","672","Frequency of ∼, I-2-7
Periodic ∼, I-9-7
Period of ∼, I-21-4
Phase of ∼, I-21-6
Plasma ∼s, II-7-9 ﬀ
Oscillator, I-5-4
Forced harmonic ∼, I-21-9 ﬀ, I-23-4 ﬀ
Harmonic ∼, I-10-1, I-21-1 ﬀ
P
Pappus, theorem of, I-19-6 f
Parabolic antenna, I-30-12 f
Parabolic motion, I-8-17
Parallel-axis theorem, I-19-9
Parallel-plate capacitor, I-14-16 f,
II-6-22 ﬀ, II-8-5
Paramagnetism, II-34-1 ﬀ, II-35-1 ﬀ,
34-1 ﬀ, 35-1 ﬀ
Paraxial rays, I-27-3
Partial derivative, I-14-15
Particles
Bose ∼, 4-1 ﬀ, 15-10 f
Fermi ∼, 4-1 ﬀ, 15-11
Identical ∼, 3-16 ﬀ, 4-1 ﬀ
Spin-one ∼, 5-1 ﬀ
Spin one-half ∼, 6-1 ﬀ, 12-1 ﬀ
Precession of ∼, 7-18 ﬀ
Pascal’s triangle, I-6-7
Passive circuit element, II-22-9
Pauli exclusion principle, II-36-31
Pauli spin exchange operator, 12-12, 15-3
Pauli spin matrices, 11-1 ﬀ
Pendulum, I-5-3
Coupled ∼s, I-49-10 ﬀ
Pendulum clock, I-5-3
Periodic table, I-2-14, I-3-2, 19-25 ﬀ
Period of oscillation, I-21-4
Permalloy, II-37-22
Permeability, II-36-18
Relative ∼, II-36-18
Perpetual motion, I-46-3
Phase of oscillation, I-21-6
Phase shift, I-21-6
Phase velocity, I-48-10, I-48-12
Photon, I-2-11, I-17-14, I-26-2, I-37-13,
1-12
Absorption of ∼s, 4-13 ﬀ
Emission of ∼s, 4-13 ﬀ
Polarization states of the ∼, 11-15 ﬀ
Photosynthesis, I-3-4
Physics
Astronomy and ∼, I-3-10 ﬀ
before 1920, I-2-4 ﬀ
Biology and ∼, I-3-3 ﬀ
Chemistry and ∼, I-3-1 ﬀ
Geology and ∼, I-3-12 f
Mathematics and ∼, I-3-1
Psychology and ∼, I-3-13 f
Relationship to other sciences, I-3-1 ﬀ
Piezoelectricity, II-11-16, II-31-23
Planck’s constant, I-4-13, I-5-19, I-17-14,
I-37-18, II-15-16, II-19-18 f,
II-28-17, 1-18, 20-24, 21-2
Plane lattice, II-30-12
Planetary motion, I-7-1 ﬀ, I-9-11 ﬀ, I-13-9
Plane waves, II-20-1 ﬀ
Plasma, II-7-9
Plasma frequency, II-7-12, II-32-18 ﬀ
Plasma oscillations, II-7-9 ﬀ
p-momentum, 21-8
Poincaré stress, II-28-7 f
Point charge, II-1-3
Electrostatic energy of a ∼, II-8-22 f
Field energy of a ∼, II-28-1 f
Poisson equation, II-6-2
Poisson’s ratio, II-38-3, II-38-6, II-38-21
Polarization, I-33-1 ﬀ
Circular ∼, I-33-3
Electronic ∼, II-11-2 ﬀ
of matter, II-32-1 ﬀ
Index-16","319"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","673","of scattered light, I-33-4
Orientation ∼, II-11-5 ﬀ
Polarization charges, II-10-6 ﬀ
Polarization vector, II-10-4 ﬀ
Polarized light, I-32-15
Polar molecule, II-11-1, II-11-5 ﬀ
Polar vector, I-20-6, I-52-10 ﬀ
Positive carriers, 14-3
Potassium, 19-31 f
Potential
Four-∼, II-25-15
Quadrupole ∼, II-6-14
Vector ∼, II-14-1 ﬀ, II-15-1 ﬀ
of known currents, II-14-5 ﬀ
Potential energy, I-4-7, I-13-1 ﬀ, I-14-1 ﬀ,
7-9 ﬀ
Potential gradient of the atmosphere,
II-9-1 ﬀ
Power, I-13-4
Poynting vector, II-27-9
Precession
Angle of ∼, II-34-7, 34-7
of atomic magnets, II-34-7 ﬀ, 34-7 ﬀ
Pressure, I-1-6
Hydrostatic ∼, II-40-1
Light ∼, I-34-20
of a gas, I-39-3 ﬀ
Radiation ∼, I-34-20
Principal quantum number, 19-22
Principle
Exclusion ∼, 4-23 ﬀ
of equivalence, II-42-17 ﬀ
of inertia, I-9-1
of least action, II-19-1 ﬀ
of superposition, II-1-5, II-4-4
of virtual work, I-4-10
Uncertainty ∼, I-2-9 f, I-6-17 ﬀ, I-7-21,
I-37-14 f, I-37-18 ﬀ, I-38-5,
I-38-11 f, I-38-15, 1-14, 1-17 ﬀ, 2-5,
2-10 ﬀ, 2-15
and stability of atoms, II-1-2, II-5-5
Probability, I-6-1 ﬀ
Probability amplitudes, I-37-16, 1-16,
3-1 ﬀ, 16-1 ﬀ
Probability density, I-6-13, I-6-15, 16-9
Probability distribution, I-6-13 ﬀ, 16-9
Propagation, in a crystal lattice, 13-1 ﬀ
Propagation factor, II-22-31
Protons, I-2-6
Proton spin, II-8-12
Pseudo force, I-12-17 ﬀ
Psychology and physics, I-3-13 f
p-type semiconductor, 14-10
Purkinje eﬀect, I-35-4
Pyroelectricity, II-11-16
Q
Quadrupole lens, II-7-6, II-29-15 f
Quadrupole potential, II-6-14
Quantized magnetic states, II-35-1 ﬀ,
35-1 ﬀ
Quantum electrodynamics, I-2-12 f, I-2-17,
I-28-5, I-42-16
and point charges, II-28-16
Quantum mechanical resonance, 10-6
Quantum mechanics, I-2-3, I-2-9 ﬀ,
I-6-17 ﬀ, I-37-1 ﬀ, I-38-1 ﬀ, 1-1 ﬀ,
2-1 ﬀ, 3-1 ﬀ
and vector potential, II-15-14 ﬀ, 21-2 f
Momentum in ∼, I-10-16 f
Quantum numbers, 12-25
R
Rabi molecular-beam method, II-35-7 ﬀ,
35-7 ﬀ
Radiant energy, I-4-3, I-4-12, I-7-20,
I-10-15
Radiation
Blackbody ∼, I-41-5 ﬀ
Bremsstrahlung, I-34-12 f
Index-17","298"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","674","Cherenkov ∼, I-51-3
Cosmic rays, I-2-9, II-9-4
Cosmic synchrotron ∼, I-34-10 ﬀ
Electromagnetic ∼, I-26-1, I-28-1 ﬀ
Gamma rays, I-2-8
Infrared ∼, I-2-8, I-23-14, I-26-1
Light, I-2-7
Relativistic eﬀects in ∼, I-34-1 ﬀ
Synchrotron ∼, I-34-6 ﬀ
Ultraviolet ∼, I-2-8, I-26-1
X-rays, I-2-8, I-26-1, I-31-11, I-34-8,
I-48-10, I-48-12
Radiation damping, I-32-1 ﬀ
Radiation pressure, I-34-20
Radiation resistance, I-32-1 ﬀ
Radioactive clock, I-5-6 ﬀ
Radioactive isotopes, I-3-7, I-5-8, I-52-16
Radius
Bohr ∼, I-38-12, 2-12, 19-5, 19-9
Classical electron ∼, I-32-6, II-28-5
Excess ∼, II-42-9 ﬀ, II-42-13 ﬀ, II-42-29
Random walk, I-6-8 ﬀ, I-41-14 ﬀ
Ratchet and pawl machine, I-46-1 ﬀ
Rayleigh’s criterion, I-30-11
Rayleigh’s law, I-41-10
Rayleigh waves, II-38-16
Reactance, II-22-25 f
Reciprocity principle, I-26-9, I-30-12
Rectiﬁcation, I-50-15
Rectiﬁer, II-22-34
Reﬂected waves, II-33-14 ﬀ
Reﬂection, I-26-3 ﬀ
Angle of ∼, I-26-6, II-33-1
of light, II-33-1 ﬀ
Total internal ∼, II-33-22 ﬀ
Refraction, I-26-3 ﬀ
Anomalous ∼, I-33-15 ﬀ
Index of ∼, I-31-1 ﬀ, II-32-1 ﬀ
of light, II-33-1 ﬀ
Relative permeability, II-36-18
Relativistic dynamics, I-15-15 ﬀ
Relativistic energy, I-16-1 ﬀ
Relativistic mass, I-16-9 ﬀ
Relativistic momentum, I-10-14 ﬀ,
I-16-1 ﬀ
Relativity
Galilean ∼, I-10-5, I-10-11
of electric ﬁeld, II-13-13 ﬀ
of magnetic ﬁeld, II-13-13 ﬀ
Special theory of ∼, I-15-1 ﬀ
Theory of ∼, I-7-20 f, I-17-1
Resistance, I-23-9
Resistor, I-23-9, I-41-4 f, I-41-14, II-22-7 f
Resolving power, I-27-14 f
of a diﬀraction grating, I-30-10 f
Resonance, I-23-1 ﬀ
Electrical ∼, I-23-8 ﬀ
in nature, I-23-12 ﬀ
Quantum mechanical ∼, 10-6
Resonant cavity, II-23-11 ﬀ
Resonant circuits, II-23-22 ﬀ
Resonant mode, II-23-21
Resonator, cavity, II-23-1 ﬀ
Retarded time, I-28-4
Retina, I-35-1 f
Reynolds number, II-41-8 ﬀ
Rigid body, I-18-1, I-20-1
Angular momentum of a ∼, I-20-14
Rotation of a ∼, I-18-4 ﬀ
Ritz combination principle, I-38-14, 2-14
Rod cells, I-35-2 ﬀ, I-35-9, I-35-17 f,
I-36-8, I-36-10 ﬀ, 13-16
Root-mean-square (RMS) distance, I-6-10
Rotation
in space, I-20-1 ﬀ
in two dimensions, I-18-1 ﬀ
of a rigid body, I-18-4 ﬀ
of axes, I-11-4 ﬀ
Plane ∼, I-18-1
Rotation matrix, 6-6
Index-18","328"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","675","Rutherford-Bohr atomic model, II-5-4
Rydberg (unit), I-38-12, 2-12
Rydberg energy, 10-6, 19-5
S
Saha equation, I-42-9
Scalar, I-11-8
Scalar ﬁeld, II-2-3 ﬀ
Scalar product, I-11-15 ﬀ
of four-vectors, II-25-5 ﬀ
Scattering of light, I-32-1 ﬀ
Schrödinger equation, II-15-21, II-41-20,
16-6, 16-18 ﬀ, 20-28
for the hydrogen atom, 19-1 ﬀ
in a classical context, 21-1 ﬀ
Scientiﬁc method, I-2-2
Screw dislocations, II-30-20 f
Screw jack, I-4-8
Second (unit), I-5-10
Seismograph, I-51-9
Self-inductance, II-16-8, II-17-20 ﬀ
Semiconductor junction, 14-15 ﬀ
Rectiﬁcation at a ∼, 14-19 ﬀ
Semiconductors, 14-1 ﬀ
Impure ∼, 14-8 ﬀ
n-type ∼, 14-10
p-type ∼, 14-10
Shear modulus, II-38-10
Shear waves, I-51-8, II-38-11 ﬀ
Sheet of charge, II-5-7 ﬀ
Side bands, I-48-7 ﬀ
Sigma electron, 12-5
Sigma matrices, 11-3
Sigma proton, 12-6
Sigma vector, 11-7
Simultaneity, I-15-13 f
Sinusoidal waves, I-29-4 ﬀ
Skin depth, II-32-18 ﬀ
Slip dislocations, II-30-20
Smooth muscle, I-14-3
Snell’s law, I-26-5, I-26-7, I-26-14, I-31-4,
II-33-1
Sodium, 19-30 f
Solenoid, II-13-11
Solid-state physics, II-8-11
Sound, I-2-5, I-47-1 ﬀ, I-50-4
Speed of ∼, I-47-12 f
Space, I-2-4, I-8-4
Curved ∼, II-42-1 ﬀ
Space-time, I-2-9, I-17-1 ﬀ, II-26-22
Geometry of ∼, I-17-1 ﬀ
Special theory of relativity, I-15-1 ﬀ
Speciﬁc heat, I-40-13 ﬀ, II-37-7
and the failure of classical physics,
I-40-16 ﬀ
at constant volume, I-45-3
Speed, I-8-4 ﬀ
and velocity, I-9-3 f
Greeks’ diﬃculties with ∼, I-8-4 f
of light, I-15-1, II-18-16 f
of sound, I-47-12 f
Sphere of charge, II-5-10 f
Spherical aberration, I-27-13, I-36-6
of an electron microscope, II-29-10
Spherical harmonics, 19-13
Spherically symmetric solutions, 19-4 ﬀ
Spherical waves, II-20-20 ﬀ, II-21-4 ﬀ
Spinel (MgAl2O4), II-37-24
Spin one-half particles, 6-1 ﬀ, 12-1 ﬀ
Precession of ∼, 7-18 ﬀ
Spin-one particles, 5-1 ﬀ
Spin orbit, II-8-13
Spin-orbit interaction, 15-25
Spin waves, 15-1 ﬀ
Spontaneous emission, I-42-15
Spontaneous magnetization, II-36-24 ﬀ
Standard deviation, I-6-15
States
Eigen∼, 11-38
Excited ∼, II-8-14, 13-15
Index-19","305"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","676","Ground ∼, 7-3
of deﬁnite energy, 13-5 ﬀ
Stationary ∼, 7-1 ﬀ, 11-38
Time-dependent ∼, 13-10 ﬀ
State vector, 8-1 ﬀ
Resolution of ∼s, 8-4 ﬀ
Stationary states, 7-1 ﬀ, 11-38
Statistical ﬂuctuations, I-6-4 ﬀ
Statistical mechanics, I-3-2, I-40-1 ﬀ
Steady ﬂow, II-40-10 ﬀ
Steap leader, II-9-21
Stefan-Boltzmann constant, I-45-14
Stern-Gerlach apparatus, 5-1 ﬀ
Stern-Gerlach experiment, II-35-4 ﬀ,
35-4 ﬀ
Stokes’ theorem, II-3-17 ﬀ
Strain, II-38-3
Volume ∼, II-38-6
Strain tensor, II-31-22, II-39-1 ﬀ
Strangeness, 11-21
Conservation of ∼, 11-21
“Strangeness” number, I-2-14
“Strange” particles, II-8-14
Streamlines, II-40-10
Stress, II-38-3
Poincaré ∼, II-28-7 f
Volume ∼, II-38-6
Stress tensor, II-31-15 ﬀ
Striated (skeletal) muscle, I-14-3
Superconductivity, 21-1 ﬀ
Supermalloy, II-36-18
Superposition, II-13-22 f
of ﬁelds, I-12-15
Principle of ∼, I-25-3 ﬀ, I-28-3, I-47-11,
II-1-5, II-4-4
Surface
Equipotential ∼s, II-4-20 ﬀ
Isothermal ∼s, II-2-5
Surface tension, II-12-8
Susceptibility
Electric ∼, II-10-7
Magnetic ∼, II-35-14, 35-14
Symmetry, I-1-8, I-11-1 f
in physical laws, I-52-1 ﬀ
Synchrotron, I-2-8, I-15-16, II-17-9,
II-29-10, II-29-15 f, II-29-20
Synchrotron radiation, I-34-6 ﬀ, II-17-9
Cosmic ∼, I-34-10 ﬀ
T
Taylor expansion, II-6-14
Temperature, I-39-10 ﬀ
Tension
Surface ∼, II-12-8
Tensor, II-26-15, II-31-1 ﬀ
of elasticity, II-39-6 ﬀ
of inertia, II-31-11 ﬀ
of polarizability, II-31-1 ﬀ
Strain ∼, II-31-22, II-39-1 ﬀ
Stress ∼, II-31-15 ﬀ
Transformation of ∼components,
II-31-4 ﬀ
Tensor algebra, 8-6
Tensor ﬁeld, II-31-21
Tetragonal lattice, II-30-17
Theorem
Bernoulli’s ∼, II-40-10 ﬀ
Fourier ∼, II-7-17
Gauss’ ∼, II-3-8 ﬀ, 21-7
Helmholtz’s ∼, II-40-22 f
Larmor’s ∼, II-34-11 ﬀ, 34-11 ﬀ
Stokes’ ∼, II-3-17 ﬀ
Theory of gravitation, II-42-28 ﬀ
Thermal conductivity, II-2-16, II-12-3,
II-12-6
of a gas, I-43-16 f
Thermal equilibrium, I-41-5 ﬀ
Thermal ionization, I-42-8 ﬀ
Thermodynamics, I-39-3, I-45-1 ﬀ,
II-37-7 ﬀ
Index-20","277"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","677","Laws of ∼, I-44-1 ﬀ
Thomson atomic model, II-5-4
Thomson scattering cross section, I-32-13
Three-body problem, I-10-1
Three-dimensional lattice, 13-12 ﬀ
Three-dimensional waves, II-20-13 ﬀ
Three-phase power, II-16-16
Thunderstorms, II-9-9 ﬀ
Thymine, I-3-9
Tides, I-7-8
Time, I-2-4, I-5-1 ﬀ, I-8-1 ﬀ
Retarded ∼, I-28-4
Standard of ∼, I-5-9 f
Transformation of ∼, I-15-9 ﬀ
Time-dependent states, 13-10 ﬀ
Torque, I-18-6, I-20-1 ﬀ
Torsion, II-38-11 ﬀ
Total internal reﬂection, II-33-22 ﬀ
Transformation
Fourier ∼, I-25-7
Galilean ∼, I-12-18, I-15-4
Linear ∼, I-11-11
Lorentz ∼, I-15-4 f, I-17-1, I-34-15,
I-52-3, II-25-1
of ﬁelds, II-26-1 ﬀ
of time, I-15-9 ﬀ
of velocity, I-16-5 ﬀ
Transformer, II-16-7 ﬀ
Transforming amplitudes, 6-1 ﬀ
Transient response, I-21-10
Transients, I-24-1 ﬀ
Electrical ∼, I-24-7 ﬀ
Transistor, 14-21 ﬀ
Translation of axes, I-11-2 ﬀ
Transmission line, II-24-1 ﬀ
Transmitted waves, II-33-14 ﬀ
Travelling ﬁeld, II-18-9 ﬀ
Triclinic lattice, II-30-15
Trigonal lattice, II-30-16
Triphenyl cyclopropenyl molecule, 15-23
Twenty-one centimeter line, 12-15
Twin paradox, I-16-4 f
Two-dimensional ﬁelds, II-7-3 ﬀ
Two-slit interference, 3-8 ﬀ
Two-state systems, 10-1 ﬀ, 11-1 ﬀ
U
Ultraviolet radiation, I-2-8, I-26-1
Uncertainty principle, I-2-9 f, I-6-17 ﬀ,
I-7-21, I-37-14 f, I-37-18 ﬀ, I-38-5,
I-38-11 f, I-38-15, 1-14, 1-17 ﬀ, 2-5,
2-10 ﬀ, 2-15
and stability of atoms, II-1-2, II-5-5
Unit cell, I-38-9, 2-9
Unit matrix, 11-4
Unit vector, I-11-18, II-2-6
Unworldliness, II-25-18
V
Van de Graaﬀgenerator, II-5-19, II-8-14
Vector, I-11-1 ﬀ
Axial ∼, I-20-6, I-52-10 ﬀ
Components of a ∼, I-11-9
Four-∼s, I-15-14 f, I-17-8 ﬀ, II-25-1 ﬀ
Polar ∼, I-20-6, I-52-10 ﬀ
Polarization ∼, II-10-4 ﬀ
Poynting, II-27-9
State ∼, 8-1 ﬀ
Resolution of ∼s, 8-4 ﬀ
Unit ∼, I-11-18, II-2-6
Vector algebra, I-11-10 ﬀ, II-2-3, II-2-13,
II-2-21 f, II-3-1, II-3-21 ﬀ, II-27-6,
II-27-8, 5-25, 8-2 f, 8-6
Four-∼, I-17-12 ﬀ
Vector analysis, I-11-8
Vector ﬁeld, II-1-8 f, II-2-3 ﬀ
Flux of a ∼, II-3-4 ﬀ
Vector integrals, II-3-1 ﬀ
Vector operator, II-2-12
Vector potential, II-14-1 ﬀ, II-15-1 ﬀ
Index-21","315"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","678","and quantum mechanics, II-15-14 ﬀ,
21-2 f
of known currents, II-14-5 ﬀ
Vector product, I-20-6
Velocity, I-8-6
Angular ∼, I-18-4 f
Components of ∼, I-9-4 ﬀ
Group ∼, I-48-11 f
Phase ∼, I-48-10, I-48-12
Speed and ∼, I-9-3 f
Transformation of ∼, I-16-5 ﬀ
Velocity potential, II-12-17
Virtual image, I-27-6
Virtual work, principle of, I-4-10
Viscosity, II-41-1 ﬀ
Coeﬃcient of ∼, II-41-2
Viscous ﬂow, II-41-6 ﬀ
Vision, I-36-1 ﬀ, 13-16
Binocular ∼, I-36-6, I-36-8 f
Color ∼, I-35-1 ﬀ, I-36-1 ﬀ
Physiochemistry of ∼, I-35-15 ﬀ
Neurology of ∼, I-36-19 ﬀ
Visual cortex, I-36-6, I-36-8
Visual purple, I-35-15, I-35-17
Voltmeter, II-16-2
Volume strain, II-38-6
Volume stress, II-38-6
Vortex lines, II-40-21 ﬀ
Vorticity, II-40-9
W
Wall energy, II-37-11
Watt (unit), I-13-5
Wave equation, I-47-1 ﬀ, II-18-17 ﬀ
Wavefront, I-33-16 f, I-47-6, I-51-2, I-51-4
Wave function, 16-7 ﬀ
Meaning of the ∼, 21-10 f
Waveguides, II-24-1 ﬀ
Wavelength, I-26-1, I-29-5
Wave nodes, 7-17
Wave number, I-29-5
Wave packet, 13-11
Waves, I-51-1 ﬀ
Electromagnetic ∼, I-2-7, II-21-1 ﬀ
Light ∼, I-48-1
Plane ∼, II-20-1 ﬀ
Reﬂected ∼, II-33-14 ﬀ
Shear ∼, I-51-8, II-38-11 ﬀ
Sinusoidal ∼, I-29-4 ﬀ
Spherical ∼, II-20-20 ﬀ, II-21-4 ﬀ
Spin ∼, 15-1 ﬀ
Three-dimensional ∼, II-20-13 ﬀ
Transmitted ∼, II-33-14 ﬀ
“Wet” water, II-41-1 ﬀ
Work, I-13-1 ﬀ, I-14-1 ﬀ
X
X-ray diﬀraction, I-30-14, I-38-9, II-8-9,
II-30-3, 2-9
X-rays, I-2-8, I-26-1, I-31-11, I-34-8,
I-48-10, I-48-12
Y
Young’s modulus, II-38-3, II-38-11
Yukawa “photon”, II-28-23
Yukawa potential, II-28-22, 10-11
Z
Zeeman eﬀect, 12-19
Zeeman splitting, 12-15 ﬀ
Zero, absolute, I-1-8, I-2-10
Zero curl, II-3-20 ﬀ, II-4-2
Zero divergence, II-3-20 ﬀ, II-4-2
Zero mass, I-2-17
Zinc, 19-31 f
Index-22","268"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","679","Name Index
A
Adams, John C. (1819–92), I-7-10
Aharonov, Yakir (1932–), II-15-21
Ampère, André-Marie (1775–1836),
II-13-7, II-18-17, II-20-17
Anderson, Carl D. (1905–91), I-52-17
Aristotle (384–322 BC), I-5-1
Avogadro, L. R. Amedeo C. (1776–1856),
I-39-3
B
Becquerel, Antoine Henri (1852–1908),
I-28-5
Bell, Alexander G. (1847–1922), II-16-6
Bessel, Friedrich W. (1784–1846), II-23-11
Boehm, Felix H. (1924–), I-52-17
Bohm, David (1917–92), II-7-13, II-15-21
Bohr, Niels (1885–1962), I-42-14, II-5-4,
16-22, 19-8
Boltzmann, Ludwig (1844–1906), I-41-2
Bopp, Friedrich A. (1909–87), II-28-13 f,
II-28-16 f
Born, Max (1882–1970), I-37-2, I-38-16,
II-28-12, II-28-17, 1-1, 2-16, 3-1,
21-10
Bragg, William Lawrence (1890–1971),
II-30-22
Brewster, David (1781–1868), I-33-9
Briggs, Henry (1561–1630), I-22-10
Brown, Robert (1773–1858), I-41-1
C
Carnot, N. L. Sadi (1796–1832), I-4-3,
I-44-4 ﬀ, I-45-6, I-45-12
Cavendish, Henry (1731–1810), I-7-16
Cherenkov, Pavel A. (1908–90), I-51-3
Clapeyron, Benoît Paul Émile
(1799–1864), I-44-4
Copernicus, Nicolaus (1473–1543), I-7-1
Coulomb, Charles-Augustin de
(1736–1806), II-5-14
D
Dedekind, J. W. Richard (1831–1916),
I-22-5
Dicke, Robert H. (1916–97), I-7-20
Dirac, Paul A. M. (1902–84), I-52-17,
II-2-2, II-28-12 f, II-28-17, 3-1, 3-3,
8-3 f, 8-6, 12-11 f, 16-15, 16-22
E
Einstein, Albert (1879–1955), I-2-9, I-4-13,
I-6-18, I-7-20 f, I-12-15, I-12-19 f,
I-15-1 f, I-15-5, I-15-15, I-15-17,
I-16-1, I-16-8, I-16-15, I-41-1,
I-41-15, I-42-14 ﬀ, I-43-15, II-13-13,
II-25-19, II-26-23, II-27-18, II-28-7,
II-42-1, II-42-11, II-42-14,
Name Index-1","208"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","680","II-42-17 f, II-42-21, II-42-24,
II-42-28 ﬀ, 4-15, 18-16
Eötvös, Roland von (1848–1919), I-7-20
Euclid (c. 300 BC), I-2-4, I-5-10, I-12-4,
II-42-8 f
F
Faraday, Michael (1791–1867), II-10-1,
II-10-4, II-16-3 ﬀ, II-16-7, II-16-17,
II-16-21, II-17-2 ﬀ, II-18-17,
II-20-17
Fermat, Pierre de (1601–65), I-26-5 f,
I-26-15
Fermi, Enrico (1901–54), I-5-18
Feynman, Richard P. (1918–88), II-21-9,
II-28-13, II-28-17
Fourier, J. B. Joseph (1768–1830),
I-50-8 ﬀ
Frank, Ilya M. (1908–90), I-51-3
Franklin, Benjamin (1706–90), II-5-13
G
Galileo Galilei (1564–1642), I-5-1, I-5-3,
I-7-4, I-9-1, I-10-7 f, I-52-4
Gauss, J. Carl F. (1777–1855), II-3-10,
II-16-3, II-36-12
Geiger, Johann W. (1882–1945), II-5-4
Gell-Mann, Murray (1929–), I-2-14,
11-21 f, 11-27 ﬀ, 11-33
Gerlach, Walther (1889–1979), II-35-4,
II-35-6, 35-4, 35-6
Goeppert-Mayer, Maria (1906–72), 15-25
H
Hamilton, William Rowan (1805–65), 8-16
Heaviside, Oliver (1850–1925), II-21-9
Heisenberg, Werner K. (1901–76), I-37-2,
I-37-14, I-37-18 f, I-38-16, II-19-19,
1-1, 1-14, 1-17 ﬀ, 2-16, 16-14,
20-27 f
Helmholtz, Hermann von (1821–94),
I-35-13, II-40-21, II-40-23
Hess, Victor F. (1883–1964), II-9-4
Huygens, Christiaan (1629–95), I-15-3,
I-26-3, I-33-16
I
Infeld, Leopold (1898–1968), II-28-12,
II-28-17
J
Jeans, James H. (1877–1946), I-40-17,
I-41-11, I-41-13, II-2-12
Jensen, J. Hans D. (1907–73), 15-25
Josephson, Brian D. (1940–), 21-25
K
Kepler, Johannes (1571–1630), I-7-2 ﬀ
L
Lamb, Willis E. (1913–2008), II-5-14
Laplace, Pierre-Simon de (1749–1827),
I-47-12
Lawton, Willard E. (1899–1946), II-5-14 f
Leibniz, Gottfried Willhelm (1646–1716),
I-8-7
Le Verrier, Urbain (1811–77), I-7-10
Liénard, Alfred-Marie (1869–1958),
II-21-21
Lorentz, Hendrik Antoon (1853–1928),
I-15-4, I-15-8, II-21-21, II-21-24,
II-25-19, II-28-6, II-28-12, II-28-20
M
MacCullagh, James (1809–47), II-1-18
Marsden, Ernest (1889–1970), II-5-4
Maxwell, James Clerk (1831–79), I-6-1,
I-6-16, I-28-1, I-28-4, I-40-16,
I-41-13, I-46-8, II-1-16 f, II-1-20,
II-5-14 f, II-17-3, II-18-1, II-18-3 f,
Name Index-2","267"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","681","II-18-6, II-18-8, II-18-15, II-18-17,
II-18-21, II-20-17, II-21-8, II-28-5,
II-32-5 ﬀ
Mayer, Julius R. von (1814–78), I-3-3
Mendeleev, Dmitri I. (1834–1907), I-2-14
Michelson, Albert A. (1852–1931), I-15-5,
I-15-8
Miller, William C. (1910–81), I-35-4
Minkowski, Hermann (1864–1909), I-17-14
Mössbauer, Rudolf L. (1929–2011),
I-23-18
Morley, Edward W. (1838–1923), I-15-5,
I-15-8
N
Nernst, Walter H. (1864–1941), I-44-21
Newton, Isaac (1643–1727), I-7-4 ﬀ, I-7-17,
I-7-20, I-8-7, I-9-1 f, I-9-6, I-10-2 f,
I-10-16 f, I-11-2, I-12-2, I-12-14,
I-14-10, I-15-1 f, I-16-2, I-16-10,
I-18-11, I-37-2, I-47-12, II-4-20,
II-19-14, II-42-1, 1-1
Nishijima, Kazuhiko (1926–2009), I-2-14,
11-21 f
Nye, John F. (1923–), II-30-22
O
Oersted, Hans C. (1777–1851), II-18-17,
II-36-12
P
Pais, Abraham (Bram) (1918–2000),
11-21, 11-27 ﬀ, 11-33
Pasteur, Louis (1822–95), I-3-16
Pauli, Wolfgang E. (1900–58), 4-5, 11-3
Pines, David (1924–), II-7-13
Planck, Max (1858–1947), I-40-19,
I-41-11 f, I-42-13 f, I-42-16, 4-22
Plimpton, Samuel J. (1883–1948),
II-5-14 f
Poincaré, J. Henri (1854–1912), I-15-5,
I-15-9, I-16-1, II-28-7
Poynting, John Henry (1852–1914),
II-27-5, II-28-5
Priestley, Joseph (1733–1804), II-5-14
Ptolemy, Claudius (c. 2nd cent.), I-26-4 f
Pythagoras (c. 6th cent. BC), I-50-1 f
R
Rabi, Isidor I. (1898–1988), II-35-7, 35-7
Ramsey, Norman F. (1915–2011), I-5-10
Retherford, Robert C. (1912–81), II-5-14
Roemer, Ole (1644–1710), I-7-9
Rushton, William A. H. (1901–80),
I-35-17 f
Rutherford, Ernest (1871–1937), II-5-4
S
Schrödinger, Erwin (1887–1961), I-35-10,
I-37-2, I-38-16, II-19-19, 1-1, 2-16,
3-1, 16-6, 16-20 ﬀ, 20-27 f, 21-10
Shannon, Claude E. (1916–2001), I-44-4
Smoluchowski, Marian (1872–1917),
I-41-15
Snell(ius), Willebrord (1580–1626), I-26-5
Stern, Otto (1888–1969), II-35-4, II-35-6,
35-4, 35-6
Stevin(us), Simon (1548/49–1620), I-4-8
T
Tamm, Igor Y. (1895–1971), I-51-3
Thomson, Joseph John (1856–1940), II-5-4
Tycho Brahe (1546–1601), I-7-2
V
Vinci, Leonardo da (1452–1519), I-36-4
von Neumann, John (1903–57), II-12-17,
II-40-6
Name Index-3","274"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","682","W
Wapstra, Aaldert Hendrik (1922–2006),
I-52-17
Weber, Wilhelm E. (1804–91), II-16-3
Weyl, Hermann (1885–1955), I-11-1, I-52-1
Wheeler, John A. (1911–2008), II-28-13,
II-28-17
Wiechert, Emil Johann (1861–1928),
II-21-21
Wilson, Charles T. R. (1869–1959),
II-9-19 f
Y
Young, Thomas (1773–1829), I-35-13
Yukawa, Hideki (1907–81), I-2-13,
II-28-21, 10-10
Yustova, Elizaveta N. (1910–2008),
I-35-15, I-35-18
Z
Zeno of Elea (c. 5th cent. BC), I-8-5
Name Index-4","62"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","683","List of Symbols
|
|
absolute value, I-6-9
 n
k

binomial coeﬃcient, n over k, I-6-7
a∗
complex conjugate of a, I-23-1
□2
D’Alembertian operator, □2 = ∂2
∂t2 −∇2, II-25-13
⟨
⟩
expectation value, I-6-9
∇2
Laplacian operator, ∇2 = ∂2
∂x2 + ∂2
∂y2 + ∂2
∂z2 , II-2-20
∇
nabla operator, ∇= (∂/∂x, ∂/∂y, ∂/∂z), I-14-15
| 1⟩, | 2⟩
a speciﬁc choice of base vectors for a two-state system, 9-1
| I⟩, | II⟩
a speciﬁc choice of base vectors for a two-state system, 9-3
⟨φ |
state φ written as a bra vector, 8-3
⟨f | s⟩
amplitude for a system prepared in the starting state | s⟩to be
found in the ﬁnal state | f⟩, 3-3
| φ⟩
state φ written as a ket vector, 8-3
≈
approximately, I-6-16
∼
of the order, I-2-17
∝
proportional to, I-5-2
α
angular acceleration, I-18-5
γ
heat capacity ratio (adiabatic index or speciﬁc heat ratio), I-39-8
ϵ0
dielectric constant or permittivity of vacuum, ϵ0 = 8.854187817×
10−12 F/m, I-12-12
κ
Boltzmann’s constant, κ = 1.3806504 × 10−23 J/K, 14-7
κ
relative permittivity, II-10-8
κ
thermal conductivity, I-43-16
λ
wavelength, I-17-14
λ
reduced wavelength, λ = λ/2π, II-15-16
List of Symbols-1","204"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","684","µ
coeﬃcient of friction, I-12-6
µ
magnetic moment, II-14-15
µ
magnetic moment vector, II-14-15
µ
shear modulus, II-38-10
ν
frequency, I-17-14
ρ
density, I-47-6
ρ
electric charge density, II-2-15
σ
cross section, I-5-15
σ
Pauli spin matrices vector, 11-7
σx, σy, σz
Pauli spin matrices, 11-3
σ
Poisson’s ratio, II-38-3
σ
Stefan-Boltzmann constant, σ = 5.6704×10−8 W/m2K4, I-45-14
τ
torque, I-18-7
τ
torque vector, I-20-7
φ
electrostatic potential, II-4-9
Φ0
basic ﬂux unit, 21-21
χ
electric susceptibility, II-10-7
ω
angular velocity, I-18-4
ω
angular velocity vector, I-20-7
Ω
vorticity, II-40-9
a
acceleration vector, I-19-3
ax, ay, az
cartesian components of the acceleration vector, I-8-16
a
magnitude or component of the acceleration vector, I-8-13
A
area, I-5-17
Aµ = (φ, A)
four-potential, II-25-15
A
vector potential, II-14-2
Ax, Ay, Az
cartesion components of the vector potential, II-14-2
B
magnetic ﬁeld vector (magnetic induction), I-12-17
Bx, By, Bz
cartesian components of the magnetic ﬁeld vector, I-12-17
c
speed of light, c = 2.99792458 × 108 m/s, I-4-13
C
capacitance, I-23-9
C
Clebsch-Gordan coeﬃcients, 18-34
CV
speciﬁc heat at constant volume, I-45-3
d
distance, I-12-10
D
electric displacement vector, II-10-11
List of Symbols-2","190"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","685","er
unit vector in the direction r, I-28-2
E
electric ﬁeld vector, I-12-13
Ex, Ey, Ez
cartesian components of the electric ﬁeld vector, I-12-17
E
energy, I-4-13
Egap
energy gap, 14-7
Etr
transverse electric ﬁeld vector, 14-14
E
electric ﬁeld vector, 9-8
E
electromotive force, II-17-2
E
energy, I-33-19
f
focal length, I-27-4
Fµν
electromagnetic tensor, II-26-12
F
force vector, I-11-9
Fx, Fy, Fz
cartesian components of the force vector, I-9-5
F
magnitude or component of the force vector, I-7-1
g
acceleration of gravity, I-9-6
G
gravitational constant, I-7-1
h
heat ﬂow vector, II-2-6
h
Planck’s constant, h = 6.62606896 × 10−34 Js, I-17-14
ℏ
reduced Planck constant, ℏ= h/2π, I-2-9
H
magnetizing ﬁeld vector, II-32-7
i
imaginary unit, I-22-11
i
unit vector in the direction x, I-11-18
I
electric current, I-23-9
I
intensity, I-30-2
I
moment of inertia, I-18-12
Iij
tensor of inertia, II-31-13
I
intensity, 9-23
j
electric current density vector, II-2-15
jx, jy, jz
cartesian components of the electric current density vector, II-
13-22
j
unit vector in the direction y, I-11-18
J
angular momentum vector of electron orbit, II-34-4
J0(x)
Bessel function of the ﬁrst kind, II-23-11
k
Boltzmann’s constant, k = 1.3806504 × 10−23 J/K, I-39-16
List of Symbols-3","204"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","686","kµ = (ω, k)
four-wave vector, I-34-18
k
unit vector in the direction z, I-11-18
k
wave vector, I-34-17
kx, ky, kz
cartesian components of the wave vector, I-34-17
k
magnitude or component of the wave vector, wave number, I-29-6
K
bulk modulus, II-38-6
L
angular momentum vector, I-20-7
L
magnitude or component of the angular momentum vector, I-18-8
L
self-inductance, I-23-10
L
Lagrangian, II-19-15
L
self-inductance, II-17-20
| L⟩
left-hand circularly polarized photon state, 11-19
m
mass, I-4-13
meﬀ
eﬀective electron mass in a crystal lattice, 13-12
m0
rest mass, I-10-15
M
magnetization vector, II-35-14
M
mutual inductance, II-22-36
M
mutual inductance, II-17-18
M
bending moment, II-38-19
n
index of refraction, I-26-7
n
the nth Roman numeral, so that n takes on the values I, II,
. . . , N, 11-37
n
unit normal vector, II-2-6
Nn
number of electrons per unit volume, 14-7
Np
number of holes per unit volume, 14-7
p
dipole moment vector, II-6-5
p
magnitude or component of the dipole moment vector, II-6-5
pµ = (E, p)
four-momentum, I-17-12
p
momentum vector, I-15-16
px, py, pz
cartesian components of the momentum vector, I-10-15
p
magnitude or component of momentum vector, I-2-9
p
pressure, II-40-3
Pspin exch
Pauli spin exchange operator, 12-12
P
polarization vector, II-10-5
P
magnitude or component of the polarization vector, II-10-7
List of Symbols-4","223"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","687","P
power, I-24-2
P
pressure, I-39-4
P(k, n)
Bernoulli or binomial probability, I-6-8
P(A)
probability of observing event A, I-6-2
q
electric charge, I-12-11
Q
heat, I-44-5
r
radius (position) vector, I-11-9
r
radius or distance, I-5-15
R
resistance, I-23-9
R
Reynold’s number, II-41-10
| R⟩
right-hand circularly polarized photon state, 11-19
s
distance, I-8-2
S
action, II-19-6
S
entropy, I-44-19
S
Poynting vector, II-27-3
S
“strangeness” number, I-2-14
Sij
stress tensor, II-31-17
t
time, I-5-2
T
absolute temperature, I-39-16
T
half-life, I-5-6
T
kinetic energy, I-13-1
u
velocity, I-15-2
U
internal energy, I-39-7
U(t2, t1)
operator designating the operation waiting from time t1 until t2,
8-12
U
potential energy, I-13-1
U
unworldliness, II-25-18
v
velocity vector, I-11-12
vx, vy, vz
cartesian components of the velocity vector, I-8-15
v
magnitude or component of velocity vector, I-8-7
V
velocity, I-4-11
V
voltage, I-23-9
V
volume, I-39-4
V
voltage, II-17-21
List of Symbols-5","151"
"Physics","Quantum Mechanics","The Feynman Lectures on Physics - Vol 3 - Quantum Mechanics","Richard Feynman","688","W
weight, I-4-7
W
work, I-14-2
x
cartesian coordinate, I-1-11
xµ = (t, r)
four-position, I-34-18
y
cartesian coordinate, I-1-11
Yl,m(θ, φ)
spherical harmonics, 19-13
Y
Young’s modulus, II-38-3
z
cartesian coordinate, I-1-11
Z
complex impedance, I-23-12
List of Symbols-6","40"
